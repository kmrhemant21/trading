{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc270dcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<frozen importlib._bootstrap>:203\u001b[0m, in \u001b[0;36m_lock_unlock_module\u001b[0;34m(name)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Error importing numpy: you should not try to import numpy from\n        its source directory; please exit the numpy source tree, and relaunch\n        your python interpreter from there.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/pyspark-env/lib/python3.9/site-packages/numpy/__init__.py:114\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__config__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show \u001b[38;5;28;01mas\u001b[39;00m show_config\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/pyspark-env/lib/python3.9/site-packages/numpy/__config__.py:4\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiarray_umath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     __cpu_features__,\n\u001b[1;32m      6\u001b[0m     __cpu_baseline__,\n\u001b[1;32m      7\u001b[0m     __cpu_dispatch__,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/pyspark-env/lib/python3.9/site-packages/numpy/_core/__init__.py:96\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# do this after everything else, to minimize the chance of this misleadingly\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# appearing in an import-time traceback\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _add_newdocs\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _add_newdocs_scalars\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/pyspark-env/lib/python3.9/site-packages/numpy/_core/_add_newdocs.py:4874\u001b[0m\n\u001b[1;32m   4865\u001b[0m add_newdoc(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy._core.multiarray\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_set_madvise_hugepage\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   4866\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4867\u001b[0m \u001b[38;5;124;03m    _set_madvise_hugepage(enabled: bool) -> bool\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4871\u001b[0m \u001b[38;5;124;03m    See `global_state` for more information.\u001b[39;00m\n\u001b[1;32m   4872\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m)\n\u001b[0;32m-> 4874\u001b[0m \u001b[43madd_newdoc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnumpy._core._multiarray_tests\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mformat_float_OSprintf_g\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4875\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[1;32m   4876\u001b[0m \u001b[38;5;124;43;03m    format_float_OSprintf_g(val, precision)\u001b[39;49;00m\n\u001b[1;32m   4877\u001b[0m \n\u001b[1;32m   4878\u001b[0m \u001b[38;5;124;43;03m    Print a floating point scalar using the system's printf function,\u001b[39;49;00m\n\u001b[1;32m   4879\u001b[0m \u001b[38;5;124;43;03m    equivalent to:\u001b[39;49;00m\n\u001b[1;32m   4880\u001b[0m \n\u001b[1;32m   4881\u001b[0m \u001b[38;5;124;43;03m        printf(\"%.*g\", precision, val);\u001b[39;49;00m\n\u001b[1;32m   4882\u001b[0m \n\u001b[1;32m   4883\u001b[0m \u001b[38;5;124;43;03m    for half/float/double, or replacing 'g' by 'Lg' for longdouble. This\u001b[39;49;00m\n\u001b[1;32m   4884\u001b[0m \u001b[38;5;124;43;03m    method is designed to help cross-validate the format_float_* methods.\u001b[39;49;00m\n\u001b[1;32m   4885\u001b[0m \n\u001b[1;32m   4886\u001b[0m \u001b[38;5;124;43;03m    Parameters\u001b[39;49;00m\n\u001b[1;32m   4887\u001b[0m \u001b[38;5;124;43;03m    ----------\u001b[39;49;00m\n\u001b[1;32m   4888\u001b[0m \u001b[38;5;124;43;03m    val : python float or numpy floating scalar\u001b[39;49;00m\n\u001b[1;32m   4889\u001b[0m \u001b[38;5;124;43;03m        Value to format.\u001b[39;49;00m\n\u001b[1;32m   4890\u001b[0m \n\u001b[1;32m   4891\u001b[0m \u001b[38;5;124;43;03m    precision : non-negative integer, optional\u001b[39;49;00m\n\u001b[1;32m   4892\u001b[0m \u001b[38;5;124;43;03m        Precision given to printf.\u001b[39;49;00m\n\u001b[1;32m   4893\u001b[0m \n\u001b[1;32m   4894\u001b[0m \u001b[38;5;124;43;03m    Returns\u001b[39;49;00m\n\u001b[1;32m   4895\u001b[0m \u001b[38;5;124;43;03m    -------\u001b[39;49;00m\n\u001b[1;32m   4896\u001b[0m \u001b[38;5;124;43;03m    rep : string\u001b[39;49;00m\n\u001b[1;32m   4897\u001b[0m \u001b[38;5;124;43;03m        The string representation of the floating point value\u001b[39;49;00m\n\u001b[1;32m   4898\u001b[0m \n\u001b[1;32m   4899\u001b[0m \u001b[38;5;124;43;03m    See Also\u001b[39;49;00m\n\u001b[1;32m   4900\u001b[0m \u001b[38;5;124;43;03m    --------\u001b[39;49;00m\n\u001b[1;32m   4901\u001b[0m \u001b[38;5;124;43;03m    format_float_scientific\u001b[39;49;00m\n\u001b[1;32m   4902\u001b[0m \u001b[38;5;124;43;03m    format_float_positional\u001b[39;49;00m\n\u001b[1;32m   4903\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   4906\u001b[0m \u001b[38;5;66;03m##############################################################################\u001b[39;00m\n\u001b[1;32m   4907\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   4908\u001b[0m \u001b[38;5;66;03m# Documentation for ufunc attributes and methods\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m \u001b[38;5;66;03m##############################################################################\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/pyspark-env/lib/python3.9/site-packages/numpy/_core/function_base.py:549\u001b[0m, in \u001b[0;36madd_newdoc\u001b[0;34m(place, obj, doc, warn_on_python)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;124;03mAdd documentation to an existing object, typically one defined in C\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;124;03mIf possible it should be avoided.\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 549\u001b[0m new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, obj)\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[0;31mImportError\u001b[0m: numpy._core.multiarray failed to import",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Tuple, Optional\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/pyspark-env/lib/python3.9/site-packages/numpy/__init__.py:119\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    116\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mError importing numpy: you should not try to import numpy from\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124m    its source directory; please exit the numpy source tree, and relaunch\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124m    your python interpreter from there.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _core\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    123\u001b[0m     False_, ScalarType, True_, _get_promotion_state, _no_nep50_warning,\n\u001b[1;32m    124\u001b[0m     _set_promotion_state, \u001b[38;5;28mabs\u001b[39m, absolute, acos, acosh, add, \u001b[38;5;28mall\u001b[39m, allclose,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m     where, zeros, zeros_like\n\u001b[1;32m    170\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: Error importing numpy: you should not try to import numpy from\n        its source directory; please exit the numpy source tree, and relaunch\n        your python interpreter from there."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, json, math, warnings, logging\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import yfinance as yf\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# =========================\n",
    "# LOGGING\n",
    "# =========================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"ichimoku_breakout_v2\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Data\n",
    "    start_date: str = \"2015-01-01\"\n",
    "    end_date: str   = \"2025-01-01\"\n",
    "    static_symbols: Optional[List[str]] = None\n",
    "    static_symbols_path: Optional[str] = None\n",
    "    cache_dir: str = \"cache\"\n",
    "    out_dir: str   = \"outputs\"\n",
    "    plot: bool     = True\n",
    "\n",
    "    # Ichimoku (classic)\n",
    "    tenkan_len: int = 9\n",
    "    kijun_len:  int = 26\n",
    "    spanb_len:  int = 52\n",
    "\n",
    "    # --- Confirmation toggles ---\n",
    "    use_ichimoku_confirms: bool = True      # master toggle (OFF by default)\n",
    "    confirm_tenkan_gt_kijun: bool = True     # Tenkan > Kijun\n",
    "    confirm_close_gt_kijun: bool = True      # Close > Kijun\n",
    "    confirm_chikou_above: bool = True        # Chikou (proxy) above past price\n",
    "    confirm_min_break_pct: float = 0.005     # >= 0.5% beyond cloud top\n",
    "    confirm_min_cloud_thickness_pct: float = 0.01  # >= 1% of price\n",
    "\n",
    "    use_volume_confirm: bool = False         # master toggle (OFF by default)\n",
    "    volume_surge_mult: float = 1.5           # Volume > 1.5 × MA\n",
    "    volume_ma_len: int = 20                  # MA window for volume\n",
    "\n",
    "    # Exits\n",
    "    stop_loss_pct: float = 0.05   # 10% SL\n",
    "    target_pct: float    = 0.10   # 10% TP\n",
    "\n",
    "    # Portfolio\n",
    "    apply_fees: bool    = True\n",
    "    initial_capital: float = 500_000.0\n",
    "    max_concurrent_positions: int = 5\n",
    "    deploy_cash_frac: float = 0.20  # cap daily deployment to 25% of available cash\n",
    "\n",
    "    # Execution\n",
    "    entry_on_next_open: bool = True\n",
    "    exit_on_next_open: bool  = True\n",
    "\n",
    "    # Candidate ranking & filters\n",
    "    benchmark_try: Tuple[str,...] = (\"^CNX500\",\"^CRSLDX\",\"^NSE500\",\"^NIFTY500\",\"^BSE500\",\"^NSEI\")\n",
    "    volar_lookback: int = 252\n",
    "    filter_52w_window: int = 252\n",
    "    within_pct_of_52w_high: float = 0.70\n",
    "    top_k_daily: int = 300\n",
    "\n",
    "    # Optional liquidity guard\n",
    "    enable_basic_liquidity: bool = False\n",
    "    min_price_inr: float = 50.0\n",
    "    min_avg_vol_20d: float = 50_000.0\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "# =========================\n",
    "# FEES (Groww-like)\n",
    "# =========================\n",
    "APPLY_FEES = True\n",
    "\n",
    "def calc_fees(turnover_buy: float, turnover_sell: float) -> float:\n",
    "    if not APPLY_FEES:\n",
    "        return 0.0\n",
    "    BROKER_PCT = 0.001\n",
    "    BROKER_MIN = 5.0\n",
    "    BROKER_CAP = 20.0\n",
    "    STT_PCT = 0.001\n",
    "    STAMP_BUY_PCT = 0.00015\n",
    "    EXCH_PCT = 0.0000297\n",
    "    SEBI_PCT = 0.000001\n",
    "    IPFT_PCT = 0.000001\n",
    "    GST_PCT = 0.18\n",
    "    DP_SELL = 20.0 if turnover_sell >= 100 else 0.0\n",
    "\n",
    "    def _broker(turnover):\n",
    "        if turnover <= 0:\n",
    "            return 0.0\n",
    "        fee = turnover * BROKER_PCT\n",
    "        fee = max(BROKER_MIN, min(fee, BROKER_CAP))\n",
    "        return fee\n",
    "\n",
    "    br_buy  = _broker(turnover_buy)\n",
    "    br_sell = _broker(turnover_sell)\n",
    "    stt   = STT_PCT * (turnover_buy + turnover_sell)\n",
    "    stamp = STAMP_BUY_PCT * turnover_buy\n",
    "    exch  = EXCH_PCT * (turnover_buy + turnover_sell)\n",
    "    sebi  = SEBI_PCT * (turnover_buy + turnover_sell)\n",
    "    ipft  = IPFT_PCT * (turnover_buy + turnover_sell)\n",
    "    dp    = DP_SELL\n",
    "    gst_base = br_buy + br_sell + dp + exch + sebi + ipft\n",
    "    gst   = GST_PCT * gst_base\n",
    "    return float((br_buy + br_sell) + stt + stamp + exch + sebi + ipft + dp + gst)\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def ensure_dirs(*paths):\n",
    "    for p in paths:\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def today_str():\n",
    "    return pd.Timestamp.today(tz=\"Asia/Kolkata\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def load_static_symbols(static_symbols: Optional[List[str]], static_symbols_path: Optional[str]) -> List[str]:\n",
    "    syms: List[str] = []\n",
    "    if static_symbols and len(static_symbols) > 0:\n",
    "        syms = list(static_symbols)\n",
    "    elif static_symbols_path and os.path.exists(static_symbols_path):\n",
    "        with open(static_symbols_path, \"r\") as f:\n",
    "            syms = [line.strip() for line in f if line.strip()]\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Provide CFG.static_symbols=[...] ('.NS' suffixes) or set CFG.static_symbols_path \"\n",
    "            \"to a file containing one symbol per line.\"\n",
    "        )\n",
    "    out = []\n",
    "    for s in syms:\n",
    "        s = s.strip().upper()\n",
    "        if not s.endswith(\".NS\"):\n",
    "            s = f\"{s}.NS\"\n",
    "        out.append(s)\n",
    "    seen = set(); uniq=[]\n",
    "    for s in out:\n",
    "        if s not in seen:\n",
    "            uniq.append(s); seen.add(s)\n",
    "    return uniq\n",
    "\n",
    "def fetch_prices(tickers: List[str], start: str, end: Optional[str], cache_dir: str) -> Dict[str, pd.DataFrame]:\n",
    "    ensure_dirs(cache_dir)\n",
    "    data = {}\n",
    "    end = end or today_str()\n",
    "    for ticker in tickers:\n",
    "        cache_path = os.path.join(cache_dir, f\"{ticker.replace('^', '_')}.parquet\")\n",
    "        if os.path.exists(cache_path):\n",
    "            try:\n",
    "                df = pd.read_parquet(cache_path)\n",
    "                if len(df) and pd.to_datetime(df.index[-1]).strftime(\"%Y-%m-%d\") >= end:\n",
    "                    data[ticker] = df\n",
    "                    continue\n",
    "            except Exception:\n",
    "                pass\n",
    "        try:\n",
    "            df = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False, multi_level_index=False)\n",
    "            if df is None or df.empty:\n",
    "                continue\n",
    "            df = df.rename(columns=str.title)  # Open, High, Low, Close, Volume\n",
    "            df = df[['Open', 'High', 'Low', 'Close', 'Volume']].dropna()\n",
    "            df.index.name = \"date\"\n",
    "            df.to_parquet(cache_path)\n",
    "            data[ticker] = df\n",
    "        except Exception:\n",
    "            continue\n",
    "    return data\n",
    "\n",
    "# =========================\n",
    "# Ichimoku + indicators\n",
    "# =========================\n",
    "def mid_of_high_low(high: pd.Series, low: pd.Series, length: int) -> pd.Series:\n",
    "    hh = high.rolling(length).max()\n",
    "    ll = low.rolling(length).min()\n",
    "    return (hh + ll) / 2.0\n",
    "\n",
    "def compute_indicators(df: pd.DataFrame, cfg: Config) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # Ichimoku (unshifted bases)\n",
    "    out[\"tenkan\"] = mid_of_high_low(out[\"High\"], out[\"Low\"], cfg.tenkan_len)\n",
    "    out[\"kijun\"]  = mid_of_high_low(out[\"High\"], out[\"Low\"], cfg.kijun_len)\n",
    "    out[\"span_a_base\"] = (out[\"tenkan\"] + out[\"kijun\"]) / 2.0\n",
    "    out[\"span_b_base\"] = mid_of_high_low(out[\"High\"], out[\"Low\"], cfg.spanb_len)\n",
    "    out[\"cloud_top\"] = out[[\"span_a_base\", \"span_b_base\"]].max(axis=1)\n",
    "    out[\"cloud_bot\"] = out[[\"span_a_base\", \"span_b_base\"]].min(axis=1)\n",
    "\n",
    "    # For filters/ranking\n",
    "    out[\"cloud_thickness\"]  = (out[\"cloud_top\"] - out[\"cloud_bot\"]) / out[\"Close\"]\n",
    "    out[\"dist_above_cloud\"] = (out[\"Close\"] / out[\"cloud_top\"]) - 1.0\n",
    "\n",
    "    # Volume MA\n",
    "    out[\"avg_vol_ma\"] = out[\"Volume\"].rolling(CFG.volume_ma_len).mean()\n",
    "\n",
    "    # Other filters\n",
    "    out[\"avg_vol_20\"] = out[\"Volume\"].rolling(20).mean()\n",
    "    out[\"high_52w\"]   = out[\"Close\"].rolling(cfg.filter_52w_window).max()\n",
    "    return out.dropna()\n",
    "\n",
    "def basic_liquidity_ok(row: pd.Series, cfg: Config) -> bool:\n",
    "    if not cfg.enable_basic_liquidity:\n",
    "        return True\n",
    "    if row[\"Close\"] < cfg.min_price_inr:\n",
    "        return False\n",
    "    if row[\"avg_vol_20\"] < cfg.min_avg_vol_20d:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def simulate_ticker(ticker: str, df: pd.DataFrame, cfg: Config):\n",
    "    d = compute_indicators(df, cfg).copy()\n",
    "    cols = [\n",
    "        \"ticker\",\"side\",\"date\",\"price\",\"shares\",\"reason\",\"signal_reason\",\"score\",\n",
    "        \"close\",\"cloud_top\",\"cloud_bot\",\"span_a\",\"span_b\",\"kijun\",\"tenkan\",\"high_52w\"\n",
    "    ]\n",
    "    if d.empty:\n",
    "        return pd.DataFrame(columns=cols), pd.Series(dtype=float)\n",
    "\n",
    "    # --- Base breakout conditions ---\n",
    "    cross_up  = (d[\"Close\"].shift(1) <= d[\"cloud_top\"].shift(1)) & (d[\"Close\"] > d[\"cloud_top\"])\n",
    "    span_bull = d[\"span_a_base\"] > d[\"span_b_base\"]\n",
    "    entry_signal = cross_up & span_bull\n",
    "\n",
    "    # --- OPTIONAL: Ichimoku confirmations ---\n",
    "    if cfg.use_ichimoku_confirms:\n",
    "        confs = []\n",
    "        if cfg.confirm_tenkan_gt_kijun:\n",
    "            confs.append(d[\"tenkan\"] > d[\"kijun\"])\n",
    "        if cfg.confirm_close_gt_kijun:\n",
    "            confs.append(d[\"Close\"] > d[\"kijun\"])\n",
    "        if cfg.confirm_chikou_above:\n",
    "            # Proxy: chikou (close shifted back 26) above past price\n",
    "            confs.append(d[\"Close\"].shift(-26) > d[\"Close\"])\n",
    "        if cfg.confirm_min_break_pct and cfg.confirm_min_break_pct > 0:\n",
    "            confs.append(((d[\"Close\"] / d[\"cloud_top\"]) - 1.0) >= cfg.confirm_min_break_pct)\n",
    "        if cfg.confirm_min_cloud_thickness_pct and cfg.confirm_min_cloud_thickness_pct > 0:\n",
    "            confs.append(d[\"cloud_thickness\"] >= cfg.confirm_min_cloud_thickness_pct)\n",
    "        for c in confs:\n",
    "            entry_signal = entry_signal & c\n",
    "\n",
    "    # --- OPTIONAL: Volume confirmation ---\n",
    "    if cfg.use_volume_confirm:\n",
    "        vol_surge = d[\"Volume\"] > (cfg.volume_surge_mult * d[\"avg_vol_ma\"])\n",
    "        entry_signal = entry_signal & vol_surge\n",
    "\n",
    "    # Indicator exit (close back into/below cloud or below kijun)\n",
    "    indicator_exit = (d[\"Close\"] < d[\"cloud_bot\"]) | (d[\"Close\"] < d[\"kijun\"])\n",
    "\n",
    "    in_pos = False\n",
    "    entry_px = stop_px = tgt_px = 0.0\n",
    "    trades = []\n",
    "    idx = list(d.index)\n",
    "\n",
    "    for i in range(len(idx)-1):\n",
    "        dt, nxt = idx[i], idx[i+1]\n",
    "        row, nxt_row = d.loc[dt], d.loc[nxt]\n",
    "\n",
    "        if not in_pos:\n",
    "            if entry_signal.loc[dt] and basic_liquidity_ok(row, cfg):\n",
    "                px = float(nxt_row[\"Open\"] if cfg.entry_on_next_open else row[\"Close\"])\n",
    "                sig_reason = \"Ichimoku breakout: Close↑CloudTop & SpanA>SpanB\"\n",
    "                # enrich reason with toggles used\n",
    "                if cfg.use_ichimoku_confirms or cfg.use_volume_confirm:\n",
    "                    parts = []\n",
    "                    if cfg.use_ichimoku_confirms:\n",
    "                        parts.append(\"IchimokuConf\")\n",
    "                    if cfg.use_volume_confirm:\n",
    "                        parts.append(\"VolConfirm\")\n",
    "                    sig_reason += f\" [{'+'.join(parts)}]\"\n",
    "                trades.append({\n",
    "                    \"ticker\": ticker, \"side\": \"BUY\", \"date\": nxt if cfg.entry_on_next_open else dt,\n",
    "                    \"price\": px, \"shares\": 0, \"reason\": \"candidate\", \"signal_reason\": sig_reason,\n",
    "                    \"score\": float(1000.0*row[\"dist_above_cloud\"]),\n",
    "                    \"close\": float(row[\"Close\"]),\n",
    "                    \"cloud_top\": float(row[\"cloud_top\"]), \"cloud_bot\": float(row[\"cloud_bot\"]),\n",
    "                    \"span_a\": float(row[\"span_a_base\"]), \"span_b\": float(row[\"span_b_base\"]),\n",
    "                    \"kijun\": float(row[\"kijun\"]), \"tenkan\": float(row[\"tenkan\"]),\n",
    "                    \"high_52w\": float(row[\"high_52w\"])\n",
    "                })\n",
    "                in_pos = True\n",
    "                entry_px = px\n",
    "                stop_px = entry_px * (1 - cfg.stop_loss_pct)\n",
    "                tgt_px  = entry_px * (1 + cfg.target_pct)\n",
    "        else:\n",
    "            hit = None\n",
    "            exec_date = nxt if cfg.exit_on_next_open else dt\n",
    "            # Priority: TP/SL, then indicator exit\n",
    "            if nxt_row[\"Low\"] <= stop_px and nxt_row[\"High\"] >= tgt_px:\n",
    "                hit, exec_price = \"target\", float(tgt_px)\n",
    "            elif nxt_row[\"Low\"] <= stop_px:\n",
    "                hit, exec_price = \"stop\", float(stop_px)\n",
    "            elif nxt_row[\"High\"] >= tgt_px:\n",
    "                hit, exec_price = \"target\", float(tgt_px)\n",
    "            elif indicator_exit.loc[dt]:\n",
    "                exec_price = float(nxt_row[\"Open\"] if cfg.exit_on_next_open else row[\"Close\"])\n",
    "                hit = \"indicator_exit\"\n",
    "\n",
    "            if hit is not None:\n",
    "                trades.append({\n",
    "                    \"ticker\": ticker, \"side\": \"SELL\", \"date\": exec_date,\n",
    "                    \"price\": float(exec_price), \"shares\": 0, \"reason\": hit,\n",
    "                    \"signal_reason\": \"\",\n",
    "                    \"score\": np.nan,\n",
    "                    \"close\": float(row[\"Close\"]),\n",
    "                    \"cloud_top\": float(row[\"cloud_top\"]), \"cloud_bot\": float(row[\"cloud_bot\"]),\n",
    "                    \"span_a\": float(row[\"span_a_base\"]), \"span_b\": float(row[\"span_b_base\"]),\n",
    "                    \"kijun\": float(row[\"kijun\"]), \"tenkan\": float(row[\"tenkan\"]),\n",
    "                    \"high_52w\": float(row[\"high_52w\"])\n",
    "                })\n",
    "                in_pos = False\n",
    "                entry_px = stop_px = tgt_px = 0.0\n",
    "\n",
    "    if in_pos:\n",
    "        last_dt = d.index[-1]; row = d.loc[last_dt]\n",
    "        trades.append({\n",
    "            \"ticker\": ticker, \"side\": \"SELL\", \"date\": last_dt,\n",
    "            \"price\": float(row[\"Close\"]), \"shares\": 0, \"reason\": \"final_close\",\n",
    "            \"signal_reason\": \"\", \"score\": np.nan,\n",
    "            \"close\": float(row[\"Close\"]),\n",
    "            \"cloud_top\": float(row[\"cloud_top\"]), \"cloud_bot\": float(row[\"cloud_bot\"]),\n",
    "            \"span_a\": float(row[\"span_a_base\"]), \"span_b\": float(row[\"span_b_base\"]),\n",
    "            \"kijun\": float(row[\"kijun\"]), \"tenkan\": float(row[\"tenkan\"]),\n",
    "            \"high_52w\": float(row[\"high_52w\"])\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(trades, columns=cols), pd.Series(dtype=float)\n",
    "\n",
    "# =========================\n",
    "# Ranking & Portfolio\n",
    "# =========================\n",
    "def pick_benchmark(benchmarks: Tuple[str,...], start: str, end: Optional[str], cache_dir: str) -> Tuple[str, pd.DataFrame]:\n",
    "    for t in benchmarks:\n",
    "        data = fetch_prices([t], start, end, cache_dir)\n",
    "        df = data.get(t)\n",
    "        if df is not None and not df.empty:\n",
    "            log.info(\"Using benchmark: %s\", t)\n",
    "            return t, df\n",
    "    idx = pd.date_range(start=start, end=end or today_str(), freq=\"B\")\n",
    "    df = pd.DataFrame({\"Close\": np.ones(len(idx))}, index=idx)\n",
    "    log.warning(\"No benchmark found; using synthetic flat series.\")\n",
    "    return \"SYNTH_BENCH\", df\n",
    "\n",
    "def compute_volar_scores(end_dt: pd.Timestamp, tickers: List[str], data_map: Dict[str,pd.DataFrame], bench_df: pd.DataFrame, lookback: int) -> Dict[str, float]:\n",
    "    scores = {}\n",
    "    bser = bench_df[\"Close\"].loc[:end_dt].pct_change().dropna().iloc[-lookback:]\n",
    "    for t in tickers:\n",
    "        df = data_map.get(t)\n",
    "        if df is None or df.empty:\n",
    "            scores[t] = 0.0; continue\n",
    "        if end_dt not in df.index:\n",
    "            df = df[df.index <= end_dt]\n",
    "            if df.empty: scores[t]=0.0; continue\n",
    "        r = df[\"Close\"].loc[:end_dt].pct_change().dropna().iloc[-lookback:]\n",
    "        common = pd.concat([r, bser], axis=1, keys=[\"s\",\"b\"]).dropna()\n",
    "        if common.shape[0] < max(20, int(0.4*lookback)):\n",
    "            scores[t] = 0.0; continue\n",
    "        excess = common[\"s\"] - common[\"b\"]\n",
    "        vol = common[\"s\"].std(ddof=0)\n",
    "        scores[t] = 0.0 if vol <= 1e-8 else float((excess.mean() / vol) * math.sqrt(252.0))\n",
    "    return scores\n",
    "\n",
    "def markowitz_long_only(mu: np.ndarray, Sigma: np.ndarray) -> np.ndarray:\n",
    "    n = len(mu)\n",
    "    eps = 1e-6\n",
    "    Sigma = Sigma + eps*np.eye(n)\n",
    "\n",
    "    def solve_lambda(lmbd: float, active_mask=None):\n",
    "        if active_mask is None:\n",
    "            A = np.block([[2*lmbd*Sigma, np.ones((n,1))],[np.ones((1,n)), np.zeros((1,1))]])\n",
    "            b = np.concatenate([mu, np.array([1.0])])\n",
    "            try:\n",
    "                sol = np.linalg.solve(A, b); w = sol[:n]\n",
    "            except np.linalg.LinAlgError:\n",
    "                w = np.full(n, 1.0/n)\n",
    "            return w\n",
    "        idx = np.where(active_mask)[0]\n",
    "        if len(idx)==0:\n",
    "            return np.full(n, 1.0/n)\n",
    "        S = Sigma[np.ix_(idx, idx)]\n",
    "        o = np.ones(len(idx)); m = mu[idx]\n",
    "        A = np.block([[2*lmbd*S, o[:,None]],[o[None,:], np.zeros((1,1))]])\n",
    "        b = np.concatenate([m, np.array([1.0])])\n",
    "        try:\n",
    "            sol = np.linalg.solve(A, b); w_sub = sol[:len(idx)]\n",
    "        except np.linalg.LinAlgError:\n",
    "            w_sub = np.full(len(idx), 1.0/len(idx))\n",
    "        w = np.zeros(n); w[idx] = w_sub\n",
    "        return w\n",
    "\n",
    "    best_w = np.full(n, 1.0/n); best_sr = -1e9\n",
    "    for lmbd in np.logspace(-3, 3, 31):\n",
    "        active = np.ones(n, dtype=bool)\n",
    "        w = None\n",
    "        for _ in range(n):\n",
    "            w = solve_lambda(lmbd, active_mask=active)\n",
    "            if not (w < 0).any(): break\n",
    "            active[np.argmin(w)] = False\n",
    "        if w is None: continue\n",
    "        w = np.clip(w, 0, None); \n",
    "        if w.sum() <= 0: continue\n",
    "        w = w / w.sum()\n",
    "        mu_p = float(mu @ w)\n",
    "        vol_p = float(np.sqrt(w @ Sigma @ w))\n",
    "        if vol_p <= 1e-8: continue\n",
    "        sr = mu_p / vol_p\n",
    "        if sr > best_sr:\n",
    "            best_sr = sr; best_w = w.copy()\n",
    "    return best_w\n",
    "\n",
    "def aggregate_and_apply(all_trades: pd.DataFrame, data_map: Dict[str, pd.DataFrame], bench_df: pd.DataFrame, cfg: Config):\n",
    "    if all_trades.empty:\n",
    "        return all_trades, pd.Series(dtype=float), {}\n",
    "\n",
    "    side_order = {\"BUY\": 0, \"SELL\": 1}\n",
    "    all_trades = (all_trades\n",
    "        .assign(_sorder=all_trades[\"side\"].map(side_order))\n",
    "        .sort_values(by=[\"date\", \"_sorder\"], kind=\"stable\")\n",
    "        .drop(columns=[\"_sorder\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    all_trades[\"date\"] = pd.to_datetime(all_trades[\"date\"])\n",
    "\n",
    "    equity_curve = []\n",
    "    dates = sorted(all_trades[\"date\"].unique().tolist())\n",
    "    cash = cfg.initial_capital\n",
    "    open_positions = {}\n",
    "    completed_legs = []\n",
    "\n",
    "    global APPLY_FEES\n",
    "    APPLY_FEES = cfg.apply_fees\n",
    "\n",
    "    def _get_close_on(tkr, dt):\n",
    "        df = data_map.get(tkr)\n",
    "        if df is None or df.empty:\n",
    "            return np.nan\n",
    "        if dt in df.index:\n",
    "            return float(df.loc[dt, \"Close\"])\n",
    "        prev = df[df.index <= dt]\n",
    "        if prev.empty:\n",
    "            return np.nan\n",
    "        return float(prev[\"Close\"].iloc[-1])\n",
    "\n",
    "    if dates:\n",
    "        seed_date = pd.to_datetime(dates[0]) - pd.Timedelta(days=1)\n",
    "        equity_curve.append((seed_date, float(cash)))\n",
    "\n",
    "    for dt in dates:\n",
    "        day_trades = all_trades[all_trades[\"date\"] == dt].copy()\n",
    "\n",
    "        # ---- SELL first ----\n",
    "        for _, tr in day_trades[day_trades[\"side\"] == \"SELL\"].iterrows():\n",
    "            tkr = tr[\"ticker\"]\n",
    "            price = float(tr[\"price\"])\n",
    "            pos = open_positions.get(tkr)\n",
    "            if pos is None:\n",
    "                continue\n",
    "            shares = int(pos[\"shares\"])\n",
    "            turnover_sell = shares * price\n",
    "            fee = calc_fees(0.0, turnover_sell)\n",
    "            pnl = (price - pos[\"entry_px\"]) * shares\n",
    "            cash += (turnover_sell - fee)\n",
    "            realized = pnl - fee - pos.get(\"buy_fee\", 0.0)\n",
    "            completed_legs.append({\n",
    "                \"ticker\": tkr, \"side\": \"SELL\", \"date\": dt, \"price\": price,\n",
    "                \"shares\": shares, \"reason\": tr.get(\"reason\",\"\"),\n",
    "                \"turnover\": turnover_sell, \"fees_inr\": fee, \"pnl_inr\": realized,\n",
    "                \"close\": tr.get(\"close\", np.nan),\n",
    "                \"cloud_top\": tr.get(\"cloud_top\", np.nan), \"cloud_bot\": tr.get(\"cloud_bot\", np.nan),\n",
    "                \"span_a\": tr.get(\"span_a\", np.nan), \"span_b\": tr.get(\"span_b\", np.nan),\n",
    "                \"kijun\": tr.get(\"kijun\", np.nan), \"tenkan\": tr.get(\"tenkan\", np.nan),\n",
    "                \"high_52w\": tr.get(\"high_52w\", np.nan),\n",
    "                \"volar\": tr.get(\"volar\", np.nan), \"mvo_weight\": np.nan, \"alloc_inr\": np.nan\n",
    "            })\n",
    "            log.info(\"Exit %-12s px=%8.2f shares=%6d reason=%s net=%.2f cash=%.2f\",\n",
    "                     tkr, price, shares, tr.get(\"reason\",\"\"), realized, cash)\n",
    "            del open_positions[tkr]\n",
    "\n",
    "        # ---- BUY candidates today ----\n",
    "        buys_today = day_trades[day_trades[\"side\"] == \"BUY\"].copy()\n",
    "\n",
    "        # 52w filter\n",
    "        if not buys_today.empty:\n",
    "            keep = []\n",
    "            for _, rr in buys_today.iterrows():\n",
    "                df = data_map.get(rr[\"ticker\"])\n",
    "                if df is None or df.empty or dt not in df.index:\n",
    "                    continue\n",
    "                close = float(df.loc[dt, \"Close\"])\n",
    "                hist = df[\"Close\"].loc[:dt]\n",
    "                window = hist.iloc[-cfg.filter_52w_window:] if len(hist)>=cfg.filter_52w_window else hist\n",
    "                high_52w = float(window.max())\n",
    "                if high_52w>0 and close >= cfg.within_pct_of_52w_high * high_52w:\n",
    "                    keep.append(rr)\n",
    "            buys_today = pd.DataFrame(keep) if keep else pd.DataFrame(columns=buys_today.columns)\n",
    "\n",
    "        # Exclude already-held tickers\n",
    "        if not buys_today.empty:\n",
    "            buys_today = buys_today[~buys_today[\"ticker\"].isin(open_positions.keys())]\n",
    "\n",
    "        # VOLAR ranking\n",
    "        if not buys_today.empty:\n",
    "            tickers = buys_today[\"ticker\"].tolist()\n",
    "            volar_scores = compute_volar_scores(dt, tickers, data_map, bench_df, cfg.volar_lookback)\n",
    "            buys_today[\"volar\"] = buys_today[\"ticker\"].map(volar_scores)\n",
    "            # Prefer stronger breaks: higher VOLAR then larger distance beyond cloud\n",
    "            buys_today[\"rank_score\"] = buys_today[\"volar\"].fillna(0.0) + 0.1*buys_today[\"score\"].fillna(0.0)\n",
    "            buys_today = buys_today.sort_values([\"rank_score\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "        slots = cfg.max_concurrent_positions - len(open_positions)\n",
    "        selected = pd.DataFrame(columns=buys_today.columns)\n",
    "        if slots > 0 and not buys_today.empty:\n",
    "            selected = buys_today.head(min(cfg.top_k_daily, slots)).copy()\n",
    "\n",
    "        if not selected.empty:\n",
    "            log.info(\"Selected %d BUY candidates on %s:\", selected.shape[0], dt.date())\n",
    "            for i, rr in selected.reset_index(drop=True).iterrows():\n",
    "                log.info(\"  %-12s volar=%6.2f rank=%d px=%8.2f\", rr[\"ticker\"], rr.get(\"volar\",0.0), i+1, rr[\"price\"])\n",
    "\n",
    "            # MVO sizing\n",
    "            names = selected[\"ticker\"].tolist()\n",
    "            rets = []\n",
    "            for t in names:\n",
    "                df = data_map.get(t)\n",
    "                ser = df[\"Close\"].loc[:dt].pct_change().dropna().iloc[-cfg.volar_lookback:]\n",
    "                rets.append(ser)\n",
    "            R = pd.concat(rets, axis=1); R.columns = names; R = R.dropna()\n",
    "            if R.empty or R.shape[0] < max(20, int(0.4*cfg.volar_lookback)) or R.shape[1] == 0:\n",
    "                weights = np.full(len(names), 1.0/len(names))\n",
    "            else:\n",
    "                mu = R.mean().values; Sigma = R.cov().values\n",
    "                weights = markowitz_long_only(mu, Sigma)\n",
    "\n",
    "            # Cap deployed cash today\n",
    "            deploy_cash = max(0.0, float(cash)) * float(cfg.deploy_cash_frac)\n",
    "            if deploy_cash <= 0:\n",
    "                log.info(\"No deployable cash (cap=%.0f%%) on %s\", 100*cfg.deploy_cash_frac, dt.date())\n",
    "            else:\n",
    "                alloc = (weights / weights.sum()) * deploy_cash if weights.sum()>0 else np.full(len(names), deploy_cash/len(names))\n",
    "                rank_map = {row[\"ticker\"]: (idx+1) for idx, (_, row) in enumerate(selected.iterrows())}\n",
    "                for w_amt, t in zip(alloc, names):\n",
    "                    df_t = data_map[t]\n",
    "                    price = float(df_t.loc[dt, \"Close\"] if dt in df_t.index else df_t[\"Close\"].loc[:dt].iloc[-1])\n",
    "                    shares = int(math.floor(w_amt / price))\n",
    "                    if shares <= 0:\n",
    "                        log.info(\"Skip BUY %-12s (alloc %.2f too small)\", t, w_amt)\n",
    "                        continue\n",
    "                    turn = shares * price\n",
    "                    fee = calc_fees(turn, 0.0)\n",
    "                    total_cost = turn + fee\n",
    "                    if total_cost > cash:\n",
    "                        shares = int(math.floor((cash - fee) / price))\n",
    "                        if shares <= 0:\n",
    "                            log.info(\"Skip BUY %-12s due to cash/fees\", t)\n",
    "                            continue\n",
    "                        turn = shares * price\n",
    "                        total_cost = turn + fee\n",
    "                    cash -= total_cost\n",
    "                    open_positions[t] = {\"entry_date\": dt, \"entry_px\": price, \"shares\": shares, \"buy_fee\": fee, \"entry_reason\": \"entry\"}\n",
    "                    row_sel = selected[selected[\"ticker\"]==t].iloc[0]\n",
    "                    volar_val = float(row_sel.get(\"volar\", np.nan))\n",
    "                    rank_pos = rank_map.get(t, np.nan)\n",
    "                    high_52w = float(row_sel.get(\"high_52w\", np.nan))\n",
    "                    close_val = float(row_sel.get(\"close\", np.nan))\n",
    "                    pct_52w = (close_val / high_52w) if (high_52w and high_52w>0) else np.nan\n",
    "                    mvo_weight_today = (w_amt / deploy_cash) if deploy_cash > 0 else 0.0\n",
    "                    sig_reason = row_sel.get(\"signal_reason\", \"Ichimoku breakout: Close↑CloudTop & SpanA>SpanB\")\n",
    "                    reason_text = (\n",
    "                        f\"{sig_reason}; 52w%={pct_52w:.1%} (>= {CFG.within_pct_of_52w_high:.0%}); \"\n",
    "                        f\"VOLAR rank {int(rank_pos)}/{len(names)} (VOLAR={volar_val:.2f}); \"\n",
    "                        f\"MVO weight={mvo_weight_today:.1%} of capped cash ({100*cfg.deploy_cash_frac:.0f}% of available)\"\n",
    "                    )\n",
    "                    completed_legs.append({\n",
    "                        \"ticker\": t, \"side\": \"BUY\", \"date\": dt, \"price\": price,\n",
    "                        \"shares\": shares, \"reason\": reason_text,\n",
    "                        \"turnover\": turn, \"fees_inr\": fee, \"pnl_inr\": 0.0,\n",
    "                        \"close\": close_val,\n",
    "                        \"cloud_top\": float(row_sel.get(\"cloud_top\", np.nan)),\n",
    "                        \"cloud_bot\": float(row_sel.get(\"cloud_bot\", np.nan)),\n",
    "                        \"span_a\": float(row_sel.get(\"span_a\", np.nan)),\n",
    "                        \"span_b\": float(row_sel.get(\"span_b\", np.nan)),\n",
    "                        \"kijun\": float(row_sel.get(\"kijun\", np.nan)),\n",
    "                        \"tenkan\": float(row_sel.get(\"tenkan\", np.nan)),\n",
    "                        \"high_52w\": high_52w,\n",
    "                        \"volar\": volar_val, \"mvo_weight\": float(mvo_weight_today), \"alloc_inr\": float(w_amt)\n",
    "                    })\n",
    "                    log.info(\"BUY %-12s px=%8.2f sh=%6d fee=%.2f cash=%.2f :: %s\",\n",
    "                             t, price, shares, fee, cash, reason_text)\n",
    "\n",
    "        # MTM valuation\n",
    "        mtm = 0.0\n",
    "        for _tkr, pos in open_positions.items():\n",
    "            px = _get_close_on(_tkr, dt)\n",
    "            if not np.isnan(px):\n",
    "                mtm += pos[\"shares\"] * px\n",
    "        total_equity = cash + mtm\n",
    "        equity_curve.append((dt, float(total_equity)))\n",
    "\n",
    "    eq_ser = pd.Series([e for _, e in equity_curve], index=[d for d, _ in equity_curve])\n",
    "    legs_df = pd.DataFrame(completed_legs).sort_values([\"date\", \"ticker\", \"side\"]).reset_index(drop=True)\n",
    "\n",
    "    # Build roundtrips\n",
    "    roundtrips = []\n",
    "    by_tkr_open = {}\n",
    "    for _, leg in legs_df.iterrows():\n",
    "        tkr = leg[\"ticker\"]\n",
    "        if leg[\"side\"] == \"BUY\":\n",
    "            by_tkr_open[tkr] = leg\n",
    "        else:\n",
    "            buy = by_tkr_open.pop(tkr, None)\n",
    "            if buy is None:\n",
    "                continue\n",
    "            fees_total = float(buy.get(\"fees_inr\", 0.0) + leg.get(\"fees_inr\", 0.0))\n",
    "            gross_pnl = (leg[\"price\"] - buy[\"price\"]) * buy[\"shares\"]\n",
    "            net_pnl   = gross_pnl - fees_total\n",
    "            ret_pct   = (leg[\"price\"] / buy[\"price\"] - 1.0) * 100.0\n",
    "            days_held = (pd.to_datetime(leg[\"date\"]) - pd.to_datetime(buy[\"date\"])).days\n",
    "            roundtrips.append({\n",
    "                \"ticker\": tkr,\n",
    "                \"entry_date\": pd.to_datetime(buy[\"date\"]),\n",
    "                \"entry_price\": float(buy[\"price\"]),\n",
    "                \"exit_date\": pd.to_datetime(leg[\"date\"]),\n",
    "                \"exit_price\": float(leg[\"price\"]),\n",
    "                \"days_held\": int(days_held),\n",
    "                \"shares\": int(buy[\"shares\"]),\n",
    "                \"entry_reason\": buy.get(\"reason\",\"\"),\n",
    "                \"exit_reason\": leg.get(\"reason\",\"\"),\n",
    "                \"gross_pnl_inr\": float(gross_pnl),\n",
    "                \"fees_total_inr\": float(fees_total),\n",
    "                \"net_pnl_inr\": float(net_pnl),\n",
    "                \"return_pct\": float(ret_pct),\n",
    "                \"close_entry\": float(buy.get(\"close\", np.nan)),\n",
    "                \"cloud_top_entry\": float(buy.get(\"cloud_top\", np.nan)),\n",
    "                \"cloud_bot_entry\": float(buy.get(\"cloud_bot\", np.nan)),\n",
    "                \"span_a_entry\": float(buy.get(\"span_a\", np.nan)),\n",
    "                \"span_b_entry\": float(buy.get(\"span_b\", np.nan)),\n",
    "                \"kijun_entry\": float(buy.get(\"kijun\", np.nan)),\n",
    "                \"tenkan_entry\": float(buy.get(\"tenkan\", np.nan)),\n",
    "                \"high_52w_entry\": float(buy.get(\"high_52w\", np.nan)),\n",
    "                \"volar_entry\": float(buy.get(\"volar\", np.nan)),\n",
    "                \"mvo_weight_entry\": float(buy.get(\"mvo_weight\", np.nan)),\n",
    "                \"alloc_inr_entry\": float(buy.get(\"alloc_inr\", np.nan))\n",
    "            })\n",
    "    trips_df = pd.DataFrame(roundtrips).sort_values([\"entry_date\",\"ticker\"]).reset_index(drop=True)\n",
    "\n",
    "    metrics = compute_metrics(eq_ser, legs_df)\n",
    "    return legs_df, trips_df, eq_ser, metrics\n",
    "\n",
    "def compute_metrics(equity: pd.Series, legs_df: pd.DataFrame):\n",
    "    out = {}\n",
    "    if equity is None or equity.empty:\n",
    "        return out\n",
    "    eq = equity.dropna()\n",
    "    daily_ret = eq.pct_change().fillna(0.0)\n",
    "\n",
    "    days = (eq.index[-1] - eq.index[0]).days or 1\n",
    "    years = days / 365.25\n",
    "    cagr = (eq.iloc[-1] / eq.iloc[0]) ** (1/years) - 1 if years > 0 else 0.0\n",
    "\n",
    "    sharpe = (daily_ret.mean() / daily_ret.std(ddof=0) * np.sqrt(252)) if daily_ret.std(ddof=0) > 0 else 0.0\n",
    "\n",
    "    cummax = eq.cummax()\n",
    "    dd = (eq - cummax) / cummax\n",
    "    max_dd = dd.min()\n",
    "\n",
    "    wins = 0\n",
    "    n_sells = legs_df[legs_df[\"side\"] == \"SELL\"].shape[0] if legs_df is not None and not legs_df.empty else 0\n",
    "    for _, r in legs_df[legs_df[\"side\"] == \"SELL\"].iterrows():\n",
    "        if float(r.get(\"pnl_inr\", 0.0)) > 0:\n",
    "            wins += 1\n",
    "    win_rate = (wins / n_sells) * 100.0 if n_sells > 0 else 0.0\n",
    "\n",
    "    out.update({\n",
    "        \"start_equity_inr\": float(eq.iloc[0]),\n",
    "        \"final_equity_inr\": float(eq.iloc[-1]),\n",
    "        \"cagr_pct\": float(cagr * 100),\n",
    "        \"sharpe\": float(sharpe),\n",
    "        \"max_drawdown_pct\": float(max_dd * 100),\n",
    "        \"win_rate_pct\": float(win_rate),\n",
    "        \"n_trades\": int(n_sells),\n",
    "    })\n",
    "    return out\n",
    "\n",
    "def plot_equity(equity: pd.Series, out_path: str):\n",
    "    if equity is None or equity.empty:\n",
    "        return\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.plot(equity.index, equity.values)\n",
    "        plt.title(\"Equity Curve\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Equity (INR)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_path)\n",
    "        plt.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def backtest(cfg: Config):\n",
    "    ensure_dirs(cfg.cache_dir, cfg.out_dir)\n",
    "    log.info(\"Universe: loading static symbols...\")\n",
    "    symbols = load_static_symbols(cfg.static_symbols, cfg.static_symbols_path)\n",
    "    log.info(\"Loaded %d symbols.\", len(symbols))\n",
    "\n",
    "    log.info(\"Data: fetching OHLCV from yfinance (adjusted)...\")\n",
    "    data_map = fetch_prices(symbols, cfg.start_date, cfg.end_date, cfg.cache_dir)\n",
    "    log.info(\"Downloaded %d symbols with data.\", len(data_map))\n",
    "\n",
    "    bench_tkr, bench_df = pick_benchmark(cfg.benchmark_try, cfg.start_date, cfg.end_date, cfg.cache_dir)\n",
    "    log.info(\"Benchmark selected: %s\", bench_tkr)\n",
    "\n",
    "    log.info(\"Signals: generating Ichimoku breakouts and candidate entries...\")\n",
    "    all_trades = []\n",
    "    for i, tkr in enumerate(symbols, 1):\n",
    "        df = data_map.get(tkr)\n",
    "        if df is None or df.empty:\n",
    "            continue\n",
    "        tr, _ = simulate_ticker(tkr, df, cfg)\n",
    "        if not tr.empty:\n",
    "            all_trades.append(tr)\n",
    "        if i % 50 == 0:\n",
    "            log.info(\"  processed %d/%d tickers...\", i, len(symbols))\n",
    "\n",
    "    if not all_trades:\n",
    "        log.warning(\"No signals generated; check universe/thresholds.\")\n",
    "        return None, None, None, {}\n",
    "    all_trades = pd.concat(all_trades, ignore_index=True)\n",
    "\n",
    "    log.info(\"Portfolio: cap daily deploy to %.0f%% of cash; 52w>=%.0f%% high; top-%d by VOLAᵣ; MVO; max %d positions.\",\n",
    "             cfg.deploy_cash_frac*100, cfg.within_pct_of_52w_high*100, cfg.top_k_daily, cfg.max_concurrent_positions)\n",
    "    legs_df, trips_df, equity, metrics = aggregate_and_apply(all_trades, data_map, bench_df, cfg)\n",
    "\n",
    "    stamp = pd.Timestamp.today(tz=\"Asia/Kolkata\").strftime(\"%Y%m%d_%H%M%S\")\n",
    "    legs_path = os.path.join(cfg.out_dir, f\"trades_legs_{stamp}.csv\")\n",
    "    trips_path = os.path.join(cfg.out_dir, f\"trades_roundtrips_{stamp}.csv\")\n",
    "    equity_path = os.path.join(cfg.out_dir, f\"equity_{stamp}.csv\")\n",
    "    metrics_path = os.path.join(cfg.out_dir, f\"metrics_{stamp}.json\")\n",
    "    eq_plot_path = os.path.join(cfg.out_dir, f\"equity_{stamp}.png\")\n",
    "\n",
    "    if legs_df is not None:\n",
    "        legs_df.to_csv(legs_path, index=False)\n",
    "    if trips_df is not None:\n",
    "        trips_df.to_csv(trips_path, index=False)\n",
    "    if equity is not None:\n",
    "        pd.DataFrame({\"date\": equity.index, \"equity\": equity.values}).to_csv(equity_path, index=False)\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    if cfg.plot and equity is not None:\n",
    "        plot_equity(equity, eq_plot_path)\n",
    "\n",
    "    log.info(\"=== METRICS ===\\n%s\", json.dumps(metrics, indent=2))\n",
    "    log.info(\"Files written:\\n  %s\\n  %s\\n  %s\\n  %s\", legs_path, trips_path, equity_path, metrics_path)\n",
    "    if cfg.plot:\n",
    "        log.info(\"  %s\", eq_plot_path)\n",
    "\n",
    "def main():\n",
    "    global APPLY_FEES\n",
    "    APPLY_FEES = bool(CFG.apply_fees)\n",
    "\n",
    "    CFG.static_symbols_path = \"nifty500.txt\"\n",
    "\n",
    "    # Example: turn on confirmations\n",
    "    CFG.use_ichimoku_confirms = False\n",
    "    CFG.use_volume_confirm = True\n",
    "\n",
    "    backtest(CFG)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
