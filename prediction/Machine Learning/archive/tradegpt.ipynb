{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "397257eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mn/tx231v5158797chnp22wm9000000gp/T/ipykernel_21176/330943430.py:132: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(symbol, interval=interval, period=period, progress=False)\n",
      "/var/folders/mn/tx231v5158797chnp22wm9000000gp/T/ipykernel_21176/330943430.py:168: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out[\"vol_z\"] = (out[\"Volume\"].replace(0, np.nan).rolling(96).apply(lambda x: (x[-1]-np.nanmean(x))/ (np.nanstd(x)+1e-9), raw=False))\n",
      "/Users/hemank/Documents/github/.talib/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=0.9667 | val_acc=0.5070 | val_f1=0.0000 | val_auc=0.5317 | val_mse=0.540099\n",
      "Epoch 02 | train_loss=0.9490 | val_acc=0.5070 | val_f1=0.0000 | val_auc=0.5294 | val_mse=0.484023\n",
      "Epoch 03 | train_loss=0.9486 | val_acc=0.5070 | val_f1=0.0000 | val_auc=0.5286 | val_mse=0.392227\n",
      "Epoch 04 | train_loss=0.8893 | val_acc=0.5070 | val_f1=0.0000 | val_auc=0.5238 | val_mse=0.281004\n",
      "Epoch 05 | train_loss=0.8581 | val_acc=0.4789 | val_f1=0.0000 | val_auc=0.5230 | val_mse=0.171366\n",
      "Epoch 06 | train_loss=0.8377 | val_acc=0.4789 | val_f1=0.0000 | val_auc=0.5167 | val_mse=0.090743\n",
      "Early stopping on val F1.\n",
      "Test: {\n",
      "  \"acc\": 0.450704,\n",
      "  \"f1\": 0.0,\n",
      "  \"auc\": 0.370994,\n",
      "  \"mse\": 0.07062,\n",
      "  \"mae\": 0.220208\n",
      "}\n",
      "Backtest stats: {\n",
      "  \"final_equity\": 73314.69910616174,\n",
      "  \"total_return_pct\": -26.68530089383826,\n",
      "  \"max_drawdown_pct\": -29.49551669723568,\n",
      "  \"sharpe\": -3.2812994309188803,\n",
      "  \"trades\": 66\n",
      "}\n",
      "Artifacts saved to: outputs/2025-11-03/011727_RELIANCE.NS_1d\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "TradeGPT — Transformer for Next‑Candle Prediction + Simple Backtester (No CLI)\n",
    "=============================================================================\n",
    "\n",
    "What this script does\n",
    "---------------------\n",
    "- Builds a **Transformer Encoder** to predict the **next candle** from recent OHLCV windows.\n",
    "- Two heads:\n",
    "  1) **Direction (classification):** probability next close > current close.\n",
    "  2) **Return (regression):** next‑close % change.\n",
    "- Chronological train/val/test split, leakage‑safe scaling.\n",
    "- **Simple backtester** on the test set that converts predictions ➜ positions ➜ equity.\n",
    "- Saves artifacts in `outputs/YYYY-MM-DD/HHMMSS_SYMBOL_INTERVAL/`:\n",
    "  - `metrics.json` (test metrics), `model.pt` (weights + scaler stats)\n",
    "  - `inference.json` (latest-window prediction)\n",
    "  - `trades.csv`, `equity_curve.csv` (backtest results)\n",
    "\n",
    "Quick start\n",
    "-----------\n",
    "1) Install: `pip install torch pandas numpy scikit-learn yfinance`\n",
    "2) Open this file and adjust the `Config` block below (symbol/interval/thresholds/costs).\n",
    "3) Run: `python tradegpt_transformer.py` (no command‑line args needed).\n",
    "\n",
    "Backtester logic (default)\n",
    "--------------------------\n",
    "- At bar *t* (end of window), generate `prob_up` and `next_pct_pred` for *t+1*.\n",
    "- **Signal:**\n",
    "  - Long if `prob_up >= long_threshold` (default 0.55)\n",
    "  - Short if `prob_up <= short_threshold` (default 0.45)\n",
    "  - Else Flat\n",
    "- **Execution:** enter at close[t] and exit at close[t+1] (1‑bar hold). This aligns with a\n",
    "  one‑step‑ahead prediction target. You can later swap to open[t+1] for stricter realism.\n",
    "- **P&L:**\n",
    "  - Long P&L% = (close[t+1]/close[t] - 1)\n",
    "  - Short P&L% = -(close[t+1]/close[t] - 1)\n",
    "  - Costs: applied per round‑trip on notional: `(cost_bps + slippage_bps)`\n",
    "    (default 2 bps cost + 3 bps slippage = 5 bps; tune to your venue). \n",
    "  - Position size = `capital * position_fraction` (default 1.0 = fully invested on signal)\n",
    "- **Annualization:** uses bars/day derived from `interval_to_bars_per_day`.\n",
    "\n",
    "Notes & extensions\n",
    "------------------\n",
    "- Add causal masks if you extend to decoder‑style models; encoder here only sees past window.\n",
    "- Try multi‑symbol training (symbol embeddings) and walk‑forward validation for production.\n",
    "- To plug into your Groww pipeline, connect `trades.csv` to your execution layer with guards\n",
    "  (max losers, daily stop, etc.).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, math, json, time, datetime as dt, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# yfinance optional; fallback to synthetic GBM so the script always runs\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    _HAS_YF = True\n",
    "except Exception:\n",
    "    _HAS_YF = False\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# -------------------------\n",
    "# Config (edit here)\n",
    "# -------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    symbol: str = \"RELIANCE.NS\"\n",
    "    interval: str = \"1d\"      # yfinance interval (1m, 5m, 15m, 1h, 1d)\n",
    "    period: str = \"730d\"       # how far back to fetch\n",
    "    seq_len: int = 64          # lookback window\n",
    "    pred_horizon: int = 1      # predict t+1\n",
    "\n",
    "    # Model\n",
    "    d_model: int = 128\n",
    "    nhead: int = 4\n",
    "    num_layers: int = 3\n",
    "    dim_feedforward: int = 256\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # Train\n",
    "    batch_size: int = 128\n",
    "    epochs: int = 20\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    warmup_steps: int = 100\n",
    "    bce_pos_weight: float = 1.0\n",
    "\n",
    "    # Split\n",
    "    val_ratio: float = 0.15\n",
    "    test_ratio: float = 0.15\n",
    "\n",
    "    # Backtest\n",
    "    initial_capital: float = 100_000.0\n",
    "    position_fraction: float = 1.0        # 0..1 of capital deployed when in position\n",
    "    long_threshold: float = 0.55          # prob up threshold\n",
    "    short_threshold: float = 0.45         # prob down threshold\n",
    "    cost_bps: float = 2.0                 # brokerage/taxes per round trip (basis points)\n",
    "    slippage_bps: float = 3.0             # simulated slippage per round trip (basis points)\n",
    "\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    out_dir: str = \"outputs\"\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "# -------------------------\n",
    "# Data utilities\n",
    "# -------------------------\n",
    "\n",
    "def load_ohlcv(symbol: str, interval: str, period: str) -> pd.DataFrame:\n",
    "    if _HAS_YF:\n",
    "        try:\n",
    "            df = yf.download(symbol, interval=interval, period=period, progress=False)\n",
    "            if isinstance(df.columns, pd.MultiIndex):\n",
    "                df.columns = [c[0].capitalize() for c in df.columns]\n",
    "            else:\n",
    "                df.columns = [c.capitalize() for c in df.columns]\n",
    "            df = df.rename(columns={\"Adj Close\": \"Adj_Close\"})\n",
    "            df = df.dropna().copy()\n",
    "            if len(df) > 0:\n",
    "                return df\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Synthetic fallback (GBM)\n",
    "    n = 10_000\n",
    "    dt_step = 1/96  # ~15m\n",
    "    mu, sigma = 0.12, 0.25\n",
    "    prices = [100.0]\n",
    "    for _ in range(n-1):\n",
    "        dW = np.sqrt(dt_step) * np.random.randn()\n",
    "        prices.append(prices[-1]*np.exp((mu-0.5*sigma**2)*dt_step + sigma*dW))\n",
    "    prices = np.array(prices)\n",
    "    high = prices * (1 + np.random.rand(n)*0.003)\n",
    "    low = prices * (1 - np.random.rand(n)*0.003)\n",
    "    open_ = prices * (1 + (np.random.rand(n)-0.5)*0.001)\n",
    "    close = prices\n",
    "    vol = np.random.lognormal(mean=12, sigma=0.5, size=n)\n",
    "    idx = pd.date_range(end=pd.Timestamp.utcnow(), periods=n, freq=\"15min\")\n",
    "    return pd.DataFrame({\"Open\": open_, \"High\": high, \"Low\": low, \"Close\": close, \"Volume\": vol}, index=idx)\n",
    "\n",
    "\n",
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"logret\"] = np.log(out[\"Close\"].pct_change().add(1))\n",
    "    out[\"range\"] = (out[\"High\"] - out[\"Low\"]) / out[\"Close\"].shift(1)\n",
    "    out[\"body\"] = (out[\"Close\"] - out[\"Open\"]) / out[\"Open\"]\n",
    "    out[\"upper_tail\"] = (out[\"High\"] - out[[\"Open\",\"Close\"]].max(axis=1)) / out[\"Close\"].shift(1)\n",
    "    out[\"lower_tail\"] = (out[[\"Open\",\"Close\"]].min(axis=1) - out[\"Low\"]) / out[\"Close\"]\n",
    "    out[\"vol_z\"] = (out[\"Volume\"].replace(0, np.nan).rolling(96).apply(lambda x: (x[-1]-np.nanmean(x))/ (np.nanstd(x)+1e-9), raw=False))\n",
    "    out = out.dropna().copy()\n",
    "    return out\n",
    "\n",
    "\n",
    "def make_supervised(df: pd.DataFrame, seq_len: int, pred_horizon: int):\n",
    "    feats = [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"logret\",\"range\",\"body\",\"upper_tail\",\"lower_tail\",\"vol_z\"]\n",
    "    arr = df[feats].values.astype(np.float32)\n",
    "    close = df[\"Close\"].values.astype(np.float32)\n",
    "    X, y_cls, y_reg = [], [], []\n",
    "    for i in range(seq_len, len(df)-pred_horizon):\n",
    "        X.append(arr[i-seq_len:i])\n",
    "        curr_close = close[i-1]\n",
    "        next_close = close[i+pred_horizon-1]\n",
    "        y_cls.append(1.0 if next_close > curr_close else 0.0)\n",
    "        y_reg.append((next_close / curr_close) - 1.0)\n",
    "    X = np.stack(X)\n",
    "    y_cls = np.array(y_cls, dtype=np.float32)\n",
    "    y_reg = np.array(y_reg, dtype=np.float32)\n",
    "    idx = df.index[seq_len:len(df)-pred_horizon]\n",
    "    return X, y_cls, y_reg, df.loc[idx]\n",
    "\n",
    "# -------------------------\n",
    "# Dataset / Loader\n",
    "# -------------------------\n",
    "class CandleDataset(Dataset):\n",
    "    def __init__(self, X, y_cls, y_reg):\n",
    "        self.X = X\n",
    "        self.yc = y_cls\n",
    "        self.yr = y_reg\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return (\n",
    "            torch.from_numpy(self.X[i]),\n",
    "            torch.tensor(self.yc[i]).view(1),\n",
    "            torch.tensor(self.yr[i]).view(1)\n",
    "        )\n",
    "\n",
    "# -------------------------\n",
    "# Model\n",
    "# -------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 10_000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10_000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TradeGPT(nn.Module):\n",
    "    def __init__(self, in_dim: int, d_model: int, nhead: int, num_layers: int, dim_feedforward: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(in_dim, d_model)\n",
    "        self.pos = PositionalEncoding(d_model, dropout=dropout)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, activation='gelu', batch_first=True, norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.cls_head = nn.Linear(d_model, 1)\n",
    "        self.reg_head = nn.Linear(d_model, 1)\n",
    "    def forward(self, x):\n",
    "        h = self.proj(x)\n",
    "        h = self.pos(h)\n",
    "        h = self.encoder(h)\n",
    "        h_last = self.norm(h[:, -1, :])\n",
    "        logit = self.cls_head(h_last)\n",
    "        reg = self.reg_head(h_last)\n",
    "        return logit.squeeze(-1), reg.squeeze(-1)\n",
    "\n",
    "# -------------------------\n",
    "# Train / Eval helpers\n",
    "# -------------------------\n",
    "\n",
    "def cosine_warmup(step, warmup, total_steps):\n",
    "    if step < warmup:\n",
    "        return step / max(1, warmup)\n",
    "    progress = (step - warmup) / max(1, total_steps - warmup)\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "class SequenceScaler:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "    def fit(self, X: np.ndarray):\n",
    "        N, T, F = X.shape\n",
    "        self.scaler.fit(X.reshape(N*T, F))\n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        N, T, F = X.shape\n",
    "        Y = self.scaler.transform(X.reshape(N*T, F))\n",
    "        return Y.reshape(N, T, F)\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, opt, sched, bce, mse, device):\n",
    "    model.train()\n",
    "    total, n = 0.0, 0\n",
    "    for xb, yb_cls, yb_reg in loader:\n",
    "        xb = xb.to(device)\n",
    "        yb_cls = yb_cls.to(device).view(-1)\n",
    "        yb_reg = yb_reg.to(device).view(-1)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logit, reg = model(xb)\n",
    "        loss = bce(logit, yb_cls) + 0.5 * mse(reg, yb_reg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # cosine lr w/ warmup\n",
    "        lr_scale = cosine_warmup(sched[\"step\"], sched[\"warmup\"], sched[\"total\"]) \n",
    "        for g in opt.param_groups:\n",
    "            g['lr'] = g.get('base_lr', g['lr']) * lr_scale\n",
    "        opt.step()\n",
    "        total += loss.item(); n += 1\n",
    "        sched[\"step\"] += 1\n",
    "    return total / max(1, n)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    logits, regs, yc, yr = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb_cls, yb_reg in loader:\n",
    "            xb = xb.to(device)\n",
    "            logit, reg = model(xb)\n",
    "            logits.append(logit.cpu()); regs.append(reg.cpu())\n",
    "            yc.append(yb_cls.view(-1)); yr.append(yb_reg.view(-1))\n",
    "    logits = torch.cat(logits).numpy(); regs = torch.cat(regs).numpy()\n",
    "    yc = torch.cat(yc).numpy(); yr = torch.cat(yr).numpy()\n",
    "    prob = 1/(1+np.exp(-logits)); pred = (prob >= 0.5).astype(int)\n",
    "    acc = float(accuracy_score(yc, pred)); f1 = float(f1_score(yc, pred))\n",
    "    try:\n",
    "        auc = float(roc_auc_score(yc, prob))\n",
    "    except Exception:\n",
    "        auc = float('nan')\n",
    "    mse_ = float(mean_squared_error(yr, regs)); mae_ = float(mean_absolute_error(yr, regs))\n",
    "    return {\"acc\":acc, \"f1\":f1, \"auc\":auc, \"mse\":mse_, \"mae\":mae_,\n",
    "            \"prob\":prob, \"pred_cls\":pred, \"pred_reg\":regs, \"y_cls\":yc, \"y_reg\":yr}\n",
    "\n",
    "# -------------------------\n",
    "# Backtester\n",
    "# -------------------------\n",
    "\n",
    "def interval_to_bars_per_day(interval: str) -> int:\n",
    "    if interval.endswith('d'):\n",
    "        return 1\n",
    "    if interval in {\"15m\", \"15min\"}:\n",
    "        return 25  # ~375 minutes / 15\n",
    "    if interval in {\"5m\", \"5min\"}:\n",
    "        return 75\n",
    "    if interval in {\"1h\", \"60m\"}:\n",
    "        return 6.25  # approx; will floor where needed\n",
    "    return 25\n",
    "\n",
    "\n",
    "def backtest_on_test_set(aligned_df: pd.DataFrame, test_idx_slice: slice, preds: dict, cfg: Config, run_dir: str):\n",
    "    \"\"\"One‑bar ahead hold: enter at close[t], exit at close[t+1].\"\"\"\n",
    "    # aligned_df aligns with the supervised windows (index length = total samples)\n",
    "    # Split indices\n",
    "    n_total = len(aligned_df)\n",
    "    start = test_idx_slice.start\n",
    "    end = test_idx_slice.stop  # exclusive\n",
    "\n",
    "    closes = aligned_df[\"Close\"].values.astype(float)\n",
    "    idx = aligned_df.index\n",
    "\n",
    "    prob = preds[\"prob\"][start:end]\n",
    "    next_ret_true = preds[\"y_reg\"][start:end]   # realized next % change\n",
    "\n",
    "    capital = cfg.initial_capital\n",
    "    equity = []\n",
    "    rows = []\n",
    "\n",
    "    notional = capital * cfg.position_fraction\n",
    "    roundtrip_cost = (cfg.cost_bps + cfg.slippage_bps) / 10_000.0\n",
    "\n",
    "    for i in range(start, end-1):  # need i+1 for exit\n",
    "        ts = idx[i]\n",
    "        p_up = float(prob[i-start])\n",
    "        # Signal\n",
    "        if p_up >= cfg.long_threshold:\n",
    "            side = 1\n",
    "        elif p_up <= cfg.short_threshold:\n",
    "            side = -1\n",
    "        else:\n",
    "            side = 0\n",
    "        # Realized one‑bar return\n",
    "        r = (closes[i+1] / closes[i]) - 1.0\n",
    "        trade_ret = side * r\n",
    "        # Apply costs only if we actually traded (non‑flat)\n",
    "        cost = notional * roundtrip_cost if side != 0 else 0.0\n",
    "        pnl = notional * trade_ret - cost\n",
    "        capital += pnl\n",
    "        equity.append([ts, capital])\n",
    "        rows.append({\n",
    "            \"time\": ts, \"prob_up\": p_up, \"signal\": side,\n",
    "            \"ret_next_bar\": r, \"trade_ret\": trade_ret,\n",
    "            \"pnl\": pnl, \"equity\": capital\n",
    "        })\n",
    "\n",
    "    eq_df = pd.DataFrame(equity, columns=[\"time\",\"equity\"]).set_index(\"time\")\n",
    "    tr_df = pd.DataFrame(rows)\n",
    "\n",
    "    # Metrics\n",
    "    rets = tr_df[\"trade_ret\"].fillna(0.0).values\n",
    "    bars_per_day = interval_to_bars_per_day(CFG.interval)\n",
    "    bars_per_year = int(252 * bars_per_day)\n",
    "    # Equity returns based on capital changes\n",
    "    eq_returns = eq_df[\"equity\"].pct_change().fillna(0.0).values\n",
    "    cum_return = float((eq_df[\"equity\"].iloc[-1] / cfg.initial_capital) - 1.0)\n",
    "    avg = float(np.mean(eq_returns))\n",
    "    std = float(np.std(eq_returns) + 1e-12)\n",
    "    sharpe = float((avg / std) * np.sqrt(bars_per_year)) if std > 0 else float('nan')\n",
    "    max_dd = 0.0\n",
    "    peak = -1e18\n",
    "    for v in eq_df[\"equity\"].values:\n",
    "        peak = max(peak, v)\n",
    "        max_dd = min(max_dd, (v/peak)-1.0)\n",
    "    stats = {\n",
    "        \"final_equity\": float(eq_df[\"equity\"].iloc[-1]),\n",
    "        \"total_return_pct\": 100*cum_return,\n",
    "        \"max_drawdown_pct\": 100*max_dd,\n",
    "        \"sharpe\": sharpe,\n",
    "        \"trades\": int((tr_df[\"signal\"]!=0).sum())\n",
    "    }\n",
    "\n",
    "    eq_df.to_csv(os.path.join(run_dir, \"equity_curve.csv\"))\n",
    "    tr_df.to_csv(os.path.join(run_dir, \"trades.csv\"), index=False)\n",
    "    with open(os.path.join(run_dir, \"bt_stats.json\"), \"w\") as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    return stats\n",
    "\n",
    "# -------------------------\n",
    "# Main pipeline\n",
    "# -------------------------\n",
    "\n",
    "def main(cfg: Config):\n",
    "    ts = dt.datetime.now().strftime(\"%Y-%m-%d/%H%M%S\")\n",
    "    run_dir = os.path.join(cfg.out_dir, ts + f\"_{cfg.symbol}_{cfg.interval}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    # 1) Data & features\n",
    "    raw = load_ohlcv(cfg.symbol, cfg.interval, cfg.period)\n",
    "    df = engineer_features(raw)\n",
    "\n",
    "    # 2) Supervised windows\n",
    "    X, y_cls, y_reg, aligned = make_supervised(df, cfg.seq_len, cfg.pred_horizon)\n",
    "\n",
    "    # 3) Chrono split\n",
    "    n = len(X)\n",
    "    n_test = int(n * cfg.test_ratio)\n",
    "    n_val  = int(n * cfg.val_ratio)\n",
    "    n_train = n - n_val - n_test\n",
    "    assert n_train > 100, \"Not enough samples after split; reduce seq_len or increase period.\"\n",
    "\n",
    "    X_tr, X_va, X_te = X[:n_train], X[n_train:n_train+n_val], X[n_train+n_val:]\n",
    "    yc_tr, yc_va, yc_te = y_cls[:n_train], y_cls[n_train:n_train+n_val], y_cls[n_train+n_val:]\n",
    "    yr_tr, yr_va, yr_te = y_reg[:n_train], y_reg[n_train:n_train+n_val], y_reg[n_train+n_val:]\n",
    "\n",
    "    # 4) Scale using TRAIN only\n",
    "    scaler = SequenceScaler(); scaler.fit(X_tr)\n",
    "    X_tr = scaler.transform(X_tr); X_va = scaler.transform(X_va); X_te = scaler.transform(X_te)\n",
    "\n",
    "    # 5) Loaders\n",
    "    train_ds = CandleDataset(X_tr, yc_tr, yr_tr)\n",
    "    val_ds   = CandleDataset(X_va, yc_va, yr_va)\n",
    "    test_ds  = CandleDataset(X_te, yc_te, yr_te)\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "    # 6) Model\n",
    "    in_dim = X.shape[-1]\n",
    "    model = TradeGPT(in_dim, cfg.d_model, cfg.nhead, cfg.num_layers, cfg.dim_feedforward, cfg.dropout).to(cfg.device)\n",
    "\n",
    "    # 7) Loss & optim\n",
    "    bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(cfg.bce_pos_weight, device=cfg.device))\n",
    "    mse = nn.MSELoss()\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    for g in opt.param_groups: g['base_lr'] = cfg.lr\n",
    "    total_steps = cfg.epochs * max(1, len(train_loader))\n",
    "    sched = {\"step\": 0, \"warmup\": cfg.warmup_steps, \"total\": total_steps}\n",
    "\n",
    "    # 8) Train with early stop on val F1\n",
    "    best_f1, best_state, no_imp, patience = -1.0, None, 0, 5\n",
    "    for ep in range(1, cfg.epochs+1):\n",
    "        tr_loss = train_epoch(model, train_loader, opt, sched, bce, mse, cfg.device)\n",
    "        val = evaluate(model, val_loader, cfg.device)\n",
    "        print(f\"Epoch {ep:02d} | train_loss={tr_loss:.4f} | val_acc={val['acc']:.4f} | val_f1={val['f1']:.4f} | val_auc={val['auc']:.4f} | val_mse={val['mse']:.6f}\")\n",
    "        if val['f1'] > best_f1:\n",
    "            best_f1 = val['f1']; best_state = {k:v.cpu() for k,v in model.state_dict().items()}; no_imp = 0\n",
    "        else:\n",
    "            no_imp += 1\n",
    "            if no_imp >= patience:\n",
    "                print(\"Early stopping on val F1.\"); break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # 9) Test metrics\n",
    "    test = evaluate(model, test_loader, cfg.device)\n",
    "    print(\"Test:\", json.dumps({k: round(test[k],6) for k in ['acc','f1','auc','mse','mae']}, indent=2))\n",
    "\n",
    "    # 10) Save artifacts\n",
    "    meta = {\n",
    "        \"symbol\": cfg.symbol, \"interval\": cfg.interval, \"period\": cfg.period,\n",
    "        \"seq_len\": cfg.seq_len, \"pred_horizon\": cfg.pred_horizon,\n",
    "        \"metrics\": {k: float(test[k]) if isinstance(test[k], float) else test[k] for k in [\"acc\",\"f1\",\"auc\",\"mse\",\"mae\"]}\n",
    "    }\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    with open(os.path.join(run_dir, \"metrics.json\"), \"w\") as f: json.dump(meta, f, indent=2)\n",
    "    torch.save({\"state_dict\": model.state_dict(), \"scaler_mean\": scaler.scaler.mean_.tolist(), \"scaler_scale\": scaler.scaler.scale_.tolist()}, os.path.join(run_dir, \"model.pt\"))\n",
    "\n",
    "    # 11) Inference (latest window)\n",
    "    latest = X[-1:]\n",
    "    latest_scaled = scaler.transform(latest)\n",
    "    model.eval();\n",
    "    with torch.no_grad():\n",
    "        xb = torch.from_numpy(latest_scaled).to(cfg.device)\n",
    "        logit, reg = model(xb)\n",
    "        prob_up = float(torch.sigmoid(logit).cpu().item())\n",
    "        next_pct = float(reg.cpu().item())\n",
    "    last_close = float(df[\"Close\"].iloc[-1])\n",
    "    next_close_pred = last_close * (1.0 + next_pct)\n",
    "    inf = {\"prob_next_close_up\": prob_up, \"predicted_next_close\": next_close_pred, \"last_close\": last_close, \"predicted_next_close_pct_change\": next_pct, \"timestamp_last\": str(df.index[-1])}\n",
    "    with open(os.path.join(run_dir, \"inference.json\"), \"w\") as f: json.dump(inf, f, indent=2)\n",
    "\n",
    "    # 12) Backtest on the test slice\n",
    "    # Map the test slice into the aligned index space\n",
    "    start = n - (n_test + n_val)\n",
    "    test_slice = slice(n_train + n_val - n_train, n)  # relative slice for preds, absolute below\n",
    "    # But our evaluate(test_loader) returned arrays matching X_te order; we need absolute indices:\n",
    "    # Absolute start index within aligned df for test set\n",
    "    abs_start = n_train + n_val\n",
    "    abs_end = n\n",
    "    preds_all = {\n",
    "        \"prob\": np.concatenate([evaluate(model, DataLoader(CandleDataset(X_tr, yc_tr, yr_tr), batch_size=cfg.batch_size, shuffle=False), cfg.device)[\"prob\"],\n",
    "                                 evaluate(model, val_loader, cfg.device)[\"prob\"],\n",
    "                                 test[\"prob\"]]),\n",
    "        \"y_reg\": np.concatenate([evaluate(model, DataLoader(CandleDataset(X_tr, yc_tr, yr_tr), batch_size=cfg.batch_size, shuffle=False), cfg.device)[\"y_reg\"],\n",
    "                                  evaluate(model, val_loader, cfg.device)[\"y_reg\"],\n",
    "                                  test[\"y_reg\"]])\n",
    "    }\n",
    "    bt_stats = backtest_on_test_set(aligned, slice(abs_start, abs_end), preds_all, cfg, run_dir)\n",
    "    print(\"Backtest stats:\", json.dumps(bt_stats, indent=2))\n",
    "\n",
    "    print(f\"Artifacts saved to: {run_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(CFG)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".talib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
