{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI News Sentiment Agent (v3)\n",
        "Fixes for empty results + broader, smarter sources.\n",
        "\n",
        "Key changes vs v2:\n",
        "- Title **and** summary keyword match for generic feeds (was title-only)\n",
        "- More general feeds: ET IT/ITES, Business Standard Companies, CNBC-TV18 Latest\n",
        "- Optional NSE Corporate Announcements JSON (with cookie priming)\n",
        "- Retry + FORCE_REFRESH\n",
        "\n",
        "**Outputs** remain the same: per-query CSVs + a daily `bias_ledger.csv` under `outputs/YYYY-MM-DD/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "name": "CONFIG"
      },
      "outputs": [],
      "source": [
        "## =============== CONFIG — Edit only this cell ===============\n",
        "QUERIES = [\n",
        "    \"TCS.NS\",  # add more: \"HDFCBANK.NS\", \"Reliance Industries\", etc.\n",
        "]\n",
        "DAYS = 5\n",
        "MAX_ARTICLES = 25\n",
        "ENABLE_TELEGRAM = False\n",
        "TELEGRAM_BOT_TOKEN = \"\"\n",
        "TELEGRAM_CHAT_ID = \"\"\n",
        "OUTPUT_BASE_DIR = \"outputs\"\n",
        "\n",
        "# Decision thresholds\n",
        "MIN_ARTICLES_FOR_STRONG_VIEW = 3\n",
        "LONG_THRESHOLD = +0.15\n",
        "SHORT_THRESHOLD = -0.15\n",
        "TAU_DAYS = 3.0\n",
        "SOURCE_WEIGHTS = {\n",
        "    \"moneycontrol.com\": 1.15,\n",
        "    \"economictimes.indiatimes.com\": 1.10,\n",
        "    \"livemint.com\": 1.05,\n",
        "    \"business-standard.com\": 1.05,\n",
        "    \"reuters.com\": 1.20,\n",
        "    \"bloomberg.com\": 1.20,\n",
        "    \"cnbctv18.com\": 1.05,\n",
        "}\n",
        "DEFAULT_SOURCE_WEIGHT = 1.0\n",
        "\n",
        "# Advanced\n",
        "FORCE_REFRESH = True   # set True for a clean run if you saw 0 articles\n",
        "MAX_RSS_PER_SOURCE = 60  # cap items consumed from each generic feed before alias filtering\n",
        "RETRY = 2\n",
        "\n",
        "# Broad finance/IT feeds (we keyword-filter by alias)\n",
        "GENERAL_FEEDS = [\n",
        "    \"https://feeds.reuters.com/reuters/INtopNews\",\n",
        "    \"https://feeds.reuters.com/reuters/businessNews\",\n",
        "    \"https://www.moneycontrol.com/rss/MCtopnews.xml\",\n",
        "    \"https://www.livemint.com/rss/companies\",\n",
        "    # Added:\n",
        "    \"https://economictimes.indiatimes.com/industry/it/ites/rssfeeds/13357270.cms\",\n",
        "    \"https://www.business-standard.com/rss/companies-101.rss\",\n",
        "    \"https://www.cnbctv18.com/rss/latest.xml\",\n",
        "]\n",
        "\n",
        "# NSE Corporate Announcements fetch (best-effort)\n",
        "ENABLE_NSE_CORP = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install (first run) — optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install feedparser requests beautifulsoup4 lxml readability-lxml html5lib numpy pandas python-dateutil yfinance tqdm transformers torch --upgrade\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports & helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, re, math, hashlib, json, time\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from dateutil import tz, parser as dateparser\n",
        "from urllib.parse import quote_plus, urlparse, parse_qs\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import feedparser\n",
        "from bs4 import BeautifulSoup\n",
        "from readability import Document\n",
        "from tqdm import tqdm\n",
        "import yfinance as yf\n",
        "\n",
        "UA = (\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
        "      \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "      \"Chrome/122.0.0.0 Safari/537.36\")\n",
        "HDRS = {\"User-Agent\": UA, \"Accept-Language\": \"en-IN,en;q=0.9\"}\n",
        "\n",
        "def ist_now():\n",
        "    return datetime.now(tz.gettz(\"Asia/Kolkata\"))\n",
        "\n",
        "def to_ist(dt: datetime) -> datetime:\n",
        "    if dt.tzinfo is None:\n",
        "        dt = dt.replace(tzinfo=timezone.utc)\n",
        "    return dt.astimezone(tz.gettz(\"Asia/Kolkata\"))\n",
        "\n",
        "def safe_filename(s: str, maxlen=140) -> str:\n",
        "    s = re.sub(r\"[^a-zA-Z0-9._-]+\", \"_\", s)\n",
        "    return s[:maxlen]\n",
        "\n",
        "def domain_of(url: str) -> str:\n",
        "    try:\n",
        "        netloc = urlparse(url).netloc.lower()\n",
        "        parts = netloc.split(\".\")\n",
        "        return \".\".join(parts[-3:]) if len(parts) >= 3 else netloc\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "CACHE_DIR = \".cache_news_agent_v3\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "def read_cache(key: str):\n",
        "    path = os.path.join(CACHE_DIR, f\"{safe_filename(key)}.json\")\n",
        "    if os.path.exists(path) and not FORCE_REFRESH:\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                return json.load(f)\n",
        "        except Exception:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def write_cache(key: str, data):\n",
        "    path = os.path.join(CACHE_DIR, f\"{safe_filename(key)}.json\")\n",
        "    try:\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data, f, ensure_ascii=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def get_with_retries(session, url, headers=None, timeout=12, tries=RETRY):\n",
        "    last = None\n",
        "    for i in range(max(1, tries)):\n",
        "        try:\n",
        "            r = session.get(url, headers=headers or HDRS, timeout=timeout, allow_redirects=True)\n",
        "            if r.status_code == 200:\n",
        "                return r\n",
        "            last = r\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "        time.sleep(0.4)\n",
        "    if isinstance(last, requests.Response):\n",
        "        raise requests.HTTPError(f\"HTTP {last.status_code} for {url}\")\n",
        "    raise RuntimeError(f\"Failed to GET {url}: {last}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Query building & alias expansion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def resolve_company_query(q: str) -> str:\n",
        "    q = q.strip()\n",
        "    if re.search(r\"[A-Za-z]{1,5}\\.NS$\", q) or re.search(r\"^[A-Za-z.\\-]{1,12}$\", q):\n",
        "        try:\n",
        "            info = yf.Ticker(q).info\n",
        "            long_name = info.get(\"longName\") or info.get(\"shortName\")\n",
        "            if long_name:\n",
        "                return long_name\n",
        "        except Exception:\n",
        "            pass\n",
        "    return q\n",
        "\n",
        "def expand_aliases(q: str):\n",
        "    aliases = set()\n",
        "    q = q.strip()\n",
        "    aliases.add(q)\n",
        "    m = re.match(r\"^([A-Za-z0-9]+)\\.NS$\", q)\n",
        "    if m:\n",
        "        aliases.add(m.group(1))\n",
        "        ln = resolve_company_query(q)\n",
        "        aliases.add(ln)\n",
        "        aliases.add(re.sub(r\"\\b[Ll]imited\\b\", \"\", ln).strip())\n",
        "        aliases.add(\"Tata Consultancy Services\") if \"Tata Consultancy Services\" not in aliases else None\n",
        "        aliases.add(\"TCS\") if \"TCS\" not in aliases else None\n",
        "    else:\n",
        "        words = [w for w in re.split(r\"\\W+\", q) if w]\n",
        "        if 2 <= len(words) <= 5:\n",
        "            initials = ''.join(w[0].upper() for w in words)\n",
        "            if len(initials) >= 2:\n",
        "                aliases.add(initials)\n",
        "    return [a for a in aliases if a]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RSS builders (Google, Bing, Site feeds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def google_news_rss_urls(company: str, days: int):\n",
        "    return [f\"https://news.google.com/rss/search?q={quote_plus(company)}+when:{days}d&hl=en-IN&gl=IN&ceid=IN:en\"]\n",
        "\n",
        "def bing_news_rss_urls(company: str):\n",
        "    return [f\"https://www.bing.com/news/search?q={quote_plus(company)}&qft=sortbydate%3d%221%22&form=RSSMHL&format=RSS\"]\n",
        "\n",
        "GENERAL_FEEDS_LOCAL = []  # wired from CONFIG later\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fetch & clean helpers (improved)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_gnews_link(link: str) -> str:\n",
        "    if 'news.google.' in link:\n",
        "        try:\n",
        "            qs = parse_qs(urlparse(link).query)\n",
        "            if 'url' in qs and qs['url']:\n",
        "                return qs['url'][0]\n",
        "        except Exception:\n",
        "            pass\n",
        "    return link\n",
        "\n",
        "def fetch_rss_entries(url: str):\n",
        "    key = f\"rss::{url}\"\n",
        "    cached = read_cache(key)\n",
        "    if isinstance(cached, list) and len(cached) > 0:\n",
        "        return cached\n",
        "    feed = feedparser.parse(url)\n",
        "    entries = []\n",
        "    for e in feed.entries[:MAX_RSS_PER_SOURCE]:\n",
        "        link = e.get(\"link\") or \"\"\n",
        "        if (not link) and e.get('links'):\n",
        "            for L in e['links']:\n",
        "                if L.get('type','').startswith('text/html') and L.get('href'):\n",
        "                    link = L['href']\n",
        "                    break\n",
        "        link = normalize_gnews_link(link)\n",
        "        title = e.get(\"title\", \"\")\n",
        "        summary = e.get(\"summary\", \"\")\n",
        "        published = e.get(\"published\") or e.get(\"updated\") or \"\"\n",
        "        entries.append({\"title\": title, \"summary\": summary, \"link\": link, \"published\": published})\n",
        "    if entries:\n",
        "        write_cache(key, entries)\n",
        "    return entries\n",
        "\n",
        "def parse_pubdate(published: str) -> datetime:\n",
        "    try:\n",
        "        dt = dateparser.parse(published)\n",
        "        return to_ist(dt) if dt else ist_now()\n",
        "    except Exception:\n",
        "        return ist_now()\n",
        "\n",
        "def fetch_article_text(url: str, timeout=12):\n",
        "    key = f\"page::{hashlib.sha256(url.encode('utf-8')).hexdigest()[:16]}\"\n",
        "    cached = read_cache(key)\n",
        "    if cached:\n",
        "        return cached.get(\"text\", \"\"), cached.get(\"title\", \"\")\n",
        "    try:\n",
        "        sess = requests.Session()\n",
        "        r = get_with_retries(sess, url, headers=HDRS, timeout=timeout)\n",
        "        html = r.text\n",
        "        doc = Document(html)\n",
        "        cleaned_html = doc.summary()\n",
        "        title = doc.short_title() or \"\"\n",
        "        soup = BeautifulSoup(cleaned_html, \"lxml\")\n",
        "        text = soup.get_text(\"\\n\", strip=True)\n",
        "        if len(text) < 400:\n",
        "            soup_full = BeautifulSoup(html, \"lxml\")\n",
        "            for tag in soup_full([\"script\", \"style\", \"noscript\"]):\n",
        "                tag.decompose()\n",
        "            text2 = soup_full.get_text(\"\\n\", strip=True)\n",
        "            if len(text2) > len(text):\n",
        "                text = text2\n",
        "        write_cache(key, {\"text\": text, \"title\": title})\n",
        "        return text, title\n",
        "    except Exception:\n",
        "        return \"\", \"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: NSE Corporate Announcements (JSON)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_nse_corp(symbol: str, days: int):\n",
        "    if not ENABLE_NSE_CORP:\n",
        "        return []\n",
        "    try:\n",
        "        sess = requests.Session()\n",
        "        home = get_with_retries(sess, \"https://www.nseindia.com/\")\n",
        "        hdrs = HDRS.copy(); hdrs[\"Referer\"] = \"https://www.nseindia.com/\"\n",
        "        url = f\"https://www.nseindia.com/api/corporate-announcements?index=equities&symbol={symbol}\"\n",
        "        r = get_with_retries(sess, url, headers=hdrs, timeout=12)\n",
        "        j = r.json()\n",
        "        out = []\n",
        "        now = ist_now()\n",
        "        for row in j.get('data', []):\n",
        "            dt_str = row.get('sm_dt') or row.get('dt') or row.get('dissemDT') or ''\n",
        "            pub = parse_pubdate(dt_str)\n",
        "            if (now - pub).days <= days:\n",
        "                title = row.get('sm_desc') or row.get('desc') or row.get('subject') or 'NSE Corporate Announcement'\n",
        "                link = row.get('attchmntFile') or row.get('pdf') or ''\n",
        "                if link and not link.startswith('http'):\n",
        "                    link = 'https://nsearchives.nseindia.com' + link\n",
        "                out.append({\"title\": title, \"summary\": row.get('more',''), \"link\": link, \"published\": pub.isoformat()})\n",
        "        return out\n",
        "    except Exception:\n",
        "        return []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sentiment, aggregation & decision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "_pipe = None\n",
        "def load_model():\n",
        "    global _pipe\n",
        "    if _pipe is None:\n",
        "        from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "        tok = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "        mdl = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
        "        _pipe = pipeline(\"text-classification\", model=mdl, tokenizer=tok, return_all_scores=True, truncation=True)\n",
        "    return _pipe\n",
        "\n",
        "def score_sentiment_finbert(text: str):\n",
        "    pipe = load_model()\n",
        "    if not text:\n",
        "        return {\"positive\": 0.0, \"neutral\": 1.0, \"negative\": 0.0}\n",
        "    res = pipe(text[:4000])\n",
        "    scores = res[0]\n",
        "    out = {\"positive\": 0.0, \"neutral\": 0.0, \"negative\": 0.0}\n",
        "    for s in scores:\n",
        "        out[s[\"label\"].lower()] = float(s[\"score\"])\n",
        "    return out\n",
        "\n",
        "def article_base_score(sentiment: dict) -> float:\n",
        "    return float(sentiment.get(\"positive\", 0.0) - sentiment.get(\"negative\", 0.0))\n",
        "\n",
        "def recency_weight(pub_dt: datetime, now_ist: datetime, tau_days: float) -> float:\n",
        "    age_days = max((now_ist - pub_dt).total_seconds() / 86400.0, 0.0)\n",
        "    return math.exp(-age_days / tau_days)\n",
        "\n",
        "def source_weight(url: str) -> float:\n",
        "    dom = domain_of(url)\n",
        "    for d, w in SOURCE_WEIGHTS.items():\n",
        "        if d in dom:\n",
        "            return w\n",
        "    return DEFAULT_SOURCE_WEIGHT\n",
        "\n",
        "def aggregate_scores(rows: pd.DataFrame):\n",
        "    if rows.empty:\n",
        "        return 0.0, 0.0\n",
        "    weights = rows[\"recency_w\"] * rows[\"source_w\"]\n",
        "    agg = (rows[\"base_score\"] * weights).sum() / (weights.sum() + 1e-9)\n",
        "    n = len(rows)\n",
        "    dispersion = float(np.std(rows[\"base_score\"])) if n > 1 else 0.0\n",
        "    conf = max(0.0, min(1.0, (math.log1p(n) / 3.0) * (1.0 - min(1.0, dispersion))))\n",
        "    return float(agg), float(conf)\n",
        "\n",
        "def final_bias(agg_score: float, n_articles: int) -> str:\n",
        "    if n_articles >= MIN_ARTICLES_FOR_STRONG_VIEW:\n",
        "        if agg_score >= LONG_THRESHOLD:\n",
        "            return \"LONG\"\n",
        "        if agg_score <= SHORT_THRESHOLD:\n",
        "            return \"SHORT\"\n",
        "    if agg_score > 0.05:\n",
        "        return \"LEAN LONG\"\n",
        "    if agg_score < -0.05:\n",
        "        return \"LEAN SHORT\"\n",
        "    return \"NEUTRAL\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Telegram (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def send_telegram(message: str, bot_token: str, chat_id: str):\n",
        "    if not (bot_token and chat_id):\n",
        "        return False\n",
        "    try:\n",
        "        url = f\"https://api.telegram.org/bot{bot_token}/sendMessage\"\n",
        "        data = {\"chat_id\": chat_id, \"text\": message, \"parse_mode\": \"HTML\", \"disable_web_page_preview\": True}\n",
        "        r = requests.post(url, data=data, timeout=10)\n",
        "        return r.ok\n",
        "    except Exception:\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gathering pipeline with broader fallbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _entries_from_rss(rss_url: str):\n",
        "    try:\n",
        "        ents = fetch_rss_entries(rss_url)\n",
        "        return ents if isinstance(ents, list) else []\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def _alias_match(title: str, summary: str, aliases):\n",
        "    t = (title or '').lower()\n",
        "    s = (summary or '').lower()\n",
        "    for a in aliases:\n",
        "        aa = a.lower()\n",
        "        if aa and (aa in t or aa in s):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def gather_news_any(company_or_ticker: str, days: int, max_articles: int = 25):\n",
        "    aliases = expand_aliases(company_or_ticker)\n",
        "    items = []\n",
        "    # Google News\n",
        "    for alias in aliases:\n",
        "        for rss in google_news_rss_urls(alias, days):\n",
        "            items.extend(_entries_from_rss(rss))\n",
        "    # Bing News\n",
        "    if len(items) < 5:\n",
        "        for alias in aliases:\n",
        "            for rss in bing_news_rss_urls(alias):\n",
        "                items.extend(_entries_from_rss(rss))\n",
        "    # NSE Corporate Announcements (if symbol like TCS.NS)\n",
        "    m = re.match(r\"^([A-Za-z0-9]+)\\.NS$\", company_or_ticker.strip())\n",
        "    if m:\n",
        "        items.extend(fetch_nse_corp(m.group(1), days))\n",
        "    # Generic feeds (keyword filter: title OR summary)\n",
        "    if len(items) < 6:\n",
        "        for feed in GENERAL_FEEDS_LOCAL:\n",
        "            ents = _entries_from_rss(feed)\n",
        "            for e in ents:\n",
        "                if _alias_match(e.get('title'), e.get('summary'), aliases):\n",
        "                    items.append(e)\n",
        "\n",
        "    now = ist_now()\n",
        "    norm = []\n",
        "    for it in items:\n",
        "        pub = parse_pubdate(it.get('published','')) or now\n",
        "        if (now - pub).days > days:\n",
        "            continue\n",
        "        url = normalize_gnews_link(it.get('link',''))\n",
        "        norm.append({\"title\": it.get('title','').strip(), \"summary\": it.get('summary',''), \"url\": url, \"published_dt\": pub})\n",
        "\n",
        "    # De-dup: (norm title, domain)\n",
        "    dedup = {}\n",
        "    for it in norm:\n",
        "        key = (re.sub(r\"\\s+\", \" \", it[\"title\"].lower()).strip(), domain_of(it[\"url\"]))\n",
        "        if key not in dedup:\n",
        "            dedup[key] = it\n",
        "    out = sorted(dedup.values(), key=lambda x: x['published_dt'], reverse=True)\n",
        "    return out[:max_articles]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the agent & export CSVs + daily ledger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scoring news for TCS.NS: 0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Result ===\n",
            "{'query': 'TCS.NS', 'resolved_query': 'Tata Consultancy Services Limited', 'days': 5, 'articles': 0, 'agg_score': 0.0, 'confidence': 0.0, 'bias': 'NEUTRAL'}\n",
            "Ledger updated: outputs/2025-09-28/bias_ledger.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import display\n",
        "\n",
        "def run_agent(query: str, days: int, max_articles: int):\n",
        "    news = gather_news_any(query, days, max_articles)\n",
        "    now = ist_now()\n",
        "    rows = []\n",
        "    for it in tqdm(news, desc=f\"Scoring news for {query}\"):\n",
        "        text, extracted_title = fetch_article_text(it[\"url\"])\n",
        "        title = extracted_title or it[\"title\"] or \"(no title)\"\n",
        "        sentiment = score_sentiment_finbert(text[:4000] if text else title)\n",
        "        base = article_base_score(sentiment)\n",
        "        rw = recency_weight(it[\"published_dt\"], now, TAU_DAYS)\n",
        "        sw = source_weight(it[\"url\"])\n",
        "        rows.append({\n",
        "            \"published_ist\": it[\"published_dt\"].strftime(\"%Y-%m-%d %H:%M\"),\n",
        "            \"domain\": domain_of(it[\"url\"]),\n",
        "            \"title\": title[:200],\n",
        "            \"url\": it[\"url\"],\n",
        "            \"sent_pos\": round(sentiment[\"positive\"], 3),\n",
        "            \"sent_neu\": round(sentiment[\"neutral\"], 3),\n",
        "            \"sent_neg\": round(sentiment[\"negative\"], 3),\n",
        "            \"base_score\": round(base, 3),\n",
        "            \"recency_w\": round(rw, 3),\n",
        "            \"source_w\": round(sw, 2),\n",
        "            \"weighted\": round(base * rw * sw, 3),\n",
        "        })\n",
        "    df = pd.DataFrame(rows, columns=[\n",
        "        \"published_ist\",\"domain\",\"title\",\"url\",\n",
        "        \"sent_pos\",\"sent_neu\",\"sent_neg\",\"base_score\",\"recency_w\",\"source_w\",\"weighted\"\n",
        "    ])\n",
        "    agg, conf = aggregate_scores(df if not df.empty else pd.DataFrame())\n",
        "    bias = final_bias(agg, len(df))\n",
        "    result = {\n",
        "        \"query\": query,\n",
        "        \"resolved_query\": resolve_company_query(query),\n",
        "        \"days\": days,\n",
        "        \"articles\": int(len(df)),\n",
        "        \"agg_score\": float(round(agg, 3)),\n",
        "        \"confidence\": float(round(conf, 3)),\n",
        "        \"bias\": bias,\n",
        "    }\n",
        "    return result, df\n",
        "\n",
        "# Wire GENERAL_FEEDS from CONFIG\n",
        "GENERAL_FEEDS_LOCAL = GENERAL_FEEDS.copy()\n",
        "\n",
        "today = ist_now().strftime(\"%Y-%m-%d\")\n",
        "out_dir = os.path.join(OUTPUT_BASE_DIR, today)\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "ledger_path = os.path.join(out_dir, \"bias_ledger.csv\")\n",
        "ledger_rows = []\n",
        "all_results = []\n",
        "for q in QUERIES:\n",
        "    result, df = run_agent(q, DAYS, MAX_ARTICLES)\n",
        "    all_results.append(result)\n",
        "    print(\"\\n=== Result ===\")\n",
        "    print(result)\n",
        "    if not df.empty:\n",
        "        display(df.sort_values(\"weighted\", ascending=False).head(10))\n",
        "        fname = f\"{safe_filename(result['resolved_query']).lower()}_articles.csv\"\n",
        "        csv_path = os.path.join(out_dir, fname)\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f\"Saved articles CSV: {csv_path}\")\n",
        "    ledger_rows.append({\n",
        "        \"run_date\": today,\n",
        "        \"query\": result[\"query\"],\n",
        "        \"resolved_query\": result[\"resolved_query\"],\n",
        "        \"days_window\": result[\"days\"],\n",
        "        \"articles\": result[\"articles\"],\n",
        "        \"agg_score\": result[\"agg_score\"],\n",
        "        \"confidence\": result[\"confidence\"],\n",
        "        \"bias\": result[\"bias\"],\n",
        "    })\n",
        "\n",
        "ledger_df = pd.DataFrame(ledger_rows, columns=[\n",
        "    \"run_date\",\"query\",\"resolved_query\",\"days_window\",\"articles\",\"agg_score\",\"confidence\",\"bias\"\n",
        "])\n",
        "if os.path.exists(ledger_path):\n",
        "    existing = pd.read_csv(ledger_path)\n",
        "    ledger_df = pd.concat([existing, ledger_df], ignore_index=True)\n",
        "ledger_df.to_csv(ledger_path, index=False)\n",
        "print(f\"Ledger updated: {ledger_path}\")\n",
        "\n",
        "if ENABLE_TELEGRAM and TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID:\n",
        "    lines = [\"<b>News Sentiment Summary</b>\"]\n",
        "    for r in all_results:\n",
        "        lines.append(f\"<b>{r['resolved_query']}</b>: Articles {r['articles']}, Agg {r['agg_score']}, Conf {r['confidence']} — <b>{r['bias']}</b>\")\n",
        "    _ok = send_telegram(\"\\n\".join(lines), TELEGRAM_BOT_TOKEN, TELEGRAM_CHAT_ID)\n",
        "    print(\"Telegram sent:\", _ok)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Debug cell — how many items each feed returned (first 3 titles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Debug counts for TCS.NS]\n",
            "\n",
            "0 https://news.google.com/rss/search?q=TCS.NS+when:5d&hl=en-IN&gl=IN&ceid=IN:en\n",
            "0 https://news.google.com/rss/search?q=Tata+Consultancy+Services+Limited+when:5d&hl=en-IN&gl=IN&ceid=IN:en\n",
            "0 https://news.google.com/rss/search?q=Tata+Consultancy+Services+when:5d&hl=en-IN&gl=IN&ceid=IN:en\n",
            "0 https://news.google.com/rss/search?q=TCS+when:5d&hl=en-IN&gl=IN&ceid=IN:en\n",
            "0 https://www.bing.com/news/search?q=TCS.NS&qft=sortbydate%3d%221%22&form=RSSMHL&format=RSS\n",
            "0 https://www.bing.com/news/search?q=Tata+Consultancy+Services+Limited&qft=sortbydate%3d%221%22&form=RSSMHL&format=RSS\n",
            "0 https://www.bing.com/news/search?q=Tata+Consultancy+Services&qft=sortbydate%3d%221%22&form=RSSMHL&format=RSS\n",
            "0 https://www.bing.com/news/search?q=TCS&qft=sortbydate%3d%221%22&form=RSSMHL&format=RSS\n",
            "0 https://feeds.reuters.com/reuters/INtopNews\n",
            "0 https://feeds.reuters.com/reuters/businessNews\n",
            "0 https://www.moneycontrol.com/rss/MCtopnews.xml\n",
            "0 https://www.livemint.com/rss/companies\n",
            "0 https://economictimes.indiatimes.com/industry/it/ites/rssfeeds/13357270.cms\n",
            "0 https://www.business-standard.com/rss/companies-101.rss\n",
            "0 https://www.cnbctv18.com/rss/latest.xml\n",
            "\n",
            "Sample titles:\n",
            "- https://news.google.com/rss/search?q=TCS.NS+when:5d&hl=en-IN&gl=IN&ceid=IN:en\n",
            "- https://news.google.com/rss/search?q=Tata+Consultancy+Services+Limited+when:5d&hl=en-IN&gl=IN&ceid=IN:en\n",
            "- https://news.google.com/rss/search?q=Tata+Consultancy+Services+when:5d&hl=en-IN&gl=IN&ceid=IN:en\n",
            "- https://news.google.com/rss/search?q=TCS+when:5d&hl=en-IN&gl=IN&ceid=IN:en\n",
            "- https://www.bing.com/news/search?q=TCS.NS&qft=sortbydate%3d%221%22&form=RSSMHL&format=RSS\n",
            "- https://www.bing.com/news/search?q=Tata+Consultancy+Services+Limited&qft=sortbydate%3d%221%22&form=RSSMHL&format=RSS\n",
            "- https://www.bing.com/news/search?q=Tata+Consultancy+Services&qft=sortbydate%3d%221%22&form=RSSMHL&format=RSS\n",
            "- https://www.bing.com/news/search?q=TCS&qft=sortbydate%3d%221%22&form=RSSMHL&format=RSS\n",
            "- https://feeds.reuters.com/reuters/INtopNews\n",
            "- https://feeds.reuters.com/reuters/businessNews\n",
            "- https://www.moneycontrol.com/rss/MCtopnews.xml\n",
            "- https://www.livemint.com/rss/companies\n",
            "- https://economictimes.indiatimes.com/industry/it/ites/rssfeeds/13357270.cms\n",
            "- https://www.business-standard.com/rss/companies-101.rss\n",
            "- https://www.cnbctv18.com/rss/latest.xml\n"
          ]
        }
      ],
      "source": [
        "def debug_sources(q: str, days: int):\n",
        "    aliases = expand_aliases(q)\n",
        "    counts = {}\n",
        "    samples = {}\n",
        "    # Google\n",
        "    for alias in aliases:\n",
        "        for rss in google_news_rss_urls(alias, days):\n",
        "            ents = fetch_rss_entries(rss)\n",
        "            counts[rss] = len(ents)\n",
        "            samples[rss] = [e.get('title') for e in ents[:3]]\n",
        "    # Bing\n",
        "    for alias in aliases:\n",
        "        for rss in bing_news_rss_urls(alias):\n",
        "            ents = fetch_rss_entries(rss)\n",
        "            counts[rss] = len(ents)\n",
        "            samples[rss] = [e.get('title') for e in ents[:3]]\n",
        "    # Generic feeds\n",
        "    for feed in GENERAL_FEEDS_LOCAL:\n",
        "        ents = fetch_rss_entries(feed)\n",
        "        counts[feed] = len(ents)\n",
        "        samples[feed] = [e.get('title') for e in ents[:3]]\n",
        "    return counts, samples\n",
        "\n",
        "for q in QUERIES:\n",
        "    c, s = debug_sources(q, DAYS)\n",
        "    print(f\"\\n[Debug counts for {q}]\\n\")\n",
        "    for k,v in c.items():\n",
        "        print(v, k)\n",
        "    print(\"\\nSample titles:\")\n",
        "    for k, arr in s.items():\n",
        "        print(\"-\", k)\n",
        "        for t in arr:\n",
        "            print(\"   *\", t)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".talib",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
