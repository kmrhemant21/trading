# Summary and Highlights: Spark Architecture

In this lesson, you learned that:

## Core Architecture
- **Spark architecture** has driver and executor processes coordinated by the SparkContext in the driver.
- The **driver creates jobs**, and the Spark Context splits jobs into tasks that can be run in parallel in the executors on the cluster. 
- **Stages** are a set of tasks that are separated by a data shuffle.

## Performance Considerations
- **Shuffles are costly**, as they require data serialization, disk, and network I/O. 
- The driver program can be run in either **client mode** (connecting the driver outside the cluster) or **cluster mode** (running the driver in the cluster).

## Cluster Management
- **Cluster managers** acquire resources and run as an abstracted service outside the application. 
- Spark can run on:
    - Spark Standalone
    - Apache Hadoop YARN
    - Apache Mesos
    - Kubernetes cluster managers
- **Choosing a cluster manager** depends on your data ecosystem and factors such as ease of configuration, portability, deployment, or data partitioning needs. 
- Spark can also run using **local mode**, which is useful for testing or debugging an application.

## Application Submission
- **"spark-submit"** is a unified interface to submit the Spark application, no matter the cluster manager or application language.
- **Mandatory options** include telling Spark which cluster manager to connect to; other options set driver deploy mode or executor resourcing.
- To **manage dependencies**, application projects or libraries must be accessible for driver and executor processes, for example, by creating a Java or Scala uber-JAR.

## Development Tools
- **Spark Shell** simplifies working with data by automatically initializing the SparkContext and SparkSession variables and providing Spark API access.