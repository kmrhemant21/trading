{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7ba12b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache: using parquet engine 'pyarrow'.\n",
      "Starting single-run backtest for DMA(10,20)+2-green with full toggles.\n",
      "Dates: 2025-01-01 to 2025-08-01 | Capital/trade: ₹50,000\n",
      "Exits: protective=False, time_exit=True (10d), HSL=True, TSL=False\n",
      "Filters enabled: ['rsi', 'sma'] | mode=all\n",
      "Fees=ON, Slippage=ON\n",
      "Warm cache: downloading 30 tickers sequentially ...\n",
      "Warm cache: done.\n",
      "[BSE.NS] trades: 9\n",
      "[BAJFINANCE.NS] trades: 10\n",
      "[BAJAJFINSV.NS] trades: 12\n",
      "[BDL.NS] trades: 10\n",
      "[BEL.NS] trades: 13\n",
      "[BHARTIARTL.NS] trades: 10\n",
      "[CHOLAFIN.NS] trades: 19\n",
      "[COFORGE.NS] trades: 11\n",
      "[DIVISLAB.NS] trades: 16\n",
      "[DIXON.NS] trades: 7\n",
      "[NYKAA.NS] trades: 4\n",
      "[HDFCBANK.NS] trades: 14\n",
      "[HDFCLIFE.NS] trades: 7\n",
      "[ICICIBANK.NS] trades: 20\n",
      "[INDHOTEL.NS] trades: 19\n",
      "[INDIGO.NS] trades: 6\n",
      "[KOTAKBANK.NS] trades: 8\n",
      "[MFSL.NS] trades: 17\n",
      "[MAXHEALTH.NS] trades: 6\n",
      "[MAZDOCK.NS] trades: 4\n",
      "[MUTHOOTFIN.NS] trades: 16\n",
      "[PAYTM.NS] trades: 5\n",
      "[PERSISTENT.NS] trades: 8\n",
      "[SBICARD.NS] trades: 4\n",
      "[SBILIFE.NS] trades: 9\n",
      "[SRF.NS] trades: 8\n",
      "[SHREECEM.NS] trades: 19\n",
      "[SOLARINDS.NS] trades: 17\n",
      "[TVSMOTOR.NS] trades: 10\n",
      "[UNITDSPR.NS] trades: 0\n",
      "\n",
      "=== Portfolio Summary ===\n",
      "total_trades: 318\n",
      "win_rate: 47.48427672955975\n",
      "avg_roi_pct: 0.37858705149777716\n",
      "total_net_profit: 60195.34118814656\n",
      "overall_roi_pct: 0.3785870514977771\n",
      "avg_holding_days: 8.00314465408805\n",
      "time_exits: 195\n",
      "two_red_exits: 34\n",
      "protective_exits: 0\n",
      "hsl_exits: 89\n",
      "tsl_exits: 0\n",
      "\n",
      "Saved trade log to dma2green_single_results.csv\n"
     ]
    }
   ],
   "source": [
    "# backtest_dma2green_single_run.py\n",
    "\"\"\"\n",
    "Single-run backtester for DMA(10,20) + strict 2-green strategy with Exit v2.\n",
    "- Entry: 10DMA crosses above 20DMA AND two consecutive green candles where 2nd candle's O/H/L/C > 1st\n",
    "- Exit v2:\n",
    "    * Two consecutive red candles AND second closes below 20DMA, OR\n",
    "    * (optional) protective exit: any close below 20DMA\n",
    "- Optional time exit: max hold N days (default 10)\n",
    "- Optional HSL/TSL (hard/trailing stop)\n",
    "- Optional indicator filters (RSI/MACD/ADX/SMA/Bollinger) combined via 'all' or 'any'\n",
    "- Groww-like fees + GST + STT + DP + (optional) slippage; per-trade isolated capital sizing\n",
    "- Warm cache (parquet preferred, CSV fallback)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "TICKER_LIST = ['BSE.NS', 'BAJFINANCE.NS', 'BAJAJFINSV.NS', 'BDL.NS', 'BEL.NS', 'BHARTIARTL.NS', 'CHOLAFIN.NS',\n",
    "               'COFORGE.NS', 'DIVISLAB.NS', 'DIXON.NS', 'NYKAA.NS', 'HDFCBANK.NS', 'HDFCLIFE.NS', 'ICICIBANK.NS',\n",
    "               'INDHOTEL.NS', 'INDIGO.NS', 'KOTAKBANK.NS', 'MFSL.NS', 'MAXHEALTH.NS', 'MAZDOCK.NS', 'MUTHOOTFIN.NS',\n",
    "               'PAYTM.NS', 'PERSISTENT.NS', 'SBICARD.NS', 'SBILIFE.NS', 'SRF.NS', 'SHREECEM.NS', 'SOLARINDS.NS',\n",
    "               'TVSMOTOR.NS', 'UNITDSPR.NS']\n",
    "\n",
    "START_DATE = \"2025-01-01\"\n",
    "END_DATE   = \"2025-08-01\"\n",
    "\n",
    "# Strategy params\n",
    "DMA_FAST = 10\n",
    "DMA_SLOW = 20\n",
    "\n",
    "# Capital per trade (isolated sizing)\n",
    "CAPITAL_PER_TRADE = 50000.0\n",
    "\n",
    "# -------------------- Toggles --------------------\n",
    "# Exits\n",
    "USE_PROTECTIVE_EXIT = False      # exit if any Close < 20DMA\n",
    "USE_TIME_EXIT       = True      # force-close after MAX_HOLD_DAYS\n",
    "MAX_HOLD_DAYS       = 10\n",
    "\n",
    "USE_HARD_STOP       = True     # hard stop as % from entry\n",
    "HARD_STOP_PCT       = 5.0       # only used if USE_HARD_STOP\n",
    "\n",
    "USE_TRAILING_STOP   = False     # trailing stop as % from peak after entry\n",
    "TRAILING_STOP_PCT   = 10.0      # only used if USE_TRAILING_STOP\n",
    "\n",
    "# Indicator filters (entry-time)\n",
    "INDICATORS = {\n",
    "    \"rsi\":  {\"enabled\": True,  \"length\": 14, \"min\": 45},     # require RSI >= min\n",
    "    \"macd\": {\"enabled\": False, \"fast\": 12, \"slow\": 26, \"signal\": 9},  # MACD>signal & hist>0\n",
    "    \"adx\":  {\"enabled\": False,  \"length\": 14, \"min_adx\": 20}, # require ADX>=min & +DI > -DI\n",
    "    \"sma\":  {\"enabled\": True, \"length\": 50},                 # require Close >= SMA_len\n",
    "    \"bb\":   {\"enabled\": False, \"length\": 20, \"stddev\": 2.0, \"rule\": \"above_mid\"}  # 'above_mid'/'below_mid'\n",
    "}\n",
    "COMBINATION_MODE = \"all\"  # 'all' or 'any'\n",
    "\n",
    "# Fees & slippage toggles\n",
    "APPLY_FEES      = True\n",
    "APPLY_SLIPPAGE  = True\n",
    "\n",
    "# -------------------- FEES & SLIPPAGE (Groww-like) --------------------\n",
    "SLIPPAGE_PCT = 0.05  # percent per side (adverse)\n",
    "\n",
    "# Brokerage: \"₹20 OR 0.1% per executed order — whichever is lower, minimum ₹5\"\n",
    "BROKERAGE_CAP_RUPEES = 20.0\n",
    "BROKERAGE_PCT_CAP    = 0.1   # percent\n",
    "BROKERAGE_MIN_RUPEES = 5.0\n",
    "\n",
    "# Regulatory / exchange (approx values; per side)\n",
    "EXCHANGE_TXN_PCT   = 0.00297  # percent\n",
    "SEBI_TURNOVER_PCT  = 0.0001   # percent\n",
    "IPFT_PCT           = 0.0001   # percent\n",
    "\n",
    "# STT for delivery sell\n",
    "STT_SELL_PCT       = 0.025    # percent on sell notional\n",
    "\n",
    "# Stamp duty (buy side)\n",
    "STAMP_DUTY_BUY_PCT = 0.003    # percent\n",
    "\n",
    "# DP charge (sell, delivery)\n",
    "DP_CHARGE_SELL     = 16.5     # rupees per sell if notional >= 100\n",
    "\n",
    "# GST on (brokerage + exch + sebi + ipft + dp)\n",
    "GST_PCT            = 18.0     # percent\n",
    "\n",
    "# -------------------- CACHE HELPERS --------------------\n",
    "CACHE_DIR = \"cache\"\n",
    "pathlib.Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "_parquet_engine = None\n",
    "try:\n",
    "    import pyarrow  # noqa: F401\n",
    "    _parquet_engine = \"pyarrow\"\n",
    "except Exception:\n",
    "    try:\n",
    "        import fastparquet  # noqa: F401\n",
    "        _parquet_engine = \"fastparquet\"\n",
    "    except Exception:\n",
    "        _parquet_engine = None\n",
    "\n",
    "if _parquet_engine:\n",
    "    print(f\"Cache: using parquet engine '{_parquet_engine}'.\")\n",
    "else:\n",
    "    print(\"Cache: parquet engine not found — falling back to CSV cache (no extra deps required).\")\n",
    "\n",
    "def ensure_cache_dir():\n",
    "    pathlib.Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _cache_path_for_ticker(ticker, use_parquet):\n",
    "    safe = ticker.replace('/', '_').replace(':', '_')\n",
    "    if use_parquet:\n",
    "        return os.path.join(CACHE_DIR, f\"{safe}.parquet\")\n",
    "    else:\n",
    "        return os.path.join(CACHE_DIR, f\"{safe}.csv\")\n",
    "\n",
    "def download_with_retry(ticker, start, end, interval=\"1d\", max_retries=3, backoff_sec=2):\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            df = yf.download(ticker, start=start, interval=interval, end=end,\n",
    "                             auto_adjust=True, progress=False, threads=True, multi_level_index=False)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            wait = backoff_sec * attempt\n",
    "            print(f\"  download error for {ticker} (attempt {attempt}/{max_retries}): {e}. retrying in {wait}s\")\n",
    "            time.sleep(wait)\n",
    "    print(f\"  download failed for {ticker} after {max_retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "def _read_cache(ticker):\n",
    "    use_parquet = _parquet_engine is not None\n",
    "    path = _cache_path_for_ticker(ticker, use_parquet)\n",
    "    alt = _cache_path_for_ticker(ticker, not use_parquet)\n",
    "\n",
    "    paths_to_try = []\n",
    "    if os.path.exists(path):\n",
    "        paths_to_try.append(path)\n",
    "    if os.path.exists(alt):\n",
    "        paths_to_try.append(alt)\n",
    "\n",
    "    for p in paths_to_try:\n",
    "        try:\n",
    "            if os.path.getsize(p) == 0:\n",
    "                print(f\"  Warning: cache file {p} is zero bytes — removing.\")\n",
    "                try:\n",
    "                    os.remove(p)\n",
    "                except Exception as e:\n",
    "                    print(f\"    Could not remove zero-byte file {p}: {e}\")\n",
    "                continue\n",
    "        except OSError:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if p.endswith('.parquet'):\n",
    "                return pd.read_parquet(p, engine=_parquet_engine)\n",
    "            else:\n",
    "                return pd.read_csv(p, index_col=0, parse_dates=True)\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: failed to read cache for {ticker} at {p}: {e}\")\n",
    "            try:\n",
    "                os.remove(p)\n",
    "                print(f\"    Removed corrupted cache file {p}.\")\n",
    "            except Exception as rm_e:\n",
    "                print(f\"    Could not remove corrupted cache file {p}: {rm_e}\")\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def _write_cache(ticker, df):\n",
    "    use_parquet = _parquet_engine is not None\n",
    "    path = _cache_path_for_ticker(ticker, use_parquet)\n",
    "    tmp_path = path + \".tmp\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        if use_parquet:\n",
    "            df.to_parquet(tmp_path, engine=_parquet_engine)\n",
    "        else:\n",
    "            df.to_csv(tmp_path)\n",
    "        os.replace(tmp_path, path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: failed to write cache for {ticker} to {path}: {e}\")\n",
    "        try:\n",
    "            if os.path.exists(tmp_path):\n",
    "                os.remove(tmp_path)\n",
    "        except Exception:\n",
    "            pass\n",
    "        if use_parquet:\n",
    "            try:\n",
    "                alt = _cache_path_for_ticker(ticker, use_parquet=False)\n",
    "                tmp_alt = alt + \".tmp\"\n",
    "                df.to_csv(tmp_alt)\n",
    "                os.replace(tmp_alt, alt)\n",
    "                print(f\"  Wrote CSV fallback cache for {ticker} to {alt}\")\n",
    "                return True\n",
    "            except Exception as e2:\n",
    "                print(f\"  Warning: fallback CSV write also failed for {ticker}: {e2}\")\n",
    "        return False\n",
    "\n",
    "def warm_cache(tickers, start=START_DATE, end=END_DATE):\n",
    "    print(f\"Warm cache: downloading {len(tickers)} tickers sequentially ...\")\n",
    "    ensure_cache_dir()\n",
    "    for t in tickers:\n",
    "        try:\n",
    "            existing = _read_cache(t)\n",
    "            if existing is not None:\n",
    "                continue\n",
    "            df = download_with_retry(t, start, end, interval=\"1d\", max_retries=3, backoff_sec=2)\n",
    "            if df is None or df.empty:\n",
    "                print(f\"  warm_cache: no data for {t}; skipping.\")\n",
    "                continue\n",
    "            df = normalize_df_columns(df)\n",
    "            df = collapse_duplicate_columns_take_first(df)\n",
    "            ok = _write_cache(t, df)\n",
    "            if not ok:\n",
    "                print(f\"  warm_cache: failed to write cache for {t}\")\n",
    "            else:\n",
    "                print(f\"  warm_cache: cached {t}\")\n",
    "            time.sleep(0.25)\n",
    "        except Exception as e:\n",
    "            print(f\"  warm_cache: error for {t}: {e}\")\n",
    "            continue\n",
    "    print(\"Warm cache: done.\")\n",
    "\n",
    "# -------------------- NORMALIZATION & DUPLICATE HANDLING --------------------\n",
    "def normalize_df_columns(df):\n",
    "    if hasattr(df, \"columns\") and getattr(df.columns, \"nlevels\", 1) > 1:\n",
    "        df.columns = [\"_\".join([str(c) for c in col if c is not None]).strip() for col in df.columns.values]\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    mapping = {}\n",
    "    lower_map = {c.lower(): c for c in cols}\n",
    "    for name in ['close', 'high', 'low', 'open', 'volume']:\n",
    "        if name in lower_map:\n",
    "            mapping[lower_map[name]] = name.capitalize()\n",
    "        else:\n",
    "            match = next((c for c in cols if name in c.lower()), None)\n",
    "            if match:\n",
    "                mapping[match] = name.capitalize()\n",
    "    if mapping:\n",
    "        df = df.rename(columns=mapping)\n",
    "    return df\n",
    "\n",
    "def collapse_duplicate_columns_take_first(df):\n",
    "    if df.columns.duplicated().any():\n",
    "        dup_names = list({c for c in df.columns[df.columns.duplicated()]})\n",
    "        print(f\"  Warning: duplicate columns found and collapsed for: {dup_names}\")\n",
    "        df = df.groupby(df.columns, axis=1).first()\n",
    "    return df\n",
    "\n",
    "# -------------------- INDICATORS --------------------\n",
    "def sma(series, length):\n",
    "    return series.rolling(length).mean()\n",
    "\n",
    "def rsi(df, length=14, column='Close'):\n",
    "    series = df[column]\n",
    "    delta = series.diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "    avg_gain = gain.ewm(alpha=1/length, adjust=False).mean()\n",
    "    avg_loss = loss.ewm(alpha=1/length, adjust=False).mean()\n",
    "    rs = avg_gain / (avg_loss.replace(0, np.nan))\n",
    "    df['RSI'] = (100 - (100 / (1 + rs))).fillna(50)\n",
    "    return df\n",
    "\n",
    "def macd(df, fast=12, slow=26, signal=9, column='Close'):\n",
    "    ema_fast = df[column].ewm(span=fast, adjust=False).mean()\n",
    "    ema_slow = df[column].ewm(span=slow, adjust=False).mean()\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    signal_line = macd_line.ewm(span=signal, adjust=False).mean()\n",
    "    df['MACD'] = macd_line\n",
    "    df['MACD_signal'] = signal_line\n",
    "    df['MACD_hist'] = df['MACD'] - df['MACD_signal']\n",
    "    return df\n",
    "\n",
    "def adx(df, n=14):\n",
    "    high = df['High']; low = df['Low']; close = df['Close']\n",
    "    tr1 = high - low\n",
    "    tr2 = (high - close.shift(1)).abs()\n",
    "    tr3 = (low - close.shift(1)).abs()\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    up_move = high.diff(); down_move = -low.diff()\n",
    "    plus_dm = np.where((up_move > down_move) & (up_move > 0), up_move, 0.0)\n",
    "    minus_dm = np.where((down_move > up_move) & (down_move > 0), down_move, 0.0)\n",
    "    tr_smooth = pd.Series(tr).rolling(window=n).sum()\n",
    "    plus_dm_smooth = pd.Series(plus_dm).rolling(window=n).sum()\n",
    "    minus_dm_smooth = pd.Series(minus_dm).rolling(window=n).sum()\n",
    "    plus_di = 100 * (plus_dm_smooth / tr_smooth).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    minus_di = 100 * (minus_dm_smooth / tr_smooth).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    dx = (abs(plus_di - minus_di) / (plus_di + minus_di)).replace([np.inf, -np.inf], 0) * 100\n",
    "    df['+DI'] = plus_di; df['-DI'] = minus_di; df['ADX'] = dx.rolling(window=n).mean()\n",
    "    return df\n",
    "\n",
    "def bbands(df, length=20, stddev=2.0, column='Close'):\n",
    "    mid = df[column].rolling(length).mean()\n",
    "    std = df[column].rolling(length).std()\n",
    "    df['BB_middle'] = mid\n",
    "    df['BB_upper'] = mid + stddev*std\n",
    "    df['BB_lower'] = mid - stddev*std\n",
    "    return df\n",
    "\n",
    "# -------------------- STRATEGY BUILDING BLOCKS --------------------\n",
    "def two_green_strict(df):\n",
    "    # True at t if t-1 and t are green, and candle t has O/H/L/C > candle t-1 O/H/L/C\n",
    "    o, h, l, c = df['Open'], df['High'], df['Low'], df['Close']\n",
    "    green_prev = df['Close'].shift(1) > df['Open'].shift(1)\n",
    "    green_now  = df['Close'] > df['Open']\n",
    "    cond_strict = (o > o.shift(1)) & (h > h.shift(1)) & (l > l.shift(1)) & (c > c.shift(1))\n",
    "    return (green_prev & green_now & cond_strict)\n",
    "\n",
    "def two_red(df):\n",
    "    red_prev = df['Close'].shift(1) < df['Open'].shift(1)\n",
    "    red_now  = df['Close'] < df['Open']\n",
    "    return (red_prev & red_now)\n",
    "\n",
    "def build_base_signals(df, dma_fast=DMA_FAST, dma_slow=DMA_SLOW):\n",
    "    df['DMA_fast'] = sma(df['Close'], dma_fast)\n",
    "    df['DMA_slow'] = sma(df['Close'], dma_slow)\n",
    "    df['dma_cross_up'] = (df['DMA_fast'] > df['DMA_slow']) & (df['DMA_fast'].shift(1) <= df['DMA_slow'].shift(1))\n",
    "    df['two_green_strict'] = two_green_strict(df)\n",
    "    df['two_red'] = two_red(df)\n",
    "    return df\n",
    "\n",
    "# -------------------- DATA PREP (cache-aware) --------------------\n",
    "def get_stock_data(ticker, start, end):\n",
    "    ensure_cache_dir()\n",
    "    raw_df = _read_cache(ticker)\n",
    "    if raw_df is None:\n",
    "        raw_df = download_with_retry(ticker, start, end, interval=\"1d\", max_retries=3, backoff_sec=2)\n",
    "        if raw_df is None or raw_df.empty:\n",
    "            return None\n",
    "        raw_df = normalize_df_columns(raw_df)\n",
    "        raw_df = collapse_duplicate_columns_take_first(raw_df)\n",
    "        ok = _write_cache(ticker, raw_df)\n",
    "        if not ok:\n",
    "            print(f\"  Warning: failed to cache {ticker} (continuing without cache).\")\n",
    "\n",
    "    df = raw_df.copy()\n",
    "    df = normalize_df_columns(df)\n",
    "    df = collapse_duplicate_columns_take_first(df)\n",
    "\n",
    "    for c in ['Close', 'High', 'Low', 'Open', 'Volume']:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "    df = build_base_signals(df, dma_fast=DMA_FAST, dma_slow=DMA_SLOW)\n",
    "\n",
    "    # Optional indicators\n",
    "    if INDICATORS.get('rsi', {}).get('enabled', False):\n",
    "        df = rsi(df, length=INDICATORS['rsi'].get('length', 14))\n",
    "\n",
    "    if INDICATORS.get('macd', {}).get('enabled', False):\n",
    "        df = macd(df,\n",
    "                  fast=INDICATORS['macd'].get('fast', 12),\n",
    "                  slow=INDICATORS['macd'].get('slow', 26),\n",
    "                  signal=INDICATORS['macd'].get('signal', 9))\n",
    "\n",
    "    if INDICATORS.get('adx', {}).get('enabled', False):\n",
    "        df = adx(df, n=INDICATORS['adx'].get('length', 14))\n",
    "\n",
    "    if INDICATORS.get('sma', {}).get('enabled', False):\n",
    "        df[f\"SMA_{INDICATORS['sma'].get('length', 50)}\"] = df['Close'].rolling(INDICATORS['sma'].get('length', 50)).mean()\n",
    "\n",
    "    if INDICATORS.get('bb', {}).get('enabled', False):\n",
    "        df = bbands(df, length=INDICATORS['bb'].get('length', 20), stddev=INDICATORS['bb'].get('stddev', 2.0))\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    if df.empty:\n",
    "        return None\n",
    "    return df\n",
    "\n",
    "# -------------------- FEES & SLIPPAGE HELPERS --------------------\n",
    "def groww_brokerage_for_order(order_value):\n",
    "    pct_based = (BROKERAGE_PCT_CAP / 100.0) * order_value\n",
    "    capped = min(BROKERAGE_CAP_RUPEES, pct_based)\n",
    "    return max(capped, BROKERAGE_MIN_RUPEES)\n",
    "\n",
    "def compute_pnl(entry_price, exit_price, qty, apply_fees=APPLY_FEES, apply_slippage=APPLY_SLIPPAGE):\n",
    "    # slippage per side\n",
    "    entry_eff = entry_price * (1 + (SLIPPAGE_PCT/100.0 if apply_slippage else 0.0))\n",
    "    exit_eff  = exit_price  * (1 - (SLIPPAGE_PCT/100.0 if apply_slippage else 0.0))\n",
    "\n",
    "    entry_notional = entry_eff * qty\n",
    "    exit_notional  = exit_eff  * qty\n",
    "\n",
    "    gross_profit = exit_notional - entry_notional\n",
    "\n",
    "    if not apply_fees:\n",
    "        return entry_eff, exit_eff, gross_profit, 0.0, {\"fees_total\": 0.0}\n",
    "\n",
    "    brok_buy  = groww_brokerage_for_order(entry_notional)\n",
    "    brok_sell = groww_brokerage_for_order(exit_notional)\n",
    "\n",
    "    exch_buy  = (EXCHANGE_TXN_PCT  / 100.0) * entry_notional\n",
    "    exch_sell = (EXCHANGE_TXN_PCT  / 100.0) * exit_notional\n",
    "    sebi_buy  = (SEBI_TURNOVER_PCT / 100.0) * entry_notional\n",
    "    sebi_sell = (SEBI_TURNOVER_PCT / 100.0) * exit_notional\n",
    "    ipft_buy  = (IPFT_PCT / 100.0) * entry_notional\n",
    "    ipft_sell = (IPFT_PCT / 100.0) * exit_notional\n",
    "    stamp_buy = (STAMP_DUTY_BUY_PCT / 100.0) * entry_notional\n",
    "    stt_sell  = (STT_SELL_PCT / 100.0) * exit_notional\n",
    "    dp_sell   = DP_CHARGE_SELL if exit_notional >= 100.0 else 0.0\n",
    "\n",
    "    taxable = brok_buy + brok_sell + exch_buy + exch_sell + sebi_buy + sebi_sell + ipft_buy + ipft_sell + dp_sell\n",
    "    gst = (GST_PCT / 100.0) * taxable\n",
    "\n",
    "    total_fees = brok_buy + brok_sell + exch_buy + exch_sell + sebi_buy + sebi_sell + ipft_buy + ipft_sell + stamp_buy + stt_sell + dp_sell + gst\n",
    "    net_profit = gross_profit - total_fees\n",
    "\n",
    "    details = {\n",
    "        \"brokerage_buy\": brok_buy, \"brokerage_sell\": brok_sell,\n",
    "        \"exchange_buy\": exch_buy, \"exchange_sell\": exch_sell,\n",
    "        \"sebi_buy\": sebi_buy, \"sebi_sell\": sebi_sell,\n",
    "        \"ipft_buy\": ipft_buy, \"ipft_sell\": ipft_sell,\n",
    "        \"stamp_buy\": stamp_buy, \"stt_sell\": stt_sell,\n",
    "        \"dp_sell\": dp_sell, \"gst\": gst, \"fees_total\": total_fees\n",
    "    }\n",
    "    return entry_eff, exit_eff, net_profit, total_fees, details\n",
    "\n",
    "# -------------------- ENTRY FILTERS --------------------\n",
    "def indicator_entry_checks(row, prev_row):\n",
    "    checks = {}\n",
    "\n",
    "    if INDICATORS.get('rsi', {}).get('enabled', False):\n",
    "        rsi_min = INDICATORS['rsi'].get('min', 45)\n",
    "        r = row.get('RSI', np.nan)\n",
    "        checks['rsi'] = (r >= rsi_min)\n",
    "\n",
    "    if INDICATORS.get('macd', {}).get('enabled', False):\n",
    "        checks['macd'] = (row.get('MACD', 0) > row.get('MACD_signal', 0)) and (row.get('MACD_hist', 0) > 0)\n",
    "\n",
    "    if INDICATORS.get('adx', {}).get('enabled', False):\n",
    "        adx_min = INDICATORS['adx'].get('min_adx', 20)\n",
    "        checks['adx'] = (row.get('ADX', 0) >= adx_min) and (row.get('+DI', 0) > row.get('-DI', 0))\n",
    "\n",
    "    if INDICATORS.get('sma', {}).get('enabled', False):\n",
    "        sma_len = INDICATORS['sma'].get('length', 50)\n",
    "        checks['sma'] = (row['Close'] >= row.get(f\"SMA_{sma_len}\", np.nan))\n",
    "\n",
    "    if INDICATORS.get('bb', {}).get('enabled', False):\n",
    "        rule = INDICATORS['bb'].get('rule', 'above_mid')\n",
    "        if rule == 'above_mid':\n",
    "            checks['bb'] = (row['Close'] >= row.get('BB_middle', np.nan))\n",
    "        else:\n",
    "            checks['bb'] = (row['Close'] <= row.get('BB_middle', np.nan))\n",
    "\n",
    "    return checks\n",
    "\n",
    "def combine_indicator_checks(checks, mode='all'):\n",
    "    if not checks:\n",
    "        return True\n",
    "    vals = list(checks.values())\n",
    "    return all(vals) if mode == 'all' else any(vals)\n",
    "\n",
    "# -------------------- BACKTEST --------------------\n",
    "def run_backtest_on_df(df, ticker, capital_per_trade=CAPITAL_PER_TRADE):\n",
    "    in_pos = False\n",
    "    trades = []\n",
    "    entry_price = 0.0; entry_date = None; qty = 0\n",
    "    peak_price = 0.0; trailing_stop_price = 0.0; hard_stop_price = 0.0\n",
    "    days_in_trade = 0\n",
    "\n",
    "    tsl_mult = 1 - (TRAILING_STOP_PCT / 100.0) if USE_TRAILING_STOP else None\n",
    "    hsl_mult = 1 - (HARD_STOP_PCT / 100.0) if USE_HARD_STOP else None\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        prev_row = df.iloc[i-1]\n",
    "        row = df.iloc[i]\n",
    "\n",
    "        # ENTRY\n",
    "        base_entry = bool(row['dma_cross_up'] and row['two_green_strict'])\n",
    "\n",
    "        ind_checks = indicator_entry_checks(row, prev_row)\n",
    "        enabled_any = any(v.get('enabled', False) for v in INDICATORS.values())\n",
    "        ind_ok = combine_indicator_checks(ind_checks, mode=COMBINATION_MODE) if enabled_any else True\n",
    "\n",
    "        entry_signal = base_entry and ind_ok\n",
    "\n",
    "        if (not in_pos) and entry_signal:\n",
    "            entry_price = row['Close']\n",
    "            expected_entry_eff = entry_price * (1 + (SLIPPAGE_PCT/100.0 if APPLY_SLIPPAGE else 0.0))\n",
    "            qty = int(capital_per_trade // expected_entry_eff)\n",
    "            if qty <= 0:\n",
    "                continue\n",
    "            in_pos = True\n",
    "            entry_date = row.name\n",
    "            days_in_trade = 0\n",
    "\n",
    "            if USE_TRAILING_STOP:\n",
    "                peak_price = entry_price\n",
    "                trailing_stop_price = peak_price * tsl_mult\n",
    "            if USE_HARD_STOP:\n",
    "                hard_stop_price = entry_price * hsl_mult\n",
    "\n",
    "        elif in_pos:\n",
    "            days_in_trade += 1\n",
    "\n",
    "            # Update TSL\n",
    "            if USE_TRAILING_STOP and row['High'] > peak_price:\n",
    "                peak_price = row['High']\n",
    "                trailing_stop_price = peak_price * tsl_mult\n",
    "\n",
    "            # EXIT CONDITIONS\n",
    "            exit_price = 0.0\n",
    "            exit_reason = None\n",
    "\n",
    "            # 1) Hard stop\n",
    "            if USE_HARD_STOP and row['Low'] <= hard_stop_price:\n",
    "                exit_price = hard_stop_price; exit_reason = \"HSL\"\n",
    "            # 2) Trailing stop\n",
    "            elif USE_TRAILING_STOP and row['Low'] <= trailing_stop_price:\n",
    "                exit_price = trailing_stop_price; exit_reason = \"TSL\"\n",
    "            # 3) Time exit\n",
    "            elif USE_TIME_EXIT and days_in_trade >= MAX_HOLD_DAYS:\n",
    "                exit_price = row['Close']; exit_reason = \"Time\"\n",
    "            else:\n",
    "                # 4) Exit v2 (two red and second close < 20DMA)\n",
    "                exit_two_red_below_20 = bool(row['two_red'] and (row['Close'] < row['DMA_slow']))\n",
    "                # 5) Protective exit (any close < 20DMA)\n",
    "                exit_protective = bool(row['Close'] < row['DMA_slow']) if USE_PROTECTIVE_EXIT else False\n",
    "\n",
    "                if exit_two_red_below_20:\n",
    "                    exit_price = row['Close']; exit_reason = \"TwoRedBelow20\"\n",
    "                elif exit_protective:\n",
    "                    exit_price = row['Close']; exit_reason = \"Protective\"\n",
    "\n",
    "            # Execute exit\n",
    "            if exit_price > 0:\n",
    "                entry_eff, exit_eff, pnl_currency, fees_total, fee_break = compute_pnl(entry_price, exit_price, qty,\n",
    "                                                                                      apply_fees=APPLY_FEES,\n",
    "                                                                                      apply_slippage=APPLY_SLIPPAGE)\n",
    "                roi_pct_on_capital = (pnl_currency / capital_per_trade) * 100.0\n",
    "\n",
    "                trades.append({\n",
    "                    \"ticker\": ticker,\n",
    "                    \"entry_date\": entry_date, \"exit_date\": row.name,\n",
    "                    \"qty\": qty,\n",
    "                    \"entry_price\": entry_price, \"exit_price\": exit_price,\n",
    "                    \"entry_eff\": entry_eff, \"exit_eff\": exit_eff,\n",
    "                    \"net_profit\": pnl_currency, \"fees_total\": fees_total,\n",
    "                    \"roi_%\": roi_pct_on_capital,\n",
    "                    \"exit_reason\": exit_reason,\n",
    "                    \"days_held\": days_in_trade\n",
    "                })\n",
    "\n",
    "                # reset\n",
    "                in_pos = False\n",
    "                entry_price = 0.0; entry_date = None; qty = 0\n",
    "                peak_price = 0.0; trailing_stop_price = 0.0; hard_stop_price = 0.0\n",
    "                days_in_trade = 0\n",
    "\n",
    "    return trades\n",
    "\n",
    "# -------------------- SUMMARY --------------------\n",
    "def aggregate_metrics(trades):\n",
    "    if not trades:\n",
    "        return {\n",
    "            \"total_trades\": 0, \"win_rate\": 0.0, \"avg_roi_pct\": 0.0,\n",
    "            \"total_net_profit\": 0.0, \"overall_roi_pct\": 0.0,\n",
    "            \"avg_holding_days\": 0.0, \"time_exits\": 0, \"two_red_exits\": 0, \"protective_exits\": 0,\n",
    "            \"hsl_exits\": 0, \"tsl_exits\": 0\n",
    "        }\n",
    "    df = pd.DataFrame(trades)\n",
    "    total_trades = len(df)\n",
    "    win_rate = (df['net_profit'] > 0).mean() * 100.0\n",
    "    avg_roi = df['roi_%'].mean()\n",
    "    total_net = df['net_profit'].sum()\n",
    "    overall_roi = (total_net / (CAPITAL_PER_TRADE * total_trades)) * 100.0 if total_trades else 0.0\n",
    "    avg_hold = df['days_held'].mean()\n",
    "\n",
    "    return {\n",
    "        \"total_trades\": int(total_trades),\n",
    "        \"win_rate\": float(win_rate),\n",
    "        \"avg_roi_pct\": float(avg_roi),\n",
    "        \"total_net_profit\": float(total_net),\n",
    "        \"overall_roi_pct\": float(overall_roi),\n",
    "        \"avg_holding_days\": float(avg_hold),\n",
    "        \"time_exits\": int((df['exit_reason'] == 'Time').sum()),\n",
    "        \"two_red_exits\": int((df['exit_reason'] == 'TwoRedBelow20').sum()),\n",
    "        \"protective_exits\": int((df['exit_reason'] == 'Protective').sum()),\n",
    "        \"hsl_exits\": int((df['exit_reason'] == 'HSL').sum()),\n",
    "        \"tsl_exits\": int((df['exit_reason'] == 'TSL').sum())\n",
    "    }\n",
    "\n",
    "# -------------------- MAIN --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting single-run backtest for DMA(10,20)+2-green with full toggles.\")\n",
    "    print(f\"Dates: {START_DATE} to {END_DATE} | Capital/trade: ₹{CAPITAL_PER_TRADE:,.0f}\")\n",
    "    print(f\"Exits: protective={USE_PROTECTIVE_EXIT}, time_exit={USE_TIME_EXIT} ({MAX_HOLD_DAYS}d), HSL={USE_HARD_STOP}, TSL={USE_TRAILING_STOP}\")\n",
    "    print(f\"Filters enabled: {[k for k,v in INDICATORS.items() if v.get('enabled', False)]} | mode={COMBINATION_MODE}\")\n",
    "    print(f\"Fees={'ON' if APPLY_FEES else 'OFF'}, Slippage={'ON' if APPLY_SLIPPAGE else 'OFF'}\")\n",
    "\n",
    "    # Warm cache (optional but recommended once)\n",
    "    warm_cache(TICKER_LIST)\n",
    "\n",
    "    all_trades = []\n",
    "    for t in TICKER_LIST:\n",
    "        try:\n",
    "            df = get_stock_data(t, START_DATE, END_DATE)\n",
    "        except Exception as e:\n",
    "            print(f\"[{t}] data error: {e}\")\n",
    "            df = None\n",
    "        if df is None:\n",
    "            print(f\"[{t}] skipped (no data).\")\n",
    "            continue\n",
    "        trades = run_backtest_on_df(df, t, capital_per_trade=CAPITAL_PER_TRADE)\n",
    "        print(f\"[{t}] trades: {len(trades)}\")\n",
    "        all_trades.extend(trades)\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n=== Portfolio Summary ===\")\n",
    "    M = aggregate_metrics(all_trades)\n",
    "    for k,v in M.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    # Save\n",
    "    if all_trades:\n",
    "        out = pd.DataFrame(all_trades)\n",
    "        out.sort_values(['ticker','entry_date'], inplace=True)\n",
    "        out.to_csv(\"dma2green_single_results.csv\", index=False)\n",
    "        print(\"\\nSaved trade log to dma2green_single_results.csv\")\n",
    "    else:\n",
    "        print(\"\\nNo trades produced with current settings.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".talib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
