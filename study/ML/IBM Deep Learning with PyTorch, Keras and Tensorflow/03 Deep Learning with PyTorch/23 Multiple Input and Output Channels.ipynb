{"cells":[{"cell_type":"markdown","id":"ffc2695a-508c-497d-94f0-3a8d9ae67cce","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n","\n","\n","\n","<h1>Multiple Input and Output Channels</h1> \n"]},{"cell_type":"markdown","id":"811b5562-2a3c-4478-b61f-0c5b05149180","metadata":{},"outputs":[],"source":["\n","<h3>Objective for this Notebook<h3>    \n","<h5> 1. Learn on Multiple Input and Multiple Output Channels.</h5>    \n","\n"]},{"cell_type":"markdown","id":"07e7304f-47b0-4456-b82e-a5f45c7a83b7","metadata":{},"outputs":[],"source":["\n","# Table of Contents\n","In this lab, you will study convolution and review how the different operations change the relationship between input and output.\n","\n","<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","<li><a href=\"#ref0\">Multiple Output Channels </a></li>\n","\n","<li><a href=\"#ref1\">Multiple Input Channels</a></li>\n","<li><a href=\"#ref2\">Multiple Input and Multiple Output Channels </a></li>\n","<li><a href=\"#ref3\">Practice Questions </a></li>\n","\n","<br>\n","<p></p>\n","Estimated Time Needed: <strong>25 min</strong>\n","</div>\n","\n","<hr>\n"]},{"cell_type":"markdown","id":"08ac97e0-f2d8-4954-8b7e-60677bd1473e","metadata":{},"outputs":[],"source":["Import the following libraries:\n"]},{"cell_type":"code","id":"d0ee63ba-219d-4b61-9a35-b0a2a27d394a","metadata":{},"outputs":[],"source":["%%time\n%pip install pandas numpy matplotlib scipy\n%pip install torch==2.8.0+cpu torchvision==0.23.0+cpu torchaudio==2.8.0+cpu \\\n    --index-url https://download.pytorch.org/whl/cpu"]},{"cell_type":"code","id":"88913572-4f09-4218-91f5-54923d18ddf1","metadata":{},"outputs":[],"source":["import torch \nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import ndimage, misc"]},{"cell_type":"markdown","id":"25a6b54b-332b-4ca0-ba02-232b90344cc4","metadata":{},"outputs":[],"source":["<a id=\"ref0\"></a>\n","<h2 align=center>Multiple Output Channels </h2>\n"]},{"cell_type":"markdown","id":"ca7d4957-b04a-493b-ba79-577b2b64d576","metadata":{},"outputs":[],"source":["In Pytroch, you can create a <code>Conv2d</code> object with multiple outputs. For each channel, a kernel is created, and each kernel performs a convolution independently. As a result, the number of outputs is equal to the number of channels. This is demonstrated in the following figure. The number 9 is convolved with three kernels: each of a different color. There are three different activation maps represented by the different colors.\n"]},{"cell_type":"markdown","id":"7a3113d7-e375-4319-8b74-860cd392fd0e","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2activationmaps.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"d2366888-cc1e-43a8-9b66-ba4109e924ff","metadata":{},"outputs":[],"source":["Symbolically, this can be represented as follows:\n"]},{"cell_type":"markdown","id":"153c66f8-ddc6-47aa-8dad-1bff3320fbea","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2activationmap2.png\" width=\"500,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"624d48ff-aad9-49e0-8310-a686500f1fb3","metadata":{},"outputs":[],"source":["Create a <code>Conv2d</code> with three channels:\n"]},{"cell_type":"code","id":"2626acec-40df-495c-ac84-a102f154a290","metadata":{},"outputs":[],"source":["conv1 = nn.Conv2d(in_channels=1, out_channels=3,kernel_size=3)"]},{"cell_type":"markdown","id":"2f239fca-b26f-4386-ab8c-24bf3e0233e3","metadata":{},"outputs":[],"source":["Pytorch randomly assigns values to each kernel. However, use kernels that have  been developed to detect edges:\n"]},{"cell_type":"code","id":"d7b3f480-2b59-4428-a790-f15458e6c9b1","metadata":{},"outputs":[],"source":["Gx=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])\nGy=torch.tensor([[1.0,2.0,1.0],[0.0,0.0,0.0],[-1.0,-2.0,-1.0]])\n\nconv1.state_dict()['weight'][0][0]=Gx\nconv1.state_dict()['weight'][1][0]=Gy\nconv1.state_dict()['weight'][2][0]=torch.ones(3,3)"]},{"cell_type":"markdown","id":"8145d0c9-042c-4374-b1d6-95cfa6171bbf","metadata":{},"outputs":[],"source":["Each kernel has its own bias, so set them all to zero:\n"]},{"cell_type":"code","id":"60170903-0740-47e3-991c-0da6818f95eb","metadata":{},"outputs":[],"source":["conv1.state_dict()['bias'][:]=torch.tensor([0.0,0.0,0.0])\nconv1.state_dict()['bias']"]},{"cell_type":"markdown","id":"a88d8854-40c5-4fd1-9ed7-f6fa6953732e","metadata":{},"outputs":[],"source":["Print out each kernel: \n"]},{"cell_type":"code","id":"1a608f98-3da3-4ffa-8f13-3d8f1a9a39f8","metadata":{},"outputs":[],"source":["for x in conv1.state_dict()['weight']:\n    print(x)"]},{"cell_type":"markdown","id":"8f431ad2-f87f-461a-be36-6c038d0d11fe","metadata":{},"outputs":[],"source":["Create an input <code>image</code> to represent the input X:\n"]},{"cell_type":"code","id":"8f0cd430-1359-4ca0-ba66-d525d6a73742","metadata":{},"outputs":[],"source":["image=torch.zeros(1,1,5,5)\nimage[0,0,:,2]=1\nimage"]},{"cell_type":"markdown","id":"579affc0-dca9-4ea7-bb4c-a0a0bb38a950","metadata":{},"outputs":[],"source":["Plot it as an image: \n"]},{"cell_type":"code","id":"b6ede7a5-69e5-4176-886d-869009634d07","metadata":{},"outputs":[],"source":["plt.imshow(image[0,0,:,:].numpy(), interpolation='nearest', cmap=plt.cm.gray)\nplt.colorbar()\nplt.show()"]},{"cell_type":"markdown","id":"8a49bb8e-2e11-4e72-ab75-5f9a3c38fb93","metadata":{},"outputs":[],"source":["Perform convolution using each channel: \n"]},{"cell_type":"code","id":"7ee9e49f-af5a-4c7c-86d3-86ba4327b4a3","metadata":{},"outputs":[],"source":["out=conv1(image)"]},{"cell_type":"markdown","id":"546c936f-0e09-45a9-8efb-2ffc81477f81","metadata":{},"outputs":[],"source":["The result is a 1x3x3x3 tensor. This represents one sample with three channels, and each channel contains a 3x3 image.  The same rules that govern the shape of each image were discussed in the last section.\n"]},{"cell_type":"code","id":"26b422f9-28b4-422e-9faf-f64260a8cfb3","metadata":{},"outputs":[],"source":["out.shape"]},{"cell_type":"markdown","id":"06a874fa-17fa-46de-a259-888969050b19","metadata":{},"outputs":[],"source":["Print out each channel as a tensor or an image: \n"]},{"cell_type":"code","id":"61a84864-a5d9-42ff-b376-fbb740cf022a","metadata":{},"outputs":[],"source":["for channel,image in enumerate(out[0]):\n    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n    print(image)\n    plt.title(\"channel {}\".format(channel))\n    plt.colorbar()\n    plt.show()"]},{"cell_type":"markdown","id":"fabdf38e-116f-46c1-b19c-78942760d4c8","metadata":{},"outputs":[],"source":["Different kernels can be used to detect various features in an image. You can see that the first channel fluctuates, and the second two channels produce a constant value. The following figure summarizes the process:\n"]},{"cell_type":"markdown","id":"d141c213-b97e-4078-8789-0553615c871d","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2outputsgray.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"0667f261-b7ce-42ee-964f-3ff68684644d","metadata":{},"outputs":[],"source":["If you use a different image, the result will be different: \n"]},{"cell_type":"code","id":"60a9ce34-ac51-4d6c-bb34-61be786b73f7","metadata":{},"outputs":[],"source":["image1=torch.zeros(1,1,5,5)\nimage1[0,0,2,:]=1\nprint(image1)\nplt.imshow(image1[0,0,:,:].detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\nplt.show()"]},{"cell_type":"markdown","id":"d8d09301-6e0c-4940-b5ad-c02e4edec002","metadata":{},"outputs":[],"source":["In this case, the second channel fluctuates, and the first and the third channels produce a constant value.\n"]},{"cell_type":"code","id":"35e3920b-f62d-4b55-bc54-2639e511a615","metadata":{},"outputs":[],"source":["out1=conv1(image1)\nfor channel,image in enumerate(out1[0]):\n    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n    print(image)\n    plt.title(\"channel {}\".format(channel))\n    plt.colorbar()\n    plt.show()"]},{"cell_type":"markdown","id":"c66aae62-9147-4a0e-aba5-adb30b0e998f","metadata":{},"outputs":[],"source":["The following figure summarizes the process:\n"]},{"cell_type":"markdown","id":"ff227295-1b94-461b-9417-34986ec9b46a","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2ouputsgray2.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"1cb4a086-c309-4cbe-aa61-44f861bc6ed5","metadata":{},"outputs":[],"source":["<a id=\"ref1\"></a>\n","<h2 align=center>Multiple Input Channels </h2>\n"]},{"cell_type":"markdown","id":"c14c700c-8275-48f7-af24-d1486291b52f","metadata":{},"outputs":[],"source":["For two inputs, you can create two kernels. Each kernel performs a convolution on its associated input channel. The resulting output is added together as shown:  \n"]},{"cell_type":"markdown","id":"7417f5d8-882e-4f3e-b350-47942eeb1391","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.22chanalsinput.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"4fa09cf8-9af1-4d39-956f-1b741ecf3f07","metadata":{},"outputs":[],"source":["Create an input with two channels:\n"]},{"cell_type":"code","id":"5f3bb913-e768-4430-8012-7e9f779ae25c","metadata":{},"outputs":[],"source":["image2=torch.zeros(1,2,5,5)\nimage2[0,0,2,:]=-2\nimage2[0,1,2,:]=1\nimage2"]},{"cell_type":"markdown","id":"3468c47a-8767-452a-9245-c25420c9a3ce","metadata":{},"outputs":[],"source":["Plot out each image: \n"]},{"cell_type":"code","id":"2af0d0a0-f10c-4810-900b-87933bdc2473","metadata":{},"outputs":[],"source":["for channel,image in enumerate(image2[0]):\n    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n    print(image)\n    plt.title(\"channel {}\".format(channel))\n    plt.colorbar()\n    plt.show()"]},{"cell_type":"markdown","id":"4e0b7cdb-d23b-4af0-9cf0-81ea55e17ae3","metadata":{},"outputs":[],"source":["Create a <code>Conv2d</code> object with two inputs:\n"]},{"cell_type":"code","id":"24bb2ca5-2068-4a52-925c-09724938adc7","metadata":{},"outputs":[],"source":["conv3 = nn.Conv2d(in_channels=2, out_channels=1,kernel_size=3)"]},{"cell_type":"markdown","id":"6833205b-3164-4a7f-8d19-c618d1931fed","metadata":{},"outputs":[],"source":["Assign kernel values to make the math a little easier: \n"]},{"cell_type":"code","id":"e57379cf-690d-4449-ab96-0fa548d6d189","metadata":{},"outputs":[],"source":["Gx1=torch.tensor([[0.0,0.0,0.0],[0,1.0,0],[0.0,0.0,0.0]])\nconv3.state_dict()['weight'][0][0]=1*Gx1\nconv3.state_dict()['weight'][0][1]=-2*Gx1\nconv3.state_dict()['bias'][:]=torch.tensor([0.0])"]},{"cell_type":"code","id":"b0d7f908-1756-48a2-9f92-f3c23dfc9fbf","metadata":{},"outputs":[],"source":["conv3.state_dict()['weight']"]},{"cell_type":"markdown","id":"17254e4f-0f74-46f5-b107-cb5266d42cac","metadata":{},"outputs":[],"source":["Perform the convolution:\n"]},{"cell_type":"code","id":"83893f1d-6835-4a15-81c0-2408900ac367","metadata":{},"outputs":[],"source":["conv3(image2)"]},{"cell_type":"markdown","id":"e63ecddb-de7a-43e7-8f19-6d0a8ad6c69a","metadata":{},"outputs":[],"source":["The following images summarize the process. The object performs Convolution.\n"]},{"cell_type":"markdown","id":"026d1434-8bd9-4208-98e2-617a5d21b03b","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_two_channal_example.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"92d7c197-ced0-47df-9382-68313779fb47","metadata":{},"outputs":[],"source":["Then, it adds the result: \n"]},{"cell_type":"markdown","id":"9bc6e732-1659-486d-9026-1b3af6a2f8a3","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_two_channal_example2.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"84c258cd-86d2-4eae-8d2a-3ed4774f9d52","metadata":{},"outputs":[],"source":["<a id=\"ref2\"></a>\n","\n","<h2>Multiple Input and Multiple Output Channels</h2>\n"]},{"cell_type":"markdown","id":"f7779f8a-44f1-4166-902f-f8ba5964d274","metadata":{},"outputs":[],"source":["When using multiple inputs and outputs, a kernel is created for each input, and the process is repeated for each output. The process is summarized in the following image. \n","\n","There are two input channels and 3 output channels. For each channel, the input in red and purple is convolved with an individual kernel that is colored differently. As a result, there are three outputs. \n"]},{"cell_type":"markdown","id":"6494359f-72a4-44e4-ad28-b7152efff422","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2mulit_input_output.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"71db3ad5-8120-4ef6-b077-481f8ddb24c4","metadata":{},"outputs":[],"source":["Create an example with two inputs and three outputs and assign the kernel values to make the math a little easier: \n"]},{"cell_type":"code","id":"8f9f4b40-53f4-4ca4-a16e-4bba704af4b2","metadata":{},"outputs":[],"source":["conv4 = nn.Conv2d(in_channels=2, out_channels=3,kernel_size=3)\nconv4.state_dict()['weight'][0][0]=torch.tensor([[0.0,0.0,0.0],[0,0.5,0],[0.0,0.0,0.0]])\nconv4.state_dict()['weight'][0][1]=torch.tensor([[0.0,0.0,0.0],[0,0.5,0],[0.0,0.0,0.0]])\n\n\nconv4.state_dict()['weight'][1][0]=torch.tensor([[0.0,0.0,0.0],[0,1,0],[0.0,0.0,0.0]])\nconv4.state_dict()['weight'][1][1]=torch.tensor([[0.0,0.0,0.0],[0,-1,0],[0.0,0.0,0.0]])\n\nconv4.state_dict()['weight'][2][0]=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])\nconv4.state_dict()['weight'][2][1]=torch.tensor([[1.0,2.0,1.0],[0.0,0.0,0.0],[-1.0,-2.0,-1.0]])"]},{"cell_type":"markdown","id":"619dd3df-3046-40c5-8ebb-158195eb461b","metadata":{},"outputs":[],"source":["For each output, there is a bias, so set them all to zero: \n"]},{"cell_type":"code","id":"41c2687a-aed9-4e75-a4a5-49113d65a473","metadata":{},"outputs":[],"source":["conv4.state_dict()['bias'][:]=torch.tensor([0.0,0.0,0.0])"]},{"cell_type":"markdown","id":"9ff25fb5-908d-4213-bc3a-fd55531914fd","metadata":{},"outputs":[],"source":["Create a two-channel image and plot the results: \n"]},{"cell_type":"code","id":"0140c8d8-5ebb-44ce-92bf-689a5495e6d3","metadata":{},"outputs":[],"source":["image4=torch.zeros(1,2,5,5)\nimage4[0][0]=torch.ones(5,5)\nimage4[0][1][2][2]=1\nfor channel,image in enumerate(image4[0]):\n    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n    print(image)\n    plt.title(\"channel {}\".format(channel))\n    plt.colorbar()\n    plt.show()"]},{"cell_type":"markdown","id":"83f01ea8-4627-44e0-9c58-f4766ed8389c","metadata":{},"outputs":[],"source":["Perform the convolution:\n"]},{"cell_type":"code","id":"9f362fa9-aa0b-4c1c-a1f4-0ea85805dd7c","metadata":{},"outputs":[],"source":["z=conv4(image4)\nz"]},{"cell_type":"markdown","id":"a900f15d-b7f6-4f64-a1b3-6cbcae33de41","metadata":{},"outputs":[],"source":["The output of the first channel is given by: \n"]},{"cell_type":"markdown","id":"ab741060-4895-470a-a276-c76a866b7402","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_%20multi_channel_1.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"aae7615c-7435-47d6-a5d9-3228298d5e32","metadata":{},"outputs":[],"source":["The output of the second channel is given by:\n"]},{"cell_type":"markdown","id":"e472d40f-f04d-4bd0-9cb1-1290da8763ac","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_%20multi_channel_2.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"b8dd2708-1589-4a09-b9c7-0aded225f337","metadata":{},"outputs":[],"source":["The output of the third channel is given by: \n"]},{"cell_type":"markdown","id":"88322bdb-7f47-4d33-bb25-dd1e42fe7a43","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_%20multi_channel_3.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"66cecb13-0810-4af3-898c-73798fc28d61","metadata":{},"outputs":[],"source":["<a id=\"ref3\"></a>\n","\n","<h2>Practice Questions </h2>\n"]},{"cell_type":"markdown","id":"0e0baa96-659b-4326-b5d2-74ebb3253e70","metadata":{},"outputs":[],"source":["Use the following two convolution objects to produce the same result as two input channel convolution on imageA and imageB as shown in the following image:\n"]},{"cell_type":"code","id":"7d859513-4cb3-46fe-98fb-317ef3799eea","metadata":{},"outputs":[],"source":["imageA=torch.zeros(1,1,5,5)\nimageB=torch.zeros(1,1,5,5)\nimageA[0,0,2,:]=-2\nimageB[0,0,2,:]=1\n\n\nconv5 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\nconv6 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\n\n\nGx1=torch.tensor([[0.0,0.0,0.0],[0,1.0,0],[0.0,0.0,0.0]])\nconv5.state_dict()['weight'][0][0]=1*Gx1\nconv6.state_dict()['weight'][0][0]=-2*Gx1\nconv5.state_dict()['bias'][:]=torch.tensor([0.0])\nconv6.state_dict()['bias'][:]=torch.tensor([0.0])"]},{"cell_type":"markdown","id":"4c2c1d4f-c1e3-4fbe-9a83-2c5f65a8e0c3","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2Practice%20Questions_1.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"ec46f389-f9ad-49f7-9acb-0ac8412b4180","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2Practice%20Questions_2.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"cb49bc4f-5098-4370-a7f6-ca0bea9a2b67","metadata":{},"outputs":[],"source":["Double-click __here__ for the solution.\n","\n","<!-- Your answer is below:\n","conv5(imageA)+conv6(imageB)\n","-->\n","\n","\n"]},{"cell_type":"markdown","id":"e9a3e7e5-968c-4850-af85-3f779d4c61eb","metadata":{},"outputs":[],"source":["### About the Authors:  \n","[Joseph Santarcangelo]( https://www.linkedin.com/in/joseph-s-50398b136/) has a PhD in Electrical Engineering. His research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. \n","\n","Other contributors: [Michelle Carey](  https://www.linkedin.com/in/michelleccarey/), [Mavis Zhou](  https://www.linkedin.com/in/jiahui-mavis-zhou-a4537814a/) \n"]},{"cell_type":"markdown","id":"3d7fc2bc-effb-4295-bada-29afa36efdbe","metadata":{},"outputs":[],"source":["<!--\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-23  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","\n","\n","\n","<hr>\n","-->\n","\n","## <h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":"python","version":"3.12.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"0de56a69a59c51ad4cdef77540550edb147efc752ec2801f47b7eac85aa3521d"},"nbformat":4,"nbformat_minor":4}