{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6dd7fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telegram ENV missing; skipping send.\n",
      "*DMA(10,20)+2-Green Entries*\n",
      "*2025-08-13*  — INDIGO.NS\n",
      "*2025-08-14*  — HDFCLIFE.NS, MFSL.NS, SOLARINDS.NS\n",
      "*2025-09-10*  — SRF.NS\n",
      "*2025-09-15*  — CHOLAFIN.NS\n",
      "*2025-09-16*  — BDL.NS\n",
      "*2025-09-17*  — BHARTIARTL.NS, KOTAKBANK.NS\n",
      "*2025-09-18*  — COFORGE.NS, HDFCBANK.NS\n",
      "*2025-09-19*  — SBILIFE.NS\n",
      "\n",
      "Added to open positions: 4\n",
      "Telegram sent.\n",
      "Open positions file: /Users/hemank/Documents/github/trading/swing/00 final/00 DMA/portfolio/open_positions.csv\n"
     ]
    }
   ],
   "source": [
    "# screener_dma2green_entries.py\n",
    "\"\"\"\n",
    "DMA(10,20) + strict 2-green screener (ENTRY) with optional indicator filters.\n",
    "- Scans a list of tickers for the last N days\n",
    "- Uses tuned params from your grid search (toggle on/off)\n",
    "- Saves date-wise CSV of signals\n",
    "- Appends new positions to portfolio/open_positions.csv (avoids duplicates, robust datetime handling)\n",
    "- Sends a Telegram message summarizing signals (optional)\n",
    "\n",
    "ENV required for Telegram:\n",
    "  TELEGRAM_BOT_TOKEN, TELEGRAM_CHAT_ID\n",
    "\n",
    "Run:\n",
    "  python screener_dma2green_entries.py\n",
    "\n",
    "Tip: schedule this daily after market close.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pathlib\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import requests\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import List, Dict\n",
    "\n",
    "# -------------------- USER CONFIG --------------------\n",
    "TICKER_LIST = [\n",
    "    'BSE.NS','BAJFINANCE.NS','BAJAJFINSV.NS','BDL.NS','BEL.NS','BHARTIARTL.NS',\n",
    "    'CHOLAFIN.NS','COFORGE.NS','DIVISLAB.NS','DIXON.NS','NYKAA.NS','HDFCBANK.NS',\n",
    "    'HDFCLIFE.NS','ICICIBANK.NS','INDHOTEL.NS','INDIGO.NS','KOTAKBANK.NS','MFSL.NS',\n",
    "    'MAXHEALTH.NS','MAZDOCK.NS','MUTHOOTFIN.NS','PAYTM.NS','PERSISTENT.NS','SBICARD.NS',\n",
    "    'SBILIFE.NS','SRF.NS','SHREECEM.NS','SOLARINDS.NS','TVSMOTOR.NS','UNITDSPR.NS'\n",
    "]\n",
    "\n",
    "START_DATE = \"2015-01-01\"\n",
    "END_DATE = None  # None = today\n",
    "\n",
    "# How many recent bars to scan for entries (date-wise output)\n",
    "SCAN_LOOKBACK_DAYS = 50\n",
    "\n",
    "# Tuned params (toggle on/off, same shape as backtest)\n",
    "PARAMS = {\n",
    "    \"combination_mode\": \"all\",  # 'all' or 'any'\n",
    "\n",
    "    \"rsi_enabled\": True,\n",
    "    \"rsi_min\": 45,\n",
    "\n",
    "    \"adx_enabled\": False,\n",
    "    \"adx_length\": 14,\n",
    "    \"adx_min\": 20,\n",
    "\n",
    "    \"sma_enabled\": False,\n",
    "    \"sma_length\": 50,\n",
    "\n",
    "    \"bb_enabled\": False,\n",
    "    \"bb_length\": 20,\n",
    "    \"bb_stddev\": 2.0,\n",
    "\n",
    "    \"macd_enabled\": False,\n",
    "    \"macd_fast\": 12,\n",
    "    \"macd_slow\": 26,\n",
    "    \"macd_signal\": 9,\n",
    "\n",
    "    # Exit/management settings to record with the position (for the exit script)\n",
    "    \"protective_exit\": False,\n",
    "    \"use_hard_stop\": True,\n",
    "    \"hard_stop_pct\": 5.0,\n",
    "    \"use_trailing_stop\": False,\n",
    "    \"trailing_stop_pct\": 10.0,\n",
    "    \"max_hold_days\": 10\n",
    "}\n",
    "\n",
    "DMA_FAST = 10\n",
    "DMA_SLOW = 20\n",
    "\n",
    "# Telegram\n",
    "ENABLE_TELEGRAM = True\n",
    "TELEGRAM_BOT_TOKEN = os.getenv(\"TELEGRAM_BOT_TOKEN\", \"\")\n",
    "TELEGRAM_CHAT_ID = os.getenv(\"TELEGRAM_CHAT_ID\", \"\")\n",
    "\n",
    "# Paths\n",
    "CACHE_DIR = pathlib.Path(\"cache\")\n",
    "SIGNALS_DIR = pathlib.Path(\"signals\")\n",
    "PORTFOLIO_DIR = pathlib.Path(\"portfolio\")\n",
    "OPEN_POSITIONS_CSV = PORTFOLIO_DIR / \"open_positions.csv\"\n",
    "\n",
    "for p in [CACHE_DIR, SIGNALS_DIR, PORTFOLIO_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------- HELPERS --------------------\n",
    "def normalize_df_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if hasattr(df, \"columns\") and getattr(df.columns, \"nlevels\", 1) > 1:\n",
    "        df.columns = [\"_\".join([str(c) for c in col if c is not None]).strip() for c in df.columns.values]\n",
    "    cols = list(df.columns)\n",
    "    mapping = {}\n",
    "    lower_map = {c.lower(): c for c in cols}\n",
    "    for name in ['close', 'high', 'low', 'open', 'volume']:\n",
    "        if name in lower_map:\n",
    "            mapping[lower_map[name]] = name.capitalize()\n",
    "        else:\n",
    "            match = next((c for c in cols if name in c.lower()), None)\n",
    "            if match:\n",
    "                mapping[match] = name.capitalize()\n",
    "    if mapping:\n",
    "        df = df.rename(columns=mapping)\n",
    "    return df\n",
    "\n",
    "def collapse_duplicate_columns_take_first(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.columns.duplicated().any():\n",
    "        df = df.groupby(df.columns, axis=1).first()\n",
    "    return df\n",
    "\n",
    "def _cache_path_for_ticker(ticker: str) -> pathlib.Path:\n",
    "    safe = ticker.replace('/', '_').replace(':', '_')\n",
    "    return CACHE_DIR / f\"{safe}.csv\"\n",
    "\n",
    "def _read_cache(ticker: str) -> pd.DataFrame:\n",
    "    p = _cache_path_for_ticker(ticker)\n",
    "    if p.exists() and p.stat().st_size > 0:\n",
    "        try:\n",
    "            return pd.read_csv(p, index_col=0, parse_dates=True)\n",
    "        except Exception:\n",
    "            try:\n",
    "                p.unlink()\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "def _write_cache(ticker: str, df: pd.DataFrame) -> None:\n",
    "    p = _cache_path_for_ticker(ticker)\n",
    "    try:\n",
    "        df.to_csv(p)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def download_with_retry(ticker: str, start: str, end: str, interval=\"1d\", max_retries=3, backoff_sec=2):\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            df = yf.download(ticker, start=start, interval=interval, end=end,\n",
    "                             auto_adjust=True, progress=False, threads=True, multi_level_index=False)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            time.sleep(backoff_sec * attempt)\n",
    "    return None\n",
    "\n",
    "def sma(series: pd.Series, length: int) -> pd.Series:\n",
    "    return series.rolling(length).mean()\n",
    "\n",
    "def rsi(df: pd.DataFrame, length=14, column='Close') -> pd.DataFrame:\n",
    "    s = df[column]\n",
    "    delta = s.diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "    avg_gain = gain.ewm(alpha=1/length, adjust=False).mean()\n",
    "    avg_loss = loss.ewm(alpha=1/length, adjust=False).mean()\n",
    "    rs = avg_gain / (avg_loss.replace(0, np.nan))\n",
    "    df['RSI'] = (100 - (100 / (1 + rs))).fillna(50)\n",
    "    return df\n",
    "\n",
    "def macd(df: pd.DataFrame, fast=12, slow=26, signal=9, column='Close') -> pd.DataFrame:\n",
    "    ema_fast = df[column].ewm(span=fast, adjust=False).mean()\n",
    "    ema_slow = df[column].ewm(span=slow, adjust=False).mean()\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    signal_line = macd_line.ewm(span=signal, adjust=False).mean()\n",
    "    df['MACD'] = macd_line\n",
    "    df['MACD_signal'] = signal_line\n",
    "    df['MACD_hist'] = df['MACD'] - df['MACD_signal']\n",
    "    return df\n",
    "\n",
    "def adx(df: pd.DataFrame, n=14) -> pd.DataFrame:\n",
    "    high = df['High']; low = df['Low']; close = df['Close']\n",
    "    tr1 = high - low\n",
    "    tr2 = (high - close.shift(1)).abs()\n",
    "    tr3 = (low - close.shift(1)).abs()\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    up_move = high.diff(); down_move = -low.diff()\n",
    "    plus_dm = np.where((up_move > down_move) & (up_move > 0), up_move, 0.0)\n",
    "    minus_dm = np.where((down_move > up_move) & (down_move > 0), down_move, 0.0)\n",
    "    tr_smooth = pd.Series(tr).rolling(window=n).sum()\n",
    "    plus_dm_smooth = pd.Series(plus_dm).rolling(window=n).sum()\n",
    "    minus_dm_smooth = pd.Series(minus_dm).rolling(window=n).sum()\n",
    "    plus_di = 100 * (plus_dm_smooth / tr_smooth).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    minus_di = 100 * (minus_dm_smooth / tr_smooth).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    dx = (abs(plus_di - minus_di) / (plus_di + minus_di)).replace([np.inf, -np.inf], 0) * 100\n",
    "    df['+DI'] = plus_di; df['-DI'] = minus_di; df['ADX'] = dx.rolling(window=n).mean()\n",
    "    return df\n",
    "\n",
    "def bbands(df: pd.DataFrame, length=20, stddev=2.0, column='Close') -> pd.DataFrame:\n",
    "    mid = df[column].rolling(length).mean()\n",
    "    std = df[column].rolling(length).std()\n",
    "    df['BB_middle'] = mid\n",
    "    df['BB_upper'] = mid + stddev*std\n",
    "    df['BB_lower'] = mid - stddev*std\n",
    "    return df\n",
    "\n",
    "def two_green_strict(df: pd.DataFrame) -> pd.Series:\n",
    "    o, h, l, c = df['Open'], df['High'], df['Low'], df['Close']\n",
    "    green_prev = df['Close'].shift(1) > df['Open'].shift(1)\n",
    "    green_now  = df['Close'] > df['Open']\n",
    "    cond_strict = (o > o.shift(1)) & (h > h.shift(1)) & (l > l.shift(1)) & (c > c.shift(1))\n",
    "    return (green_prev & green_now & cond_strict)\n",
    "\n",
    "def build_base_signals(df: pd.DataFrame, dma_fast=10, dma_slow=20) -> pd.DataFrame:\n",
    "    df['DMA_fast'] = sma(df['Close'], dma_fast)\n",
    "    df['DMA_slow'] = sma(df['Close'], dma_slow)\n",
    "    df['dma_cross_up'] = (df['DMA_fast'] > df['DMA_slow']) & (df['DMA_fast'].shift(1) <= df['DMA_slow'].shift(1))\n",
    "    df['two_green_strict'] = two_green_strict(df)\n",
    "    return df\n",
    "\n",
    "def indicator_entry_filter(row: pd.Series, params: Dict) -> Dict[str, bool]:\n",
    "    checks = {}\n",
    "    if params.get('rsi_enabled', False):\n",
    "        checks['rsi'] = (row.get('RSI', np.nan) >= params.get('rsi_min', 45))\n",
    "    if params.get('adx_enabled', False):\n",
    "        adx_val = row.get('ADX', 0); plus = row.get('+DI', 0); minus = row.get('-DI', 0)\n",
    "        checks['adx'] = (adx_val >= params.get('adx_min', 20)) and (plus > minus)\n",
    "    if params.get('sma_enabled', False):\n",
    "        smalen = params.get('sma_length', 50)\n",
    "        checks['sma'] = (row['Close'] >= row.get(f'SMA_{smalen}', np.nan))\n",
    "    if params.get('bb_enabled', False):\n",
    "        checks['bb'] = (row['Close'] >= row.get('BB_middle', np.nan))\n",
    "    if params.get('macd_enabled', False):\n",
    "        checks['macd'] = (row.get('MACD', 0) > row.get('MACD_signal', 0)) and (row.get('MACD_hist', 0) > 0)\n",
    "    return checks\n",
    "\n",
    "def combine_indicator_signals(ind_results: Dict[str, bool], mode='all') -> bool:\n",
    "    if not ind_results:\n",
    "        return True\n",
    "    vals = list(ind_results.values())\n",
    "    return all(vals) if mode == 'all' else any(vals)\n",
    "\n",
    "def get_stock_data(ticker: str, start: str, end: str, params: Dict) -> pd.DataFrame:\n",
    "    raw = _read_cache(ticker)\n",
    "    if raw is None:\n",
    "        raw = download_with_retry(ticker, start, end, interval=\"1d\", max_retries=3, backoff_sec=2)\n",
    "        if raw is None or raw.empty:\n",
    "            return None\n",
    "        raw = normalize_df_columns(raw)\n",
    "        raw = collapse_duplicate_columns_take_first(raw)\n",
    "        _write_cache(ticker, raw)\n",
    "\n",
    "    df = raw.copy()\n",
    "    df = normalize_df_columns(df)\n",
    "    df = collapse_duplicate_columns_take_first(df)\n",
    "\n",
    "    for c in ['Close','High','Low','Open','Volume']:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "    df = build_base_signals(df, dma_fast=DMA_FAST, dma_slow=DMA_SLOW)\n",
    "\n",
    "    if params.get('rsi_enabled', False):\n",
    "        df = rsi(df, length=14)\n",
    "    if params.get('macd_enabled', False):\n",
    "        df = macd(df, fast=params.get('macd_fast',12), slow=params.get('macd_slow',26), signal=params.get('macd_signal',9))\n",
    "    if params.get('adx_enabled', False):\n",
    "        df = adx(df, n=params.get('adx_length',14))\n",
    "    if params.get('sma_enabled', False):\n",
    "        smalen = params.get('sma_length',50)\n",
    "        df[f\"SMA_{smalen}\"] = df['Close'].rolling(smalen).mean()\n",
    "    if params.get('bb_enabled', False):\n",
    "        df = bbands(df, length=params.get('bb_length',20), stddev=params.get('bb_stddev',2.0))\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    return df if not df.empty else None\n",
    "\n",
    "def find_entries(df: pd.DataFrame, params: Dict, lookback_days: int) -> pd.DataFrame:\n",
    "    \"\"\"Return rows where entry condition is met within the last N bars. Stores date as ISO string.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_tail = df.tail(lookback_days)\n",
    "    rows = []\n",
    "    for idx, row in df_tail.iterrows():\n",
    "        base_entry = bool(row['dma_cross_up'] and row['two_green_strict'])\n",
    "        ind_results = indicator_entry_filter(row, params)\n",
    "        enabled_any = any([params.get(k) for k in ['rsi_enabled','macd_enabled','adx_enabled','sma_enabled','bb_enabled']])\n",
    "        ind_ok = combine_indicator_signals(ind_results, mode=params.get('combination_mode','all'))\n",
    "        if base_entry and (ind_ok if enabled_any else True):\n",
    "            date_iso = pd.to_datetime(idx).strftime('%Y-%m-%d')\n",
    "            rows.append({\n",
    "                \"date\": date_iso,\n",
    "                \"close\": float(row['Close']),\n",
    "                \"dma10\": float(row['DMA_fast']),\n",
    "                \"dma20\": float(row['DMA_slow']),\n",
    "                \"rsi\": float(row.get('RSI', np.nan)) if 'RSI' in df.columns else np.nan,\n",
    "                \"adx\": float(row.get('ADX', np.nan)) if 'ADX' in df.columns else np.nan\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def send_telegram(text: str) -> None:\n",
    "    if not ENABLE_TELEGRAM:\n",
    "        print(\"[DRY] Telegram disabled. Message would be:\\n\", text)\n",
    "        return\n",
    "    token = TELEGRAM_BOT_TOKEN\n",
    "    chat_id = TELEGRAM_CHAT_ID\n",
    "    if not token or not chat_id:\n",
    "        print(\"Telegram ENV missing; skipping send.\")\n",
    "        print(text)\n",
    "        return\n",
    "    url = f\"https://api.telegram.org/bot{token}/sendMessage\"\n",
    "    try:\n",
    "        requests.post(url, data={\"chat_id\": chat_id, \"text\": text, \"parse_mode\": \"Markdown\"})\n",
    "    except Exception as e:\n",
    "        print(\"Telegram send error:\", e)\n",
    "        print(text)\n",
    "\n",
    "def _init_open_positions_df() -> pd.DataFrame:\n",
    "    cols = [\n",
    "        \"ticker\",\"entry_date\",\"entry_price\",\n",
    "        \"protective_exit\",\"use_hard_stop\",\"hard_stop_pct\",\n",
    "        \"use_trailing_stop\",\"trailing_stop_pct\",\"max_hold_days\",\n",
    "        \"peak_price\"\n",
    "    ]\n",
    "    return pd.DataFrame(columns=cols)\n",
    "\n",
    "def append_open_positions(ticker: str, signal_rows: pd.DataFrame, params: Dict) -> int:\n",
    "    \"\"\"Append new positions for this ticker for each signal date that's not already open.\"\"\"\n",
    "    if signal_rows.empty:\n",
    "        return 0\n",
    "\n",
    "    # Load current open positions\n",
    "    if OPEN_POSITIONS_CSV.exists():\n",
    "        open_df = pd.read_csv(OPEN_POSITIONS_CSV)\n",
    "    else:\n",
    "        open_df = _init_open_positions_df()\n",
    "\n",
    "    # Normalize types\n",
    "    if 'entry_date' in open_df.columns:\n",
    "        open_df['entry_date'] = pd.to_datetime(open_df['entry_date'], errors='coerce').dt.date\n",
    "    if 'ticker' in open_df.columns:\n",
    "        open_df['ticker'] = open_df['ticker'].astype(str)\n",
    "\n",
    "    existing_keys = set(zip(open_df.get('ticker', pd.Series(dtype=str)),\n",
    "                            open_df.get('entry_date', pd.Series(dtype='datetime64[ns]'))))\n",
    "\n",
    "    new_rows = []\n",
    "    for _, r in signal_rows.iterrows():\n",
    "        # r['date'] is ISO str; keep it as ISO to avoid dtype surprises in CSV\n",
    "        date_iso = str(r['date'])\n",
    "        key = (ticker, pd.to_datetime(date_iso).date())\n",
    "        if key in existing_keys:\n",
    "            continue\n",
    "        new_rows.append({\n",
    "            \"ticker\": ticker,\n",
    "            \"entry_date\": date_iso,  # keep ISO string; we'll parse on read\n",
    "            \"entry_price\": float(r['close']),\n",
    "            \"protective_exit\": bool(params.get('protective_exit', True)),\n",
    "            \"use_hard_stop\": bool(params.get('use_hard_stop', False)),\n",
    "            \"hard_stop_pct\": float(params.get('hard_stop_pct', 5.0)),\n",
    "            \"use_trailing_stop\": bool(params.get('use_trailing_stop', False)),\n",
    "            \"trailing_stop_pct\": float(params.get('trailing_stop_pct', 10.0)),\n",
    "            \"max_hold_days\": int(params.get('max_hold_days', 10)),\n",
    "            \"peak_price\": float(r['close'])\n",
    "        })\n",
    "\n",
    "    if not new_rows:\n",
    "        return 0\n",
    "\n",
    "    # Align columns to avoid FutureWarning behavior changes\n",
    "    add_df = pd.DataFrame(new_rows)\n",
    "    for col in open_df.columns:\n",
    "        if col not in add_df.columns:\n",
    "            add_df[col] = np.nan\n",
    "    add_df = add_df[open_df.columns] if list(open_df.columns) else add_df\n",
    "\n",
    "    open_df = pd.concat([open_df, add_df], ignore_index=True)\n",
    "    open_df.to_csv(OPEN_POSITIONS_CSV, index=False)\n",
    "    return len(new_rows)\n",
    "\n",
    "def main():\n",
    "    end = END_DATE\n",
    "    if end is None:\n",
    "        end = (datetime.now(timezone.utc) + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    date_buckets = {}  # date_iso -> [tickers]\n",
    "\n",
    "    total_new_positions = 0\n",
    "    for t in TICKER_LIST:\n",
    "        try:\n",
    "            df = get_stock_data(t, START_DATE, end, PARAMS)\n",
    "            if df is None:\n",
    "                continue\n",
    "            signals = find_entries(df, PARAMS, SCAN_LOOKBACK_DAYS)\n",
    "            if signals.empty:\n",
    "                continue\n",
    "\n",
    "            # group for telegram\n",
    "            for _, r in signals.iterrows():\n",
    "                d = r['date']\n",
    "                date_buckets.setdefault(d, []).append(t)\n",
    "\n",
    "            # append to open positions\n",
    "            added = append_open_positions(t, signals, PARAMS)\n",
    "            total_new_positions += added\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{t}] error: {e}\\n{traceback.format_exc()}\")\n",
    "\n",
    "    # Save date-wise CSV\n",
    "    for d, names in sorted(date_buckets.items()):\n",
    "        outpath = SIGNALS_DIR / f\"entries_{d}.csv\"\n",
    "        pd.DataFrame({\"ticker\": sorted(names)}).to_csv(outpath, index=False)\n",
    "\n",
    "    # Telegram summary\n",
    "    if date_buckets:\n",
    "        lines = [\"*DMA(10,20)+2-Green Entries*\"]\n",
    "        for d, names in sorted(date_buckets.items()):\n",
    "            lines.append(f\"*{d}*  — {', '.join(sorted(names))}\")\n",
    "        lines.append(f\"\\nAdded to open positions: {total_new_positions}\")\n",
    "        send_telegram(\"\\n\".join(lines))\n",
    "        print(\"Telegram sent.\")\n",
    "    else:\n",
    "        print(\"No entries in lookback window.\")\n",
    "\n",
    "    print(f\"Open positions file: {OPEN_POSITIONS_CSV.resolve()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".talib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
