{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c2db3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 23:45:04 | INFO | Universe: 500 symbols\n",
      "2025-11-06 23:47:50 | INFO | As-of date: 2025-11-04\n",
      "2025-11-06 23:47:51 | INFO | No new entries.\n",
      "2025-11-06 23:47:51 | INFO | No exits triggered.\n",
      "2025-11-06 23:47:51 | INFO | Plans written to: outputs/2025-11-06\n",
      "2025-11-06 23:47:51 | INFO | Entries CSV: outputs/2025-11-06/next_day_entries.csv\n",
      "2025-11-06 23:47:51 | INFO | Exits   CSV: outputs/2025-11-06/next_day_exits.csv\n",
      "2025-11-06 23:47:51 | INFO | Portfolio file: portfolio_positions.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Ichimoku Breakout ‚Äî Daily Scheduler with Entries, Exits, Telegram, and Auto-Append Portfolio\n",
    "--------------------------------------------------------------------------------------------\n",
    "\n",
    "RUN AFTER MARKET CLOSE (EOD).\n",
    "\n",
    "What it does\n",
    "------------\n",
    "1) Scans your universe and builds NEXT-DAY entry plan from Ichimoku breakout:\n",
    "     - Base: Close crosses ABOVE CloudTop AND SpanA > SpanB\n",
    "     - Optional confirmations: Tenkan>Kijun, Close>Kijun, Chikou proxy, min breakout %, min cloud thickness, Volume surge.\n",
    "2) Filters: within X% of 52w high; optional liquidity checks.\n",
    "3) Ranks by VOLAR vs benchmark + breakout distance; selects top-K entries.\n",
    "4) **Auto-append entries**: By default, assumes you will take the signals and appends them to\n",
    "   `portfolio_positions.csv` as `status=open` with:\n",
    "     - `entry_date` = next weekday after `as_of` (no holiday calendar),\n",
    "     - `entry_price` = estimated (today close proxy),\n",
    "     - `shares` = estimated via deployable cash cap,\n",
    "     - `stop_price` / `target_price` derived from Config.\n",
    "   (Toggle via `CFG.auto_append_entries`.)\n",
    "5) Generates EXIT plan for existing open trades (indicator exit or SL/TP hit on EOD).\n",
    "6) Writes artifacts under outputs/YYYY-MM-DD/:\n",
    "     - next_day_entries.{csv,json}\n",
    "     - next_day_exits.{csv,json}\n",
    "7) Optional Telegram alerts for entries/exits.\n",
    "\n",
    "Portfolio file\n",
    "--------------\n",
    "- Path: CFG.portfolio_path (default: portfolio_positions.csv)\n",
    "- Auto-created if missing, with headers.\n",
    "- Minimal columns: ticker, entry_date, entry_price, shares, status\n",
    "- Optional: stop_price, target_price, exit_date, exit_price, notes\n",
    "- De-dup: won‚Äôt add a new row if an OPEN row for the same ticker already exists.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "- Entry/exit executions are *not* automated; this is a planner/tracker.\n",
    "- Entry prices/qty are estimates (using EOD close as next-open proxy).\n",
    "- Exit checks are EOD-based (no intraday).\n",
    "- Next session date helper skips weekends only (not official holidays).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os, json, math, warnings, logging\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import yfinance as yf\n",
    "except Exception:\n",
    "    yf = None\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "except Exception:\n",
    "    requests = None\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# =========================\n",
    "# LOGGING\n",
    "# =========================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"ichimoku_scheduler_v3\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "@dataclass\n",
    "class Config:\n",
    "    # --- Data / I/O ---\n",
    "    start_date: str = \"2015-01-01\"\n",
    "    end_date: Optional[str] = None\n",
    "    static_symbols: Optional[List[str]] = None\n",
    "    static_symbols_path: Optional[str] = None\n",
    "    cache_dir: str = \"cache\"\n",
    "    out_root: str = \"outputs\"\n",
    "    portfolio_path: str = \"portfolio_positions.csv\"\n",
    "\n",
    "    benchmark_try: Tuple[str,...] = (\"^CNX500\",\"^CRSLDX\",\"^NSE500\",\"^NIFTY500\",\"^BSE500\",\"^NSEI\")\n",
    "\n",
    "    # --- Ichimoku (classic) ---\n",
    "    tenkan_len: int = 9\n",
    "    kijun_len:  int = 26\n",
    "    spanb_len:  int = 52\n",
    "\n",
    "    # --- Confirmation toggles ---\n",
    "    use_ichimoku_confirms: bool = True\n",
    "    confirm_tenkan_gt_kijun: bool = True\n",
    "    confirm_close_gt_kijun: bool = True\n",
    "    confirm_chikou_above: bool = True\n",
    "    confirm_min_break_pct: float = 0.005\n",
    "    confirm_min_cloud_thickness_pct: float = 0.01\n",
    "\n",
    "    use_volume_confirm: bool = True\n",
    "    volume_surge_mult: float = 1.5\n",
    "    volume_ma_len: int = 20\n",
    "\n",
    "    # --- Candidate filters ---\n",
    "    filter_52w_window: int = 252\n",
    "    within_pct_of_52w_high: float = 0.70\n",
    "    enable_basic_liquidity: bool = False\n",
    "    min_price_inr: float = 50.0\n",
    "    min_avg_vol_20d: float = 50_000.0\n",
    "\n",
    "    # --- Ranking & selection ---\n",
    "    volar_lookback: int = 252\n",
    "    top_k_daily: int = 5\n",
    "\n",
    "    # --- ‚ÄúPaper‚Äù cash model for entry sizing estimates ---\n",
    "    current_cash_inr: float = 500_000.0\n",
    "    deploy_cash_frac: float = 0.25\n",
    "    per_pick_min_inr: float = 5_000.0\n",
    "\n",
    "    # --- Stops/Targets (used for both entry plan estimates and exit checks) ---\n",
    "    stop_loss_pct: float = 0.05\n",
    "    target_pct: float    = 0.10\n",
    "\n",
    "    # --- Telegram (optional) ---\n",
    "    telegram_enabled: bool = False\n",
    "    telegram_bot_token: str = \"\"\n",
    "    telegram_chat_id: str = \"\"\n",
    "    alert_on_entries: bool = True\n",
    "    alert_on_exits: bool = True\n",
    "\n",
    "    # --- Auto-append entries into portfolio ---\n",
    "    auto_append_entries: bool = True      # << default ON per your request\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "# =========================\n",
    "# Helpers / I/O\n",
    "# =========================\n",
    "def ensure_dir(p: str):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def today_str():\n",
    "    return pd.Timestamp.today(tz=\"Asia/Kolkata\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def out_dir_for_today(root: str) -> str:\n",
    "    d = today_str()\n",
    "    path = os.path.join(root, d)\n",
    "    ensure_dir(path)\n",
    "    return path\n",
    "\n",
    "def next_weekday(d: pd.Timestamp) -> pd.Timestamp:\n",
    "    # simple next business day: skips Sat/Sun only (no holiday calendar)\n",
    "    x = d + pd.Timedelta(days=1)\n",
    "    while x.weekday() >= 5:  # 5=Sat, 6=Sun\n",
    "        x += pd.Timedelta(days=1)\n",
    "    return x\n",
    "\n",
    "def load_static_symbols(static_symbols: Optional[List[str]], static_symbols_path: Optional[str]) -> List[str]:\n",
    "    syms: List[str] = []\n",
    "    if static_symbols and len(static_symbols) > 0:\n",
    "        syms = list(static_symbols)\n",
    "    elif static_symbols_path and os.path.exists(static_symbols_path):\n",
    "        with open(static_symbols_path, \"r\") as f:\n",
    "            syms = [line.strip() for line in f if line.strip()]\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Provide CFG.static_symbols=[...] ('.NS' suffixes) or set CFG.static_symbols_path \"\n",
    "            \"to a file containing one symbol per line.\"\n",
    "        )\n",
    "    out = []\n",
    "    for s in syms:\n",
    "        s = s.strip().upper()\n",
    "        if not s.endswith(\".NS\") and not s.startswith(\"^\"):\n",
    "            s = f\"{s}.NS\"\n",
    "        out.append(s)\n",
    "    uniq = []\n",
    "    seen = set()\n",
    "    for s in out:\n",
    "        if s not in seen:\n",
    "            uniq.append(s); seen.add(s)\n",
    "    return uniq\n",
    "\n",
    "def fetch_prices(tickers: List[str], start: str, end: Optional[str], cache_dir: str) -> Dict[str, pd.DataFrame]:\n",
    "    ensure_dir(cache_dir)\n",
    "    data: Dict[str, pd.DataFrame] = {}\n",
    "    end = end or today_str()\n",
    "    for ticker in tickers:\n",
    "        cache_path = os.path.join(cache_dir, f\"{ticker.replace('^','_')}.parquet\")\n",
    "        if os.path.exists(cache_path):\n",
    "            try:\n",
    "                df = pd.read_parquet(cache_path)\n",
    "                if len(df) and pd.to_datetime(df.index[-1]).strftime(\"%Y-%m-%d\") >= end:\n",
    "                    data[ticker] = df\n",
    "                    continue\n",
    "            except Exception:\n",
    "                pass\n",
    "        try:\n",
    "            if yf is None:\n",
    "                raise RuntimeError(\"yfinance not available in this environment\")\n",
    "            df = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False, multi_level_index=False)\n",
    "            if df is None or df.empty:\n",
    "                continue\n",
    "            df = df.rename(columns=str.title)\n",
    "            df = df[['Open','High','Low','Close','Volume']].dropna()\n",
    "            df.index.name = \"date\"\n",
    "            df.to_parquet(cache_path)\n",
    "            data[ticker] = df\n",
    "        except Exception:\n",
    "            continue\n",
    "    return data\n",
    "\n",
    "# =========================\n",
    "# Indicators & Signals\n",
    "# =========================\n",
    "def rolling_mid(high: pd.Series, low: pd.Series, length: int) -> pd.Series:\n",
    "    hh = high.rolling(length).max()\n",
    "    ll = low.rolling(length).min()\n",
    "    return (hh + ll) / 2.0\n",
    "\n",
    "def compute_ichimoku_base(df: pd.DataFrame, cfg: Config) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"tenkan\"] = rolling_mid(out[\"High\"], out[\"Low\"], cfg.tenkan_len)\n",
    "    out[\"kijun\"]  = rolling_mid(out[\"High\"], out[\"Low\"], cfg.kijun_len)\n",
    "    out[\"span_a_base\"] = (out[\"tenkan\"] + out[\"kijun\"]) / 2.0\n",
    "    out[\"span_b_base\"] = rolling_mid(out[\"High\"], out[\"Low\"], cfg.spanb_len)\n",
    "    out[\"cloud_top\"] = out[[\"span_a_base\", \"span_b_base\"]].max(axis=1)\n",
    "    out[\"cloud_bot\"] = out[[\"span_a_base\", \"span_b_base\"]].min(axis=1)\n",
    "\n",
    "    out[\"cloud_thickness\"]  = (out[\"cloud_top\"] - out[\"cloud_bot\"]) / out[\"Close\"]\n",
    "    out[\"dist_above_cloud\"] = (out[\"Close\"] / out[\"cloud_top\"]) - 1.0\n",
    "\n",
    "    out[\"avg_vol_ma\"] = out[\"Volume\"].rolling(cfg.volume_ma_len).mean()\n",
    "    out[\"avg_vol_20\"] = out[\"Volume\"].rolling(20).mean()\n",
    "    out[\"high_52w\"]   = out[\"Close\"].rolling(cfg.filter_52w_window).max()\n",
    "    return out.dropna()\n",
    "\n",
    "def entry_signal_series(d: pd.DataFrame, cfg: Config) -> pd.Series:\n",
    "    cross_up  = (d[\"Close\"].shift(1) <= d[\"cloud_top\"].shift(1)) & (d[\"Close\"] > d[\"cloud_top\"])\n",
    "    span_bull = d[\"span_a_base\"] > d[\"span_b_base\"]\n",
    "    sig = cross_up & span_bull\n",
    "\n",
    "    if cfg.use_ichimoku_confirms:\n",
    "        confs = []\n",
    "        if cfg.confirm_tenkan_gt_kijun:\n",
    "            confs.append(d[\"tenkan\"] > d[\"kijun\"])\n",
    "        if cfg.confirm_close_gt_kijun:\n",
    "            confs.append(d[\"Close\"] > d[\"kijun\"])\n",
    "        if cfg.confirm_chikou_above:\n",
    "            confs.append(d[\"Close\"].shift(-26) > d[\"Close\"])\n",
    "        if cfg.confirm_min_break_pct and cfg.confirm_min_break_pct > 0:\n",
    "            confs.append(((d[\"Close\"] / d[\"cloud_top\"]) - 1.0) >= cfg.confirm_min_break_pct)\n",
    "        if cfg.confirm_min_cloud_thickness_pct and cfg.confirm_min_cloud_thickness_pct > 0:\n",
    "            confs.append(d[\"cloud_thickness\"] >= cfg.confirm_min_cloud_thickness_pct)\n",
    "        for c in confs:\n",
    "            sig = sig & c\n",
    "\n",
    "    if cfg.use_volume_confirm:\n",
    "        vol_surge = d[\"Volume\"] > (cfg.volume_surge_mult * d[\"avg_vol_ma\"])\n",
    "        sig = sig & vol_surge\n",
    "\n",
    "    return sig\n",
    "\n",
    "def indicator_exit_series(d: pd.DataFrame) -> pd.Series:\n",
    "    return (d[\"Close\"] < d[\"cloud_bot\"]) | (d[\"Close\"] < d[\"kijun\"])\n",
    "\n",
    "# =========================\n",
    "# Benchmark / VOLAR\n",
    "# =========================\n",
    "def pick_benchmark(benchmarks: Tuple[str,...], start: str, end: Optional[str], cache_dir: str) -> Tuple[str, pd.DataFrame]:\n",
    "    for t in benchmarks:\n",
    "        data = fetch_prices([t], start, end, cache_dir)\n",
    "        df = data.get(t)\n",
    "        if df is not None and not df.empty:\n",
    "            log.info(\"Using benchmark: %s\", t)\n",
    "            return t, df\n",
    "    idx = pd.date_range(start=start, end=end or today_str(), freq=\"B\")\n",
    "    df = pd.DataFrame({\"Close\": np.ones(len(idx))}, index=idx)\n",
    "    log.warning(\"No benchmark found; using synthetic flat series.\")\n",
    "    return \"SYNTH_BENCH\", df\n",
    "\n",
    "def compute_VOLAR(end_dt: pd.Timestamp, tickers: List[str], data_map: Dict[str, pd.DataFrame], bench_df: pd.DataFrame, lb: int) -> Dict[str, float]:\n",
    "    scores: Dict[str, float] = {}\n",
    "    bser = bench_df[\"Close\"].loc[:end_dt].pct_change().dropna().iloc[-lb:]\n",
    "    for t in tickers:\n",
    "        df = data_map.get(t)\n",
    "        if df is None or df.empty:\n",
    "            scores[t] = 0.0; continue\n",
    "        if end_dt not in df.index:\n",
    "            df = df[df.index <= end_dt]\n",
    "            if df.empty:\n",
    "                scores[t] = 0.0; continue\n",
    "        r = df[\"Close\"].loc[:end_dt].pct_change().dropna().iloc[-lb:]\n",
    "        common = pd.concat([r, bser], axis=1, keys=[\"s\",\"b\"]).dropna()\n",
    "        if common.shape[0] < max(20, int(0.4*lb)):\n",
    "            scores[t] = 0.0; continue\n",
    "        excess = common[\"s\"] - common[\"b\"]\n",
    "        vol = common[\"s\"].std(ddof=0)\n",
    "        scores[t] = 0.0 if vol <= 1e-8 else float((excess.mean() / vol) * math.sqrt(252.0))\n",
    "    return scores\n",
    "\n",
    "# =========================\n",
    "# Portfolio I/O\n",
    "# =========================\n",
    "REQ_POS_COLUMNS = [\"ticker\",\"entry_date\",\"entry_price\",\"shares\",\"status\"]\n",
    "\n",
    "def load_portfolio(path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(path):\n",
    "        empty = pd.DataFrame(columns=REQ_POS_COLUMNS + [\"exit_date\",\"exit_price\",\"stop_price\",\"target_price\",\"notes\"])\n",
    "        empty.to_csv(path, index=False)\n",
    "        return empty\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    for c in REQ_POS_COLUMNS:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "    if \"entry_date\" in df.columns:\n",
    "        df[\"entry_date\"] = pd.to_datetime(df[\"entry_date\"], errors=\"coerce\")\n",
    "    if \"exit_date\" in df.columns:\n",
    "        df[\"exit_date\"] = pd.to_datetime(df[\"exit_date\"], errors=\"coerce\")\n",
    "    for numc in [\"entry_price\",\"shares\",\"exit_price\",\"stop_price\",\"target_price\"]:\n",
    "        if numc in df.columns:\n",
    "            df[numc] = pd.to_numeric(df[numc], errors=\"coerce\")\n",
    "    if \"status\" in df.columns:\n",
    "        df[\"status\"] = df[\"status\"].astype(str).str.lower()\n",
    "    return df\n",
    "\n",
    "def save_portfolio(df: pd.DataFrame, path: str):\n",
    "    cols = REQ_POS_COLUMNS + [c for c in df.columns if c not in REQ_POS_COLUMNS]\n",
    "    df[cols].to_csv(path, index=False)\n",
    "\n",
    "def append_entries_to_portfolio(port_df: pd.DataFrame, entry_plan: pd.DataFrame, as_of: pd.Timestamp, cfg: Config) -> pd.DataFrame:\n",
    "    \"\"\"Append new entries as OPEN with next-session entry_date, avoiding duplicates.\"\"\"\n",
    "    if entry_plan is None or entry_plan.empty:\n",
    "        return port_df\n",
    "    next_sess = next_weekday(as_of)\n",
    "\n",
    "    # Existing open tickers (avoid duplicates)\n",
    "    open_tickers = set(port_df.loc[port_df[\"status\"] == \"open\", \"ticker\"].astype(str).str.upper())\n",
    "\n",
    "    rows = []\n",
    "    for _, r in entry_plan.iterrows():\n",
    "        tkr = str(r[\"ticker\"]).upper()\n",
    "        if tkr in open_tickers:\n",
    "            continue  # already open; skip\n",
    "\n",
    "        est_entry = float(r[\"est_entry_price\"])\n",
    "        est_shares = int(r.get(\"est_shares\", 0) or 0)\n",
    "        stop_p = est_entry * (1 - cfg.stop_loss_pct)\n",
    "        tgt_p  = est_entry * (1 + cfg.target_pct)\n",
    "\n",
    "        rows.append({\n",
    "            \"ticker\": tkr,\n",
    "            \"entry_date\": next_sess.strftime(\"%Y-%m-%d\"),\n",
    "            \"entry_price\": est_entry,\n",
    "            \"shares\": est_shares,\n",
    "            \"status\": \"open\",\n",
    "            \"stop_price\": stop_p,\n",
    "            \"target_price\": tgt_p,\n",
    "            \"notes\": \"auto-appended from next_day_entries\"\n",
    "        })\n",
    "\n",
    "    if not rows:\n",
    "        return port_df\n",
    "\n",
    "    add_df = pd.DataFrame(rows)\n",
    "    add_df[\"entry_date\"] = pd.to_datetime(add_df[\"entry_date\"])\n",
    "    combined = pd.concat([port_df, add_df], ignore_index=True)\n",
    "    return combined\n",
    "\n",
    "# =========================\n",
    "# Telegram\n",
    "# =========================\n",
    "def send_telegram(msg: str, cfg: Config):\n",
    "    if not cfg.telegram_enabled:\n",
    "        return\n",
    "    if not cfg.telegram_bot_token or not cfg.telegram_chat_id:\n",
    "        log.warning(\"Telegram enabled but missing token/chat id.\")\n",
    "        return\n",
    "    if requests is None:\n",
    "        log.warning(\"requests not available; cannot send Telegram.\")\n",
    "        return\n",
    "    try:\n",
    "        url = f\"https://api.telegram.org/bot{cfg.telegram_bot_token}/sendMessage\"\n",
    "        payload = {\"chat_id\": cfg.telegram_chat_id, \"text\": msg, \"parse_mode\": \"HTML\", \"disable_web_page_preview\": True}\n",
    "        r = requests.post(url, json=payload, timeout=10)\n",
    "        if r.status_code != 200:\n",
    "            log.warning(\"Telegram send failed: %s | %s\", r.status_code, r.text)\n",
    "    except Exception as e:\n",
    "        log.warning(\"Telegram exception: %s\", e)\n",
    "\n",
    "# =========================\n",
    "# Build ENTRY / EXIT plans\n",
    "# =========================\n",
    "def build_entry_plan(cfg: Config, data_map: Dict[str, pd.DataFrame], as_of: pd.Timestamp) -> pd.DataFrame:\n",
    "    enriched: Dict[str, pd.DataFrame] = {}\n",
    "    for t, df in data_map.items():\n",
    "        d = compute_ichimoku_base(df, cfg)\n",
    "        if as_of not in d.index:\n",
    "            d = d[d.index <= as_of]\n",
    "            if d.empty: \n",
    "                continue\n",
    "        enriched[t] = d\n",
    "\n",
    "    candidates = []\n",
    "    for t, d in enriched.items():\n",
    "        sig = entry_signal_series(d, cfg)\n",
    "        if as_of not in sig.index or not bool(sig.loc[as_of]):\n",
    "            continue\n",
    "        row = d.loc[as_of]\n",
    "        c = float(row[\"Close\"]); h52 = float(row[\"high_52w\"])\n",
    "        if not (h52 > 0 and c >= cfg.within_pct_of_52w_high * h52):\n",
    "            continue\n",
    "        if cfg.enable_basic_liquidity and (c < cfg.min_price_inr or row[\"avg_vol_20\"] < cfg.min_avg_vol_20d):\n",
    "            continue\n",
    "        candidates.append({\n",
    "            \"ticker\": t,\n",
    "            \"signal_date\": as_of,\n",
    "            \"close\": c,\n",
    "            \"cloud_top\": float(row[\"cloud_top\"]),\n",
    "            \"cloud_bot\": float(row[\"cloud_bot\"]),\n",
    "            \"span_a\": float(row[\"span_a_base\"]),\n",
    "            \"span_b\": float(row[\"span_b_base\"]),\n",
    "            \"kijun\": float(row[\"kijun\"]),\n",
    "            \"tenkan\": float(row[\"tenkan\"]),\n",
    "            \"high_52w\": float(row[\"high_52w\"]),\n",
    "            \"cloud_thickness\": float(row[\"cloud_thickness\"]),\n",
    "            \"dist_above_cloud\": float(row[\"dist_above_cloud\"]),\n",
    "            \"score\": float(1000.0 * row[\"dist_above_cloud\"]),\n",
    "            \"volume\": float(row[\"Volume\"]),\n",
    "            \"avg_vol_ma\": float(row[\"avg_vol_ma\"]),\n",
    "        })\n",
    "\n",
    "    if not candidates:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    cands_df = pd.DataFrame(candidates).reset_index(drop=True)\n",
    "\n",
    "    bench_tkr, bench_df = pick_benchmark(cfg.benchmark_try, cfg.start_date, cfg.end_date, cfg.cache_dir)\n",
    "    volar_map = compute_VOLAR(as_of, cands_df[\"ticker\"].tolist(), data_map, bench_df, cfg.volar_lookback)\n",
    "    cands_df[\"volar\"] = cands_df[\"ticker\"].map(volar_map)\n",
    "    cands_df[\"rank_score\"] = cands_df[\"volar\"].fillna(0.0) + 0.1*cands_df[\"score\"].fillna(0.0)\n",
    "    cands_df = cands_df.sort_values(\"rank_score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    top = cands_df.head(cfg.top_k_daily).copy()\n",
    "    if top.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    deploy_cash = float(cfg.current_cash_inr) * float(cfg.deploy_cash_frac)\n",
    "    per_pick_inr = deploy_cash / top.shape[0] if top.shape[0] > 0 else 0.0\n",
    "\n",
    "    rows = []\n",
    "    for i, r in top.iterrows():\n",
    "        px = float(r[\"close\"])\n",
    "        shares = 0 if (px <= 0 or per_pick_inr < cfg.per_pick_min_inr) else int(math.floor(per_pick_inr / px))\n",
    "        est_entry = px\n",
    "        est_stop  = est_entry * (1 - cfg.stop_loss_pct)\n",
    "        est_tgt   = est_entry * (1 + cfg.target_pct)\n",
    "        reason_bits = [\"Ichimoku breakout: Close‚ÜëCloudTop & SpanA>SpanB\"]\n",
    "        if cfg.use_ichimoku_confirms:\n",
    "            sub=[]; \n",
    "            if cfg.confirm_tenkan_gt_kijun: sub.append(\"Tenkan>Kijun\")\n",
    "            if cfg.confirm_close_gt_kijun:  sub.append(\"Close>Kijun\")\n",
    "            if cfg.confirm_chikou_above:    sub.append(\"ChikouProxy‚Üë\")\n",
    "            if cfg.confirm_min_break_pct and cfg.confirm_min_break_pct>0: sub.append(f\"Break‚â•{cfg.confirm_min_break_pct:.2%}\")\n",
    "            if cfg.confirm_min_cloud_thickness_pct and cfg.confirm_min_cloud_thickness_pct>0: sub.append(f\"Kumo‚â•{cfg.confirm_min_cloud_thickness_pct:.2%}\")\n",
    "            if sub: reason_bits.append(\"IchimokuConf[\" + \",\".join(sub) + \"]\")\n",
    "        if cfg.use_volume_confirm:\n",
    "            reason_bits.append(f\"Vol>{cfg.volume_surge_mult:.1f}√óMA{cfg.volume_ma_len}\")\n",
    "\n",
    "        rows.append({\n",
    "            \"rank\": int(i+1),\n",
    "            \"ticker\": r[\"ticker\"],\n",
    "            \"signal_date\": r[\"signal_date\"].strftime(\"%Y-%m-%d\"),\n",
    "            \"rank_score\": float(r[\"rank_score\"]),\n",
    "            \"volar\": float(r[\"volar\"]),\n",
    "            \"close\": float(r[\"close\"]),\n",
    "            \"cloud_top\": float(r[\"cloud_top\"]),\n",
    "            \"cloud_bot\": float(r[\"cloud_bot\"]),\n",
    "            \"kijun\": float(r[\"kijun\"]),\n",
    "            \"tenkan\": float(r[\"tenkan\"]),\n",
    "            \"dist_above_cloud\": float(r[\"dist_above_cloud\"]),\n",
    "            \"cloud_thickness\": float(r[\"cloud_thickness\"]),\n",
    "            \"within_52w_pct\": float(r[\"close\"]/r[\"high_52w\"]) if r[\"high_52w\"]>0 else np.nan,\n",
    "            \"est_entry_price\": float(est_entry),\n",
    "            \"est_stop_price\": float(est_stop),\n",
    "            \"est_target_price\": float(est_tgt),\n",
    "            \"est_shares\": int(shares),\n",
    "            \"est_alloc_inr\": float(shares * est_entry),\n",
    "            \"reason\": \" ; \".join(reason_bits),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def build_exit_plan(cfg: Config, data_map: Dict[str, pd.DataFrame], as_of: pd.Timestamp, portfolio_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if portfolio_df is None or portfolio_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    open_pos = portfolio_df[portfolio_df[\"status\"] == \"open\"].copy()\n",
    "    if open_pos.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if \"stop_price\" not in open_pos.columns:\n",
    "        open_pos[\"stop_price\"] = np.nan\n",
    "    if \"target_price\" not in open_pos.columns:\n",
    "        open_pos[\"target_price\"] = np.nan\n",
    "    for idx, r in open_pos.iterrows():\n",
    "        if pd.isna(r.get(\"stop_price\")) and not pd.isna(r.get(\"entry_price\")):\n",
    "            open_pos.at[idx, \"stop_price\"] = float(r[\"entry_price\"]) * (1 - cfg.stop_loss_pct)\n",
    "        if pd.isna(r.get(\"target_price\")) and not pd.isna(r.get(\"entry_price\")):\n",
    "            open_pos.at[idx, \"target_price\"] = float(r[\"entry_price\"]) * (1 + cfg.target_pct)\n",
    "\n",
    "    rows = []\n",
    "    for _, pos in open_pos.iterrows():\n",
    "        tkr = str(pos[\"ticker\"]).upper().strip()\n",
    "        df = data_map.get(tkr)\n",
    "        if df is None or df.empty:\n",
    "            continue\n",
    "        d = compute_ichimoku_base(df, cfg)\n",
    "        if as_of not in d.index:\n",
    "            d = d[d.index <= as_of]\n",
    "            if d.empty:\n",
    "                continue\n",
    "        row = d.loc[as_of]\n",
    "        close = float(row[\"Close\"])\n",
    "        ind_exit = bool(indicator_exit_series(d).loc[as_of])\n",
    "\n",
    "        eprice = float(pos.get(\"entry_price\", np.nan))\n",
    "        stop_p = float(pos.get(\"stop_price\", np.nan))\n",
    "        tgt_p  = float(pos.get(\"target_price\", np.nan))\n",
    "        stop_hit = (not np.isnan(stop_p)) and (close <= stop_p)\n",
    "        tgt_hit  = (not np.isnan(tgt_p)) and (close >= tgt_p)\n",
    "\n",
    "        reason_bits = []\n",
    "        if ind_exit: reason_bits.append(\"IndicatorExit (Close<KumoBot or Close<Kijun)\")\n",
    "        if stop_hit: reason_bits.append(\"Close<=Stop (EOD)\")\n",
    "        if tgt_hit:  reason_bits.append(\"Close>=Target (EOD)\")\n",
    "\n",
    "        if reason_bits:\n",
    "            rows.append({\n",
    "                \"ticker\": tkr,\n",
    "                \"signal_date\": as_of.strftime(\"%Y-%m-%d\"),\n",
    "                \"close\": close,\n",
    "                \"cloud_bot\": float(row[\"cloud_bot\"]),\n",
    "                \"kijun\": float(row[\"kijun\"]),\n",
    "                \"entry_date\": pos.get(\"entry_date\").strftime(\"%Y-%m-%d\") if pd.notna(pos.get(\"entry_date\")) else \"\",\n",
    "                \"entry_price\": eprice,\n",
    "                \"stop_price\": stop_p,\n",
    "                \"target_price\": tgt_p,\n",
    "                \"advice\": \"EXIT ON OPEN\",\n",
    "                \"reason\": \" ; \".join(reason_bits)\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "def main():\n",
    "    # === Configure your universe here ===\n",
    "    # CFG.static_symbols = [\n",
    "    #     'RELIANCE.NS','TCS.NS','INFY.NS','HDFCBANK.NS','ICICIBANK.NS',\n",
    "    #     'LT.NS','ITC.NS','SBIN.NS','BHARTIARTL.NS','HINDUNILVR.NS'\n",
    "    # ]\n",
    "    CFG.static_symbols_path = \"nifty500.txt\"\n",
    "    CFG.use_ichimoku_confirms = True\n",
    "    CFG.use_volume_confirm = False\n",
    "    # Load universe data\n",
    "    symbols = load_static_symbols(CFG.static_symbols, CFG.static_symbols_path)\n",
    "    log.info(\"Universe: %d symbols\", len(symbols))\n",
    "    data_map = fetch_prices(symbols, CFG.start_date, CFG.end_date, CFG.cache_dir)\n",
    "    data_map = {k:v for k,v in data_map.items() if v is not None and not v.empty}\n",
    "    if not data_map:\n",
    "        log.error(\"No data fetched.\")\n",
    "        return\n",
    "\n",
    "    # As-of date (EOD)\n",
    "    as_of = max(df.index[-1] for df in data_map.values())\n",
    "    log.info(\"As-of date: %s\", as_of.date())\n",
    "\n",
    "    # ENTRY plan (new trades for next session)\n",
    "    entry_plan = build_entry_plan(CFG, data_map, as_of)\n",
    "\n",
    "    # Portfolio (auto-create if missing)\n",
    "    portfolio_df = load_portfolio(CFG.portfolio_path)\n",
    "\n",
    "    # Auto-append entries as OPEN (assume we take them)\n",
    "    if CFG.auto_append_entries and entry_plan is not None and not entry_plan.empty:\n",
    "        before_n = len(portfolio_df)\n",
    "        portfolio_df = append_entries_to_portfolio(portfolio_df, entry_plan, as_of, CFG)\n",
    "        after_n = len(portfolio_df)\n",
    "        if after_n > before_n:\n",
    "            save_portfolio(portfolio_df, CFG.portfolio_path)\n",
    "            log.info(\"Auto-appended %d entries to portfolio.\", after_n - before_n)\n",
    "\n",
    "    # EXIT plan for current open trades\n",
    "    exit_plan = build_exit_plan(CFG, data_map, as_of, portfolio_df)\n",
    "\n",
    "    # Write artifacts\n",
    "    out_dir = out_dir_for_today(CFG.out_root)\n",
    "\n",
    "    # Entries\n",
    "    entries_csv = os.path.join(out_dir, \"next_day_entries.csv\")\n",
    "    entries_json = os.path.join(out_dir, \"next_day_entries.json\")\n",
    "    if entry_plan is None or entry_plan.empty:\n",
    "        with open(entries_json, \"w\") as f:\n",
    "            json.dump({\"as_of_date\": str(as_of.date()), \"entries\": []}, f, indent=2)\n",
    "        pd.DataFrame(columns=[\"rank\",\"ticker\",\"signal_date\",\"est_entry_price\",\"est_stop_price\",\"est_target_price\",\"est_shares\"]).to_csv(entries_csv, index=False)\n",
    "        log.info(\"No new entries.\")\n",
    "    else:\n",
    "        entry_plan.to_csv(entries_csv, index=False)\n",
    "        payload = {\n",
    "            \"as_of_date\": str(as_of.date()),\n",
    "            \"config\": {\n",
    "                \"use_ichimoku_confirms\": CFG.use_ichimoku_confirms,\n",
    "                \"use_volume_confirm\": CFG.use_volume_confirm,\n",
    "                \"top_k_daily\": CFG.top_k_daily,\n",
    "                \"within_pct_of_52w_high\": CFG.within_pct_of_52w_high,\n",
    "                \"deploy_cash_frac\": CFG.deploy_cash_frac,\n",
    "                \"stop_loss_pct\": CFG.stop_loss_pct,\n",
    "                \"target_pct\": CFG.target_pct,\n",
    "                \"auto_append_entries\": CFG.auto_append_entries,\n",
    "            },\n",
    "            \"entries\": entry_plan.to_dict(orient=\"records\"),\n",
    "        }\n",
    "        with open(entries_json, \"w\") as f:\n",
    "            json.dump(payload, f, indent=2)\n",
    "        if CFG.telegram_enabled and CFG.alert_on_entries:\n",
    "            lines = [f\"üìà <b>Next-Day Entries ({as_of.date()})</b>\"]\n",
    "            for _, r in entry_plan.iterrows():\n",
    "                lines.append(f\"{int(r['rank']):>2}. <b>{r['ticker']}</b> ~{r['est_entry_price']:.2f} | SL {r['est_stop_price']:.2f} | TP {r['est_target_price']:.2f}\")\n",
    "            send_telegram(\"\\n\".join(lines), CFG)\n",
    "\n",
    "    # Exits\n",
    "    exits_csv = os.path.join(out_dir, \"next_day_exits.csv\")\n",
    "    exits_json = os.path.join(out_dir, \"next_day_exits.json\")\n",
    "    if exit_plan is None or exit_plan.empty:\n",
    "        with open(exits_json, \"w\") as f:\n",
    "            json.dump({\"as_of_date\": str(as_of.date()), \"exits\": []}, f, indent=2)\n",
    "        pd.DataFrame(columns=[\"ticker\",\"signal_date\",\"advice\",\"reason\"]).to_csv(exits_csv, index=False)\n",
    "        log.info(\"No exits triggered.\")\n",
    "    else:\n",
    "        exit_plan.to_csv(exits_csv, index=False)\n",
    "        payload = {\n",
    "            \"as_of_date\": str(as_of.date()),\n",
    "            \"exits\": exit_plan.to_dict(orient=\"records\"),\n",
    "        }\n",
    "        with open(exits_json, \"w\") as f:\n",
    "            json.dump(payload, f, indent=2)\n",
    "        if CFG.telegram_enabled and CFG.alert_on_exits:\n",
    "            lines = [f\"üì§ <b>Next-Day Exits ({as_of.date()})</b>\"]\n",
    "            for _, r in exit_plan.iterrows():\n",
    "                lines.append(f\"‚Ä¢ <b>{r['ticker']}</b> EXIT ON OPEN | close {r['close']:.2f} | SL {r['stop_price']:.2f} | TP {r['target_price']:.2f} | {r['reason']}\")\n",
    "            send_telegram(\"\\n\".join(lines), CFG)\n",
    "\n",
    "    log.info(\"Plans written to: %s\", out_dir)\n",
    "    log.info(\"Entries CSV: %s\", entries_csv)\n",
    "    log.info(\"Exits   CSV: %s\", exits_csv)\n",
    "    log.info(\"Portfolio file: %s\", CFG.portfolio_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".talib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
