{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffa2479b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1198353847.py, line 126)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 126\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mmlp_units=,\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Step 1: Import Necessary Libraries\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import date, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==============================================================================\n",
    "# Step 2: Define Model Components (Time2Vec and Transformer Encoder)\n",
    "# ==============================================================================\n",
    "\n",
    "class Time2Vec(layers.Layer):\n",
    "    \"\"\"Custom Keras layer for Time2Vec embedding.\"\"\"\n",
    "    def __init__(self, kernel_size=1, **kwargs):\n",
    "        super(Time2Vec, self).__init__(**kwargs)\n",
    "        self.k = kernel_size\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.wb = self.add_weight(name='wb', shape=(input_shape[-1],), initializer='uniform', trainable=True)\n",
    "        self.bb = self.add_weight(name='bb', shape=(input_shape[-1],), initializer='uniform', trainable=True)\n",
    "        self.wa = self.add_weight(name='wa', shape=(1, input_shape[-2], self.k), initializer='uniform', trainable=True)\n",
    "        self.ba = self.add_weight(name='ba', shape=(1, input_shape[-2], self.k), initializer='uniform', trainable=True)\n",
    "        super(Time2Vec, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        bias = self.wb * inputs + self.bb\n",
    "        dp = K.dot(inputs, self.wa) + self.ba\n",
    "        wgts = K.sin(dp)\n",
    "        ret = K.concatenate([K.expand_dims(bias, -1), wgts], -1)\n",
    "        ret = K.reshape(ret, (-1, ret.shape, ret.shape * ret.shape))\n",
    "        return ret\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape, input_shape, input_shape * (self.k + 1))\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    \"\"\"A single Transformer encoder block.\"\"\"\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res\n",
    "\n",
    "# ==============================================================================\n",
    "# Step 3: Define Model Building Function\n",
    "# ==============================================================================\n",
    "\n",
    "def build_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks,\n",
    "                mlp_units, dropout=0, mlp_dropout=0, time2vec_dim=1):\n",
    "    \"\"\"Builds the complete Transformer model.\"\"\"\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    time_embedding = Time2Vec(kernel_size=time2vec_dim)(inputs)\n",
    "    x = layers.Concatenate(axis=-1)([inputs, time_embedding])\n",
    "    \n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    \n",
    "    outputs = layers.Dense(4)(x)  # Predicting 4 features: Open, High, Low, Close\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# ==============================================================================\n",
    "# Step 4: Data Acquisition and Preprocessing\n",
    "# ==============================================================================\n",
    "\n",
    "def create_dataset(dataset, look_back=60):\n",
    "    \"\"\"Creates input sequences (X) and corresponding target values (y).\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        X.append(dataset[i:(i + look_back), :])\n",
    "        y.append(dataset[i + look_back, :])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# --- Configuration ---\n",
    "TICKER_SYMBOL = \"NVDA\"\n",
    "LOOKBACK_WINDOW = 60\n",
    "START_DATE = (date.today() - timedelta(days=5*365)).strftime('%Y-%m-%d')\n",
    "END_DATE = date.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# --- Data Loading ---\n",
    "stock_data = yf.download(TICKER_SYMBOL, start=START_DATE, end=END_DATE)\n",
    "ohlc_data = stock_data[['Open', 'High', 'Low', 'Close']]\n",
    "\n",
    "# --- Normalization and Splitting ---\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "split_fraction = 0.9\n",
    "split_index = int(len(ohlc_data) * split_fraction)\n",
    "train_data = ohlc_data[:split_index]\n",
    "test_data = ohlc_data[split_index:]\n",
    "\n",
    "scaled_train_data = scaler.fit_transform(train_data)\n",
    "scaled_test_data = scaler.transform(test_data)\n",
    "\n",
    "# --- Create Windowed Datasets ---\n",
    "X_train, y_train = create_dataset(scaled_train_data, LOOKBACK_WINDOW)\n",
    "X_test, y_test = create_dataset(scaled_test_data, LOOKBACK_WINDOW)\n",
    "\n",
    "# ==============================================================================\n",
    "# Step 5: Model Training\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Build Model ---\n",
    "model = build_model(\n",
    "    input_shape=(LOOKBACK_WINDOW, X_train.shape),\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=,\n",
    "    dropout=0.25,\n",
    "    mlp_dropout=0.4,\n",
    "    time2vec_dim=64\n",
    ")\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='mean_squared_error')\n",
    "model.summary()\n",
    "\n",
    "# --- Callbacks ---\n",
    "checkpoint_path = \"best_model.weights.h5\"\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    ")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# --- Train ---\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=150,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[model_checkpoint, early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# Step 6: Evaluation and Visualization\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Load Best Weights ---\n",
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "# --- Make Predictions on Test Set ---\n",
    "test_predictions_scaled = model.predict(X_test)\n",
    "test_predictions = scaler.inverse_transform(test_predictions_scaled)\n",
    "\n",
    "# --- Plot Results ---\n",
    "predicted_close_prices = test_predictions[:, 3]\n",
    "actual_close_prices = test_data['Close'].values\n",
    "prediction_dates = test_data.index\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(prediction_dates, actual_close_prices, color='blue', label='Actual Close Price')\n",
    "plt.plot(prediction_dates, predicted_close_prices, color='red', linestyle='--', label='Predicted Close Price')\n",
    "plt.title(f'{TICKER_SYMBOL} Stock Price Prediction (Test Set)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price (USD)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Performance Metrics ---\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "mse = mean_squared_error(actual_close_prices, predicted_close_prices)\n",
    "mae = mean_absolute_error(actual_close_prices, predicted_close_prices)\n",
    "print(f\"\\n--- Model Performance on Test Set ---\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Step 7: Predict the Next Day\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Prepare Input for Next-Day Prediction ---\n",
    "# Use the last LOOKBACK_WINDOW days from the entire available dataset\n",
    "full_scaled_data = scaler.transform(ohlc_data)\n",
    "last_sequence = full_scaled_data\n",
    "last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "\n",
    "# --- Predict and Inverse Transform ---\n",
    "predicted_scaled_ohlc = model.predict(last_sequence)\n",
    "predicted_ohlc = scaler.inverse_transform(predicted_scaled_ohlc)\n",
    "next_day_date = ohlc_data.index[-1] + timedelta(days=1)\n",
    "\n",
    "print(f\"\\n--- Forecast for {next_day_date.strftime('%Y-%m-%d')} ---\")\n",
    "print(f\"Predicted Open:  ${predicted_ohlc:.2f}\")\n",
    "print(f\"Predicted High:  ${predicted_ohlc:.2f}\")\n",
    "print(f\"Predicted Low:   ${predicted_ohlc:.2f}\")\n",
    "print(f\"Predicted Close: ${predicted_ohlc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
