{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac013dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trafilatura in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (2.0.0)\n",
      "Requirement already satisfied: readability-lxml in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (0.8.4.1)\n",
      "Requirement already satisfied: lxml in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (5.4.0)\n",
      "Requirement already satisfied: newspaper3k in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (0.2.8)\n",
      "Requirement already satisfied: certifi in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from trafilatura) (2025.7.14)\n",
      "Requirement already satisfied: charset_normalizer>=3.4.0 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from trafilatura) (3.4.2)\n",
      "Requirement already satisfied: courlan>=1.3.2 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from trafilatura) (1.3.2)\n",
      "Requirement already satisfied: htmldate>=1.9.2 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from trafilatura) (1.9.3)\n",
      "Requirement already satisfied: justext>=3.0.1 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from trafilatura) (3.0.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from trafilatura) (2.5.0)\n",
      "Requirement already satisfied: chardet in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from readability-lxml) (5.2.0)\n",
      "Requirement already satisfied: cssselect in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from readability-lxml) (1.3.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from newspaper3k) (4.13.4)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from newspaper3k) (11.3.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from newspaper3k) (6.0.2)\n",
      "Requirement already satisfied: nltk>=3.2.1 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from newspaper3k) (3.9.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from newspaper3k) (2.32.4)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from newspaper3k) (6.0.11)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from newspaper3k) (5.3.0)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from newspaper3k) (2.9.0.post0)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.14.1)\n",
      "Requirement already satisfied: babel>=2.16.0 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from courlan>=1.3.2->trafilatura) (2.17.0)\n",
      "Requirement already satisfied: tld>=0.13 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from courlan>=1.3.2->trafilatura) (0.13.1)\n",
      "Requirement already satisfied: six in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
      "Requirement already satisfied: sgmllib3k in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: dateparser>=1.1.2 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from htmldate>=1.9.2->trafilatura) (1.2.2)\n",
      "Requirement already satisfied: pytz>=2024.2 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2025.2)\n",
      "Requirement already satisfied: regex>=2024.9.11 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2024.11.6)\n",
      "Requirement already satisfied: tzlocal>=0.2 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (5.3.1)\n",
      "Requirement already satisfied: lxml_html_clean in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from lxml[html_clean]->readability-lxml) (0.4.2)\n",
      "Requirement already satisfied: click in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from nltk>=3.2.1->newspaper3k) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from nltk>=3.2.1->newspaper3k) (1.5.1)\n",
      "Requirement already satisfied: tqdm in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
      "Requirement already satisfied: requests-file>=1.4 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from tldextract>=2.0.1->newspaper3k) (3.19.1)\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers torch requests beautifulsoup4 pandas nltk feedparser tf-keras\n",
    "!pip install trafilatura readability-lxml lxml newspaper3k\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd7195e",
   "metadata": {},
   "source": [
    "# News-Sentiment DMA Screener — README\n",
    "\n",
    "A single-file Python tool that:\n",
    "\n",
    "* Fetches India equity news via **Google News RSS** using **dynamic company names** loaded from **NSE index CSVs** (Nifty 50 / Next 50 / Bank / 500 / Midcap 100 / Smallcap 100).\n",
    "* Filters to an **allowlist of publishers** (Moneycontrol, Economic Times, Mint/LiveMint, Business Standard, CNBC TV18) with a safe **fallback** if none match.\n",
    "* Extracts **full article text** (trafilatura → readability-lxml → newspaper3k) + AMP/canonical cleanup.\n",
    "* Runs **FinBERT** sentiment on **headlines** and **articles**.\n",
    "* Applies **recency weighting** (≤24h = 1.5×, 24–48h = 1.2×).\n",
    "* Outputs a per-ticker **bias**: `LONG` / `SHORT` / `NEUTRAL`.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) What you get\n",
    "\n",
    "For each ticker, the script prints a table:\n",
    "\n",
    "| column           | meaning                                                     |\n",
    "| ---------------- | ----------------------------------------------------------- |\n",
    "| `symbol`         | NSE ticker with `.NS` suffix (e.g., `LUPIN.NS`)             |\n",
    "| `headline_avg`   | Recency-weighted sentiment from headlines only (−1..+1)     |\n",
    "| `article_avg`    | Average sentiment from extracted article bodies / summaries |\n",
    "| `combined_score` | Final score (prefers article if available; else headline)   |\n",
    "| `bias`           | `LONG` if > +0.05, `SHORT` if < −0.05, else `NEUTRAL`       |\n",
    "| `n_headlines`    | Number of headlines considered                              |\n",
    "| `n_articles`     | Number of articles whose text was extracted / used          |\n",
    "\n",
    "> Tip: If `n_articles` is 0, the script still uses headline sentiment (and, if enabled, RSS summary fallback).\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Requirements\n",
    "\n",
    "* Python **3.10+**\n",
    "* Packages:\n",
    "\n",
    "  * Always: `requests`, `feedparser`, `pandas`, `transformers`\n",
    "  * Optional (recommended for better extraction):\n",
    "    `trafilatura`, `readability-lxml`, `lxml`, `newspaper3k`\n",
    "* Model: `ProsusAI/finbert` (downloaded automatically by 🤗 Transformers)\n",
    "* macOS/Apple Silicon: MPS is fine (transformers prints `Device set to use mps:0`)\n",
    "\n",
    "Install:\n",
    "\n",
    "```bash\n",
    "pip install requests feedparser pandas transformers\n",
    "# optional but recommended:\n",
    "pip install trafilatura readability-lxml lxml newspaper3k\n",
    "```\n",
    "\n",
    "> If you use a GPU/Metal, Transformers will auto-choose the device. No config needed.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) How it works\n",
    "\n",
    "1. **Dynamic company names**\n",
    "   The script warms up an NSE session, downloads multiple index CSVs, and builds `NAME_MAP = {SYMBOL: \"Company Name\"}`.\n",
    "   Example: `\"LUPIN\" → \"Lupin\"`, `\"CENTRALBK\" → \"Central Bank of India\"`.\n",
    "\n",
    "2. **News fetching (Google News RSS)**\n",
    "   For each ticker, it queries with:\n",
    "\n",
    "   * `\"Company Name\" stock india`\n",
    "   * `Company Name shares`\n",
    "   * `SYMBOL stock india`\n",
    "     It keeps **allowlisted publishers** if present; otherwise it **returns all** to avoid empty results.\n",
    "\n",
    "3. **Text extraction**\n",
    "   For each link, it:\n",
    "\n",
    "   * Canonicalizes/cleans the URL (removes AMP and tracking where safe).\n",
    "   * Tries `trafilatura` → `readability` → `newspaper3k` in order.\n",
    "   * Uses article text if ≥120 chars; else falls back to RSS summary (if available).\n",
    "\n",
    "4. **Sentiment & weighting**\n",
    "\n",
    "   * Headline sentiment: recency-weighted (≤24h: 1.5×, 24–48h: 1.2×, else 1.0×).\n",
    "   * Article sentiment: average of chunked body text (512-token budget heuristic).\n",
    "   * Combined: `0.7 * article_avg + 0.3 * headline_avg` *if* any articles were read; otherwise `headline_avg`.\n",
    "\n",
    "5. **Bias rule**\n",
    "\n",
    "   * `combined_score > +0.05` → **LONG**\n",
    "   * `combined_score < −0.05` → **SHORT**\n",
    "   * otherwise **NEUTRAL**\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Running it\n",
    "\n",
    "Edit the `tickers` list at the bottom and run:\n",
    "\n",
    "```bash\n",
    "python news_sentiment_dma.py\n",
    "```\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    tickers = [\n",
    "        \"CENTRALBK.NS\",\n",
    "        \"LUPIN.NS\",\n",
    "        \"UCOBANK.NS\",\n",
    "    ]\n",
    "    sentiment_df = build_sentiment_table(tickers)\n",
    "    print(sentiment_df)\n",
    "```\n",
    "\n",
    "> Note: In Python, each item in the list needs a comma. A missing comma will concatenate adjacent strings.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Configuration knobs\n",
    "\n",
    "* **Allowlist domains**: update `ALLOWLIST` to tighten/loosen publisher filtering.\n",
    "* **Recency weighting**: tweak `recency_weight()` thresholds/weights.\n",
    "* **Neutral band**: adjust `SENTIMENT_NEUTRAL_BAND` (default 0.05).\n",
    "* **Extraction threshold**: change `MIN_ARTICLE_CHARS` (default 200; logic uses 120 in the final gate).\n",
    "* **Max items**: `MAX_HEADLINES`, `MAX_ARTICLES_PER_TICKER`.\n",
    "* **Token budget**: `MAX_TOKENS_PER_ARTICLE` (rough 4 chars/token heuristic).\n",
    "* **Dynamic names**: extend/override `DEFAULT_NSE_INDEX_URLS` or add entries to `EXTRA_NAME_MAP`.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Troubleshooting\n",
    "\n",
    "* **All zeros / no headlines**\n",
    "\n",
    "  * Your query might be too strict or network is blocked. Try printing raw rows:\n",
    "\n",
    "    ```python\n",
    "    raw_df = pd.concat([fetch_news_for_ticker(\"LUPIN.NS\")], ignore_index=True)\n",
    "    print(raw_df[[\"title\",\"link\",\"allowlisted\"]])\n",
    "    ```\n",
    "  * If allowlist filters out everything, the script **falls back** to returning all publishers.\n",
    "\n",
    "* **`n_articles = 0`**\n",
    "\n",
    "  * Many finance sites are AMP/JS/paywalled; extraction can fail.\n",
    "  * Lower thresholds (`MIN_ARTICLE_CHARS`), ensure optional libs are installed, and rely on RSS **summary fallback** (already enabled).\n",
    "\n",
    "* **NSE CSV errors**\n",
    "\n",
    "  * NSE can be finicky without cookies. The script warms up a session; re-run if a CSV fails transiently.\n",
    "  * You can limit to fewer CSV URLs if needed.\n",
    "\n",
    "* **Model errors**\n",
    "\n",
    "  * If Transformers downloads stall, try `pip install -U transformers` and ensure internet access.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Extending it\n",
    "\n",
    "* **Combine with DMA/RSI screener**\n",
    "  Use `combined_score`/`bias` as a **news gate**: only consider longs where both **technicals** (DMA/RSI) and **news** are bullish.\n",
    "\n",
    "* **Add Bing News RSS fallback**\n",
    "  You can implement a second fetcher to merge Bing RSS results if Google News is sparse.\n",
    "\n",
    "* **Recency within articles**\n",
    "  Weight article paragraphs by detected timestamps or TF-IDF to emphasize fresh info.\n",
    "\n",
    "* **Caching**\n",
    "  Cache `NAME_MAP` (JSON) and news results to speed up repeated runs.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Notes & disclaimers\n",
    "\n",
    "* This is **for research/education**. It’s not investment advice. Backtest before live trading.\n",
    "* Respect publishers’ **robots/terms**; avoid aggressive scraping.\n",
    "* Sentiment models can misread sarcasm, corporate wording, or headlines that invert sentiment (e.g., “loss narrows”). Use as one input among many.\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Quick reference (key functions)\n",
    "\n",
    "* `load_name_map_from_nse()` → builds `{SYMBOL: Company}` dynamically\n",
    "* `get_company_name(ticker)` → returns company name from `NAME_MAP` for `\"LUPIN.NS\"`\n",
    "* `fetch_news_for_ticker(ticker)` → DataFrame of news rows for that ticker\n",
    "* `analyze_ticker_news(df_news, ticker)` → dict with sentiment & bias for 1 ticker\n",
    "* `build_sentiment_table(tickers)` → final table across tickers\n",
    "\n",
    "---\n",
    "\n",
    "Happy screening! If you want, I can add a **CSV export** (e.g., `sentiment_df.to_csv`) or a small **CLI** wrapper (`--tickers`, `--since`) for notebook-free runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30b03da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_sentiment_dma.py\n",
    "import re, time, urllib.parse, requests, feedparser\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from transformers import pipeline\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "UA = (\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "      \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "      \"Chrome/122.0.0.0 Safari/537.36\")\n",
    "HDRS = {\n",
    "    \"User-Agent\": UA,\n",
    "    \"Accept-Language\": \"en-IN,en;q=0.9\",\n",
    "    \"Accept\": \"application/rss+xml, application/xml;q=0.9, */*;q=0.8\",\n",
    "}\n",
    "\n",
    "SENTIMENT_NEUTRAL_BAND = 0.05\n",
    "MAX_HEADLINES = 10\n",
    "MAX_ARTICLES_PER_TICKER = 8\n",
    "MAX_TOKENS_PER_ARTICLE = 512\n",
    "BATCH_SIZE = 8\n",
    "MIN_ARTICLE_CHARS = 200\n",
    "\n",
    "# Publisher allowlist (substring match on URL)\n",
    "ALLOWLIST = [\n",
    "    \"moneycontrol.com\",\n",
    "    \"economictimes.indiatimes.com\",\n",
    "    \"livemint.com\",\n",
    "    \"mint\",  # include \"mint\" as some mirrors use it\n",
    "    \"business-standard.com\",\n",
    "    \"cnbctv18.com\",\n",
    "]\n",
    "\n",
    "# FinBERT model\n",
    "MODEL_NAME = \"ProsusAI/finbert\"\n",
    "sentiment_model = pipeline(\"sentiment-analysis\", model=MODEL_NAME, tokenizer=MODEL_NAME)\n",
    "\n",
    "\n",
    "# ========== DYNAMIC NAME_MAP FROM NSE INDEX CSVs ==========\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from typing import Iterable\n",
    "\n",
    "NSE_HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/124.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Referer\": \"https://www.nseindia.com/market-data/live-market-indices\"\n",
    "}\n",
    "\n",
    "# Common index CSVs — add/remove as you like\n",
    "DEFAULT_NSE_INDEX_URLS = [\n",
    "    \"https://nsearchives.nseindia.com/content/indices/ind_nifty50list.csv\",\n",
    "    \"https://nsearchives.nseindia.com/content/indices/ind_niftynext50list.csv\",\n",
    "    \"https://nsearchives.nseindia.com/content/indices/ind_niftybanklist.csv\",\n",
    "    \"https://nsearchives.nseindia.com/content/indices/ind_nifty500list.csv\",\n",
    "    \"https://nsearchives.nseindia.com/content/indices/ind_niftymidcap100list.csv\",\n",
    "    \"https://nsearchives.nseindia.com/content/indices/ind_niftysmallcap100list.csv\",\n",
    "]\n",
    "\n",
    "def load_name_map_from_nse(csv_urls: Iterable[str] = None, timeout: int = 10) -> dict:\n",
    "    \"\"\"\n",
    "    Build {SYMBOL: Company Name} from one or more NSE index CSVs.\n",
    "    Symbols are upper-cased WITHOUT the '.NS' suffix (e.g., 'LUPIN', 'CENTRALBK').\n",
    "    \"\"\"\n",
    "    csv_urls = list(csv_urls) if csv_urls else list(DEFAULT_NSE_INDEX_URLS)\n",
    "\n",
    "    sess = requests.Session()\n",
    "    sess.headers.update(NSE_HEADERS)\n",
    "    # Warm up cookies (important for NSE)\n",
    "    try:\n",
    "        sess.get(\"https://www.nseindia.com\", timeout=timeout)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    frames = []\n",
    "    for url in csv_urls:\n",
    "        try:\n",
    "            r = sess.get(url, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            df = pd.read_csv(StringIO(r.text))\n",
    "            # Normalize column names (NSE uses \"Symbol\" and \"Company Name\")\n",
    "            cols = {c.lower(): c for c in df.columns}\n",
    "            sym_col = cols.get(\"symbol\")\n",
    "            name_col = cols.get(\"company name\") or cols.get(\"company\") or cols.get(\"companyname\")\n",
    "            if not sym_col or not name_col:\n",
    "                continue\n",
    "            slim = df[[sym_col, name_col]].rename(columns={sym_col: \"Symbol\", name_col: \"Company\"})\n",
    "            frames.append(slim)\n",
    "        except Exception:\n",
    "            # Skip bad/temporarily unavailable indices\n",
    "            continue\n",
    "\n",
    "    if not frames:\n",
    "        return {}\n",
    "\n",
    "    merged = pd.concat(frames, ignore_index=True)\n",
    "    # Clean + dedupe\n",
    "    merged[\"Symbol\"] = merged[\"Symbol\"].astype(str).str.strip().str.upper()\n",
    "    merged[\"Company\"] = merged[\"Company\"].astype(str).str.strip()\n",
    "    merged = merged.dropna(subset=[\"Symbol\", \"Company\"]).drop_duplicates(subset=[\"Symbol\"], keep=\"first\")\n",
    "\n",
    "    # Build dict\n",
    "    name_map = dict(zip(merged[\"Symbol\"], merged[\"Company\"]))\n",
    "    return name_map\n",
    "\n",
    "# Load dynamically at startup\n",
    "NAME_MAP = load_name_map_from_nse()\n",
    "\n",
    "# Optional: manual overrides if you want to fix odd cases or add missing ones\n",
    "EXTRA_NAME_MAP = {\n",
    "    # \"TARIL\": \"Tata Realty & Infrastructure Ltd\",  # example if needed\n",
    "}\n",
    "NAME_MAP.update(EXTRA_NAME_MAP)\n",
    "\n",
    "def get_company_name(symbol_with_ns: str) -> str:\n",
    "    \"\"\"\n",
    "    Given 'LUPIN.NS' → returns 'Lupin' if known, else 'LUPIN'.\n",
    "    \"\"\"\n",
    "    base = (symbol_with_ns or \"\").split(\".\")[0].upper()\n",
    "    return NAME_MAP.get(base, base)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# UTILITIES\n",
    "# =========================\n",
    "def _clean_title(t: str) -> str:\n",
    "    \"\"\"Drop trailing publisher and compress whitespace.\"\"\"\n",
    "    t = re.sub(r\"\\s+-\\s+[^-]+$\", \"\", (t or \"\").strip())\n",
    "    return re.sub(r\"\\s+\", \" \", t)\n",
    "\n",
    "def canonicalize_url(url: str) -> str:\n",
    "    \"\"\"Remove common AMP patterns & excessive tracking params to improve extraction.\"\"\"\n",
    "    if not url:\n",
    "        return url\n",
    "    url = re.sub(r\"/amp/?$\", \"/\", url)\n",
    "    url = re.sub(r\"[?&](amp|outputType=amp)\\b[^&]*\", \"\", url)\n",
    "    parsed = urllib.parse.urlparse(url)\n",
    "    qs = urllib.parse.parse_qsl(parsed.query, keep_blank_values=True)\n",
    "    # Keep only a tiny set of params that are sometimes essential\n",
    "    allow = {\"id\", \"story_fbid\"}\n",
    "    qs = [(k, v) for (k, v) in qs if k in allow]\n",
    "    new_qs = urllib.parse.urlencode(qs)\n",
    "    return urllib.parse.urlunparse(parsed._replace(query=new_qs))\n",
    "\n",
    "def _normalize_label(lab: str) -> str:\n",
    "    lab = str(lab).lower()\n",
    "    if lab.startswith(\"pos\"): return \"positive\"\n",
    "    if lab.startswith(\"neg\"): return \"negative\"\n",
    "    return \"neutral\"\n",
    "\n",
    "def score_text_blocks(texts: list[str]) -> list[dict]:\n",
    "    \"\"\"Run FinBERT on text blocks.\"\"\"\n",
    "    if not texts: return []\n",
    "    outs = sentiment_model(texts, truncation=True, batch_size=BATCH_SIZE)\n",
    "    return [{\"label\": _normalize_label(o[\"label\"]), \"score\": float(o[\"score\"])} for o in outs]\n",
    "\n",
    "def chunk_text(text: str, max_tokens: int = MAX_TOKENS_PER_ARTICLE) -> list[str]:\n",
    "    \"\"\"Lightweight chunk by char budget (~4 chars per token).\"\"\"\n",
    "    char_budget = max_tokens * 4\n",
    "    t = text[:min(len(text), char_budget)]\n",
    "    if len(t) < char_budget // 2:\n",
    "        return [t]\n",
    "    mid = len(t) // 2\n",
    "    return [t[:mid], t[mid:]]\n",
    "\n",
    "def aggregate_sentiment(rows: list[dict], weights: list[float] | None = None) -> float:\n",
    "    \"\"\"Map labels to +1/0/-1 and average by model confidence (and optional weights).\"\"\"\n",
    "    if not rows: return 0.0\n",
    "    mapping = {\"positive\": 1.0, \"neutral\": 0.0, \"negative\": -1.0}\n",
    "    vals = [mapping[r[\"label\"]] * r[\"score\"] for r in rows]\n",
    "    if weights and len(weights) == len(vals):\n",
    "        # normalize by sum of weights to avoid bias\n",
    "        wsum = sum(weights) or 1.0\n",
    "        return float(sum(v * w for v, w in zip(vals, weights)) / wsum)\n",
    "    return float(sum(vals) / len(vals))\n",
    "\n",
    "def recency_weight(published: str) -> float:\n",
    "    \"\"\"≤24h:1.5, 24–48h:1.2, else:1.0; robust to missing/odd formats.\"\"\"\n",
    "    try:\n",
    "        # Common Google News RSS format: 'Mon, 16 Sep 2025 08:30:00 GMT'\n",
    "        dt = datetime.strptime(published, \"%a, %d %b %Y %H:%M:%S %Z\")\n",
    "        delta = datetime.utcnow() - dt.replace(tzinfo=None)\n",
    "        if delta <= timedelta(hours=24): return 1.5\n",
    "        if delta <= timedelta(hours=48): return 1.2\n",
    "    except Exception:\n",
    "        pass\n",
    "    return 1.0\n",
    "\n",
    "def bias_from_score(avg: float) -> str:\n",
    "    if avg > SENTIMENT_NEUTRAL_BAND: return \"LONG\"\n",
    "    if avg < -SENTIMENT_NEUTRAL_BAND: return \"SHORT\"\n",
    "    return \"NEUTRAL\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# FETCH GOOGLE NEWS (with allowlist & fallback)\n",
    "# =========================\n",
    "def fetch_google_news_df(query_text: str, max_items: int = MAX_HEADLINES) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build Google News RSS URL for last 7 days, download with headers, parse with feedparser.\n",
    "    Keep only allowlisted publishers if any exist; else return all (never empty due to filter).\n",
    "    \"\"\"\n",
    "    q = urllib.parse.quote_plus(query_text + \" when:7d\")\n",
    "    url = f\"https://news.google.com/rss/search?q={q}&hl=en-IN&gl=IN&ceid=IN:en\"\n",
    "    r = requests.get(url, headers=HDRS, timeout=10, allow_redirects=True)\n",
    "    if r.status_code != 200 or not r.content:\n",
    "        return pd.DataFrame(columns=[\"title\",\"summary\",\"link\",\"published\",\"fetched_at\",\"source_title\",\"allowlisted\"])\n",
    "\n",
    "    fp = feedparser.parse(r.content)\n",
    "    entries = getattr(fp, \"entries\", []) or []\n",
    "    rows = []\n",
    "    for e in entries[:max_items]:\n",
    "        link = getattr(e, \"link\", \"\") or \"\"\n",
    "        allow = any(dom in link.lower() for dom in ALLOWLIST)\n",
    "        rows.append({\n",
    "            \"title\": _clean_title(getattr(e, \"title\", \"\")),\n",
    "            \"summary\": getattr(e, \"summary\", None),\n",
    "            \"link\": link,\n",
    "            \"published\": getattr(e, \"published\", None),\n",
    "            \"fetched_at\": datetime.now(),\n",
    "            \"source_title\": fp.feed.get(\"title\", \"\"),\n",
    "            \"allowlisted\": allow,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # Prefer allowlisted only, but if none matched, return everything (fallback)\n",
    "    if df[\"allowlisted\"].any():\n",
    "        return df[df[\"allowlisted\"]].reset_index(drop=True)\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def fetch_news_for_ticker(ticker: str, max_items: int = MAX_HEADLINES) -> pd.DataFrame:\n",
    "    company = get_company_name(ticker)  # ← dynamic lookup\n",
    "    base = ticker.split(\".\")[0]         # e.g., 'LUPIN'\n",
    "    queries = [\n",
    "        f'\"{company}\" stock india',\n",
    "        f\"{company} shares\",\n",
    "        f\"{base} stock india\",\n",
    "    ]\n",
    "    for q in queries:\n",
    "        df = fetch_google_news_df(q, max_items=max_items)\n",
    "        if not df.empty:\n",
    "            df.insert(0, \"symbol\", ticker)\n",
    "            return df\n",
    "    return pd.DataFrame(columns=[\"symbol\",\"title\",\"summary\",\"link\",\"published\",\"fetched_at\",\"source_title\",\"allowlisted\"])\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ARTICLE TEXT EXTRACTION (multi-fallback)\n",
    "# =========================\n",
    "def extract_text_trafilatura(url: str, timeout: int = 10) -> str | None:\n",
    "    try:\n",
    "        import trafilatura\n",
    "    except Exception:\n",
    "        return None\n",
    "    try:\n",
    "        downloaded = trafilatura.fetch_url(url, no_ssl=True)\n",
    "        if not downloaded:\n",
    "            r = requests.get(url, headers=HDRS, timeout=timeout)\n",
    "            if r.status_code != 200 or not r.text:\n",
    "                return None\n",
    "            txt = trafilatura.extract(r.text, url=url)\n",
    "        else:\n",
    "            txt = trafilatura.extract(downloaded, url=url)\n",
    "        if not txt: return None\n",
    "        txt = \" \".join(txt.split())\n",
    "        return txt if len(txt) >= MIN_ARTICLE_CHARS else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_text_readability(url: str, timeout: int = 10) -> str | None:\n",
    "    try:\n",
    "        from readability import Document\n",
    "        from lxml import html\n",
    "    except Exception:\n",
    "        return None\n",
    "    try:\n",
    "        r = requests.get(url, headers=HDRS, timeout=timeout, allow_redirects=True)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "        doc = Document(r.text)\n",
    "        summ_html = doc.summary(html_partial=True)\n",
    "        tree = html.fromstring(summ_html)\n",
    "        txt = \" \".join(tree.text_content().split())\n",
    "        return txt if len(txt) >= MIN_ARTICLE_CHARS else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_text_newspaper(url: str) -> str | None:\n",
    "    try:\n",
    "        from newspaper import Article\n",
    "    except Exception:\n",
    "        return None\n",
    "    try:\n",
    "        art = Article(url, language=\"en\", fetch_images=False, browser_user_agent=UA)\n",
    "        art.download()\n",
    "        art.parse()\n",
    "        txt = \" \".join(art.text.split())\n",
    "        return txt if len(txt) >= MIN_ARTICLE_CHARS else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_article_text(url: str) -> str | None:\n",
    "    \"\"\"Try trafilatura → readability → newspaper3k; relax length requirement.\"\"\"\n",
    "    url = canonicalize_url(url)\n",
    "    for fn in (extract_text_trafilatura, extract_text_readability, extract_text_newspaper):\n",
    "        txt = fn(url)\n",
    "        if txt and len(txt) >= 120:   # relaxed threshold\n",
    "            return re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PIPELINE: HEADLINE + ARTICLE SENTIMENT with RECENCY WEIGHTS\n",
    "# =========================\n",
    "def analyze_ticker_news(df_news: pd.DataFrame, ticker: str) -> dict:\n",
    "    df = df_news[df_news[\"symbol\"] == ticker].head(MAX_ARTICLES_PER_TICKER).copy()\n",
    "    if df.empty:\n",
    "        return {\"symbol\": ticker, \"headline_avg\": 0.0, \"article_avg\": 0.0,\n",
    "                \"combined_score\": 0.0, \"bias\": \"NEUTRAL\",\n",
    "                \"n_headlines\": 0, \"n_articles\": 0}\n",
    "\n",
    "    # Headline sentiment (recency-weighted)\n",
    "    headline_rows = score_text_blocks(df[\"title\"].tolist())\n",
    "    weights = [recency_weight(p) for p in df[\"published\"]] if \"published\" in df else None\n",
    "    headline_avg = aggregate_sentiment(headline_rows, weights)\n",
    "\n",
    "    # Article sentiment\n",
    "    article_avgs, extracted_count = [], 0\n",
    "    for _, row in df.iterrows():\n",
    "        link = row.get(\"link\", \"\")\n",
    "        body = extract_article_text(link)\n",
    "        if not body:\n",
    "            # Fallback: RSS summary\n",
    "            summary = row.get(\"summary\")\n",
    "            if summary and len(summary) >= 80:\n",
    "                body = re.sub(r\"<[^>]+>\", \" \", summary)\n",
    "        if not body:\n",
    "            continue\n",
    "\n",
    "        extracted_count += 1\n",
    "        chunks = chunk_text(body, MAX_TOKENS_PER_ARTICLE)\n",
    "        chunk_scores = score_text_blocks(chunks)\n",
    "        article_avgs.append(aggregate_sentiment(chunk_scores))\n",
    "        time.sleep(0.05)\n",
    "\n",
    "    article_avg = sum(article_avgs)/len(article_avgs) if article_avgs else 0.0\n",
    "    combined = 0.7*article_avg + 0.3*headline_avg if article_avgs else headline_avg\n",
    "\n",
    "    return {\n",
    "        \"symbol\": ticker,\n",
    "        \"headline_avg\": round(headline_avg, 4),\n",
    "        \"article_avg\": round(article_avg, 4),\n",
    "        \"combined_score\": round(combined, 4),\n",
    "        \"bias\": bias_from_score(combined),\n",
    "        \"n_headlines\": int(len(df)),\n",
    "        \"n_articles\": int(extracted_count),\n",
    "    }\n",
    "\n",
    "def build_sentiment_table(tickers: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"Fetch news for all tickers, then compute per-ticker bias table.\"\"\"\n",
    "    frames = [fetch_news_for_ticker(t) for t in tickers]\n",
    "    all_news = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "    results = [analyze_ticker_news(all_news, t) for t in tickers]\n",
    "    out = pd.DataFrame(results).sort_values([\"bias\", \"combined_score\"], ascending=[True, False]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# MAIN (example)\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    tickers = [\n",
    "        \"SBILIFE.NS\"\n",
    "]\n",
    "    sentiment_df = build_sentiment_table(tickers)\n",
    "    print(sentiment_df)\n",
    "    # If you want to inspect raw news rows:\n",
    "    raw_df = pd.concat([fetch_news_for_ticker(t) for t in tickers], ignore_index=True)\n",
    "    # print(raw_df[[\"symbol\",\"title\",\"link\",\"published\",\"allowlisted\"]].head(20))\n",
    "\n",
    "\n",
    "    # make wide tables readable in the terminal\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    pd.set_option(\"display.width\", 140)\n",
    "    pd.set_option(\"display.max_colwidth\", 90)   # control title/link truncation\n",
    "\n",
    "    print(\n",
    "        raw_df[[\"symbol\",\"title\",\"link\",\"published\",\"allowlisted\"]]\n",
    "            .head(20)\n",
    "            .to_string(index=False)\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".talib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
