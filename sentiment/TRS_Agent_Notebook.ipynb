{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "866e8670",
   "metadata": {},
   "source": [
    "\n",
    "# Trade Readiness Score (TRS) â€” Fundamentals Ã— Sentiment Ã— News\n",
    "\n",
    "**Created:** 2025-09-09 18:17 UTC  \n",
    "**Purpose:** Daily pipeline to generate a **Topâ€‘N list** of trade candidates by fusing:\n",
    "- **Fundamentals Analyst:** quality/valuation/red flags â†’ *Q score (0â€“100)*\n",
    "- **Sentiment Analyst:** finance + social text sentiment â†’ *S score (âˆ’1 to +1 â†’ scaled)*\n",
    "- **News Analyst:** event/catalyst & macro risk tagging â†’ *N score (0â€“100)*\n",
    "- **Fusion:** `TRS = 0.45Â·Q + 0.35Â·S' + 0.20Â·N'`\n",
    "\n",
    "**Outputs**\n",
    "- `trs_signals.csv`: Ticker, TRS, sub-scores, price/ATR/DMA, entry/stop/targets, position size\n",
    "- On-screen **Topâ€‘N** table\n",
    "\n",
    "> ðŸš¦ Designed to be robust: all external sources are **optional** with graceful fallbacks.  \n",
    "> If you don't set any API keys, it still runs with Google News RSS + price data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5dc005",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5015eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests_cache in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (1.2.1)\n",
      "Requirement already satisfied: attrs>=21.2 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from requests_cache) (25.3.0)\n",
      "Requirement already satisfied: cattrs>=22.2 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from requests_cache) (25.2.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from requests_cache) (4.3.8)\n",
      "Requirement already satisfied: requests>=2.22 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from requests_cache) (2.32.4)\n",
      "Requirement already satisfied: url-normalize>=1.4 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from requests_cache) (2.2.1)\n",
      "Requirement already satisfied: urllib3>=1.25.5 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from requests_cache) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from cattrs>=22.2->requests_cache) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from requests>=2.22->requests_cache) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from requests>=2.22->requests_cache) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hemank/Documents/github/.talib/lib/python3.12/site-packages (from requests>=2.22->requests_cache) (2025.7.14)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If running first time, uncomment the next cell to install requirements.\n",
    "# (Keep them commented if your environment already has these packages.)\n",
    "# %pip install -q --upgrade pandas numpy yfinance transformers torch scikit-learn #     feedparser beautifulsoup4 lxml requests_cache duckdb python-dateutil tqdm nltk\n",
    "#\n",
    "# Optional (for charts):\n",
    "# %pip install -q matplotlib\n",
    "#\n",
    "# Note: Transformers models (FinBERT, Tweet-RoBERTa) will download on first use.\n",
    "!pip install requests_cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94bcdf1",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eff85c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading vader_lexicon: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, sys, math, time, json, re, gc, logging, textwrap, itertools, statistics, hashlib\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import requests_cache\n",
    "import feedparser\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "from dateutil import parser as dateparser\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Optional plotting (disable if headless):\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    HAVE_MPL = True\n",
    "except Exception:\n",
    "    HAVE_MPL = False\n",
    "\n",
    "# NLP (lazy import later to speed cold start)\n",
    "TRANSFORMERS_AVAILABLE = True\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "except Exception as e:\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "\n",
    "# Optional VADER fallback\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    nltk.download('vader_lexicon', quiet=True)\n",
    "    HAVE_VADER = True\n",
    "except Exception:\n",
    "    HAVE_VADER = False\n",
    "\n",
    "# Cache HTTP\n",
    "requests_cache.install_cache('trs_cache', expire_after=300)  # 5 minutes\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s')\n",
    "logger = logging.getLogger(\"TRS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf613e16",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e9d9137",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========= USER CONFIG =========\n",
    "\n",
    "# Universe (NSE examples â€” you can mix in US tickers as well)\n",
    "TICKERS = [\n",
    "    \"RELIANCE.NS\", \"TCS.NS\", \"INFY.NS\", \"HDFCBANK.NS\", \"BAJFINANCE.NS\",\n",
    "    # \"AAPL\", \"MSFT\", \"NVDA\"  # US examples\n",
    "]\n",
    "\n",
    "# Output\n",
    "TOP_N = 10\n",
    "OUTPUT_CSV = \"trs_signals.csv\"\n",
    "\n",
    "# Risk & Sizing\n",
    "CAPITAL = 1_000_000  # total portfolio in INR (or your base currency)\n",
    "RISK_PER_TRADE = 0.0075  # 0.75% of equity risk per trade\n",
    "ATR_MULT_STOP = 1.5\n",
    "ATR_MULT_TARGET = 2.5\n",
    "\n",
    "# Price data\n",
    "HISTORY_PERIOD = \"9mo\"\n",
    "PRICE_INTERVAL = \"1d\"\n",
    "\n",
    "# TRS Weights\n",
    "W_Q = 0.45\n",
    "W_S = 0.35\n",
    "W_N = 0.20\n",
    "\n",
    "# Sentiment windows\n",
    "SENTIMENT_LOOKBACK_HOURS = 72  # aggregate sentiment over last 72 hours\n",
    "ROLLING_SENT_PERCENTILE_DAYS = 90  # map S to rolling percentile\n",
    "\n",
    "# Macro blackout\n",
    "MACRO_BLACKOUT_HOURS = 6  # suppress signals around major macro events\n",
    "\n",
    "# API Keys (optional)\n",
    "ALPHA_VANTAGE_KEY = os.getenv(\"ALPHA_VANTAGE_KEY\", \"\")   # Fundamentals + News\n",
    "NEWSAPI_KEY       = os.getenv(\"NEWSAPI_KEY\", \"\")         # News headlines\n",
    "# Reddit social is fetched via public JSON (no key), but can be flaky.\n",
    "\n",
    "# NLP model choices (change if you like)\n",
    "MODEL_FINBERT = \"ProsusAI/finbert\"\n",
    "MODEL_TWEET   = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "# Region for macro events (manual list below). Set to [] to disable.\n",
    "REGION = \"IN\"\n",
    "\n",
    "# Macro events (manual list). You can add your local calendars here.\n",
    "MANUAL_MACRO_EVENTS = [\n",
    "    # Example format: (\"IN\", \"RBI MPC Policy\", \"2025-10-04 10:00\", \"Asia/Kolkata\"),\n",
    "    # (\"IN\", \"MoSPI CPI Release\", \"2025-09-12 17:30\", \"Asia/Kolkata\"),\n",
    "]\n",
    "\n",
    "# ==============================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25f78f5",
   "metadata": {},
   "source": [
    "## 3. Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d3b9aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ts_now_utc():\n",
    "    return datetime.now(timezone.utc)\n",
    "\n",
    "def to_utc(dt):\n",
    "    if dt.tzinfo is None:\n",
    "        return dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.astimezone(timezone.utc)\n",
    "\n",
    "def parse_when(s):\n",
    "    try:\n",
    "        return to_utc(dateparser.parse(s))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def safe_pct(a, b):\n",
    "    try:\n",
    "        if b == 0 or pd.isna(b): return np.nan\n",
    "        return 100.0 * (a - b) / b\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def winsorize(series, lower=0.01, upper=0.99):\n",
    "    if len(series) == 0:\n",
    "        return series\n",
    "    lo = series.quantile(lower)\n",
    "    hi = series.quantile(upper)\n",
    "    return series.clip(lo, hi)\n",
    "\n",
    "def zscore(series):\n",
    "    s = series.astype(float)\n",
    "    return (s - s.mean()) / (s.std(ddof=0) + 1e-9)\n",
    "\n",
    "def percentile_rank(x, vec):\n",
    "    # returns 0..100 percentile rank of x within vec\n",
    "    vec = np.array(vec, dtype=float)\n",
    "    if len(vec) == 0 or np.all(np.isnan(vec)):\n",
    "        return 50.0\n",
    "    return float(np.sum(vec <= x)) / len(vec) * 100.0\n",
    "\n",
    "def rolling_percentile(series, window=90):\n",
    "    out = []\n",
    "    for i in range(len(series)):\n",
    "        ref = series[max(0, i - window):i+1]\n",
    "        out.append(percentile_rank(series.iloc[i], ref))\n",
    "    return pd.Series(out, index=series.index)\n",
    "\n",
    "def ema(series, span=20):\n",
    "    return series.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "def atr(df, period=14):\n",
    "    high = df[\"High\"]\n",
    "    low = df[\"Low\"]\n",
    "    close = df[\"Close\"]\n",
    "    prev_close = close.shift(1)\n",
    "    tr = pd.concat([high - low, (high - prev_close).abs(), (low - prev_close).abs()], axis=1).max(axis=1)\n",
    "    return tr.rolling(period).mean()\n",
    "\n",
    "def position_size(capital, risk_per_trade, stop_distance):\n",
    "    if stop_distance <= 0 or pd.isna(stop_distance):\n",
    "        return 0.0\n",
    "    risk_amt = capital * risk_per_trade\n",
    "    return math.floor(risk_amt / stop_distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4771d0ad",
   "metadata": {},
   "source": [
    "## 4. Fundamentals Analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccd50837",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AV_BASE = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "def av_get(function, **params):\n",
    "    if not ALPHA_VANTAGE_KEY:\n",
    "        return None, \"No Alpha Vantage key\"\n",
    "    p = dict(apikey=ALPHA_VANTAGE_KEY, function=function)\n",
    "    p.update(params)\n",
    "    try:\n",
    "        r = requests.get(AV_BASE, params=p, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        if \"Note\" in data or \"Error Message\" in data:\n",
    "            return None, data.get(\"Note\") or data.get(\"Error Message\")\n",
    "        return data, None\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "def fundamentals_overview(symbol):\n",
    "    data, err = av_get(\"OVERVIEW\", symbol=symbol)\n",
    "    if err or not data:\n",
    "        return None\n",
    "    # Keep key ratios as floats where possible\n",
    "    keep = [\n",
    "        \"EBITDA\", \"PERatio\", \"PEGRatio\", \"BookValue\", \"DividendYield\", \"ProfitMargin\",\n",
    "        \"OperatingMarginTTM\", \"ReturnOnEquityTTM\", \"ReturnOnAssetsTTM\", \"QuarterlyEarningsGrowthYOY\",\n",
    "        \"QuarterlyRevenueGrowthYOY\", \"AnalystTargetPrice\", \"TrailingPE\", \"ForwardPE\",\n",
    "        \"PriceToBookRatio\", \"EVToEBITDA\", \"EVToRevenue\", \"Beta\"\n",
    "    ]\n",
    "    out = {}\n",
    "    for k in keep:\n",
    "        v = data.get(k, None)\n",
    "        try:\n",
    "            out[k] = float(v) if v not in (None, \"None\", \"null\", \"\") else np.nan\n",
    "        except Exception:\n",
    "            out[k] = np.nan\n",
    "    return out\n",
    "\n",
    "def fundamentals_score(symbols):\n",
    "    # Returns Q score per symbol (0..100), robust to missing data\n",
    "    rows = []\n",
    "    for s in symbols:\n",
    "        row = {\"symbol\": s}\n",
    "        f = fundamentals_overview(s)\n",
    "        if f is None:\n",
    "            row.update({\"Q_raw\": np.nan, \"Q\": 50.0, \"Q_detail\": {\"note\": \"No AV data\"}})\n",
    "        else:\n",
    "            # Simple composite: quality + profitability + valuation (lower EV/EBITDA better)\n",
    "            prof = winsorize(pd.Series([\n",
    "                f.get(\"ReturnOnEquityTTM\"),\n",
    "                f.get(\"ReturnOnAssetsTTM\"),\n",
    "                f.get(\"OperatingMarginTTM\"),\n",
    "                f.get(\"ProfitMargin\"),\n",
    "            ], dtype=float), 0.05, 0.95).mean()\n",
    "\n",
    "            growth = winsorize(pd.Series([\n",
    "                f.get(\"QuarterlyEarningsGrowthYOY\"),\n",
    "                f.get(\"QuarterlyRevenueGrowthYOY\"),\n",
    "            ], dtype=float), 0.05, 0.95).mean()\n",
    "\n",
    "            # Valuation inverse (cheaper â†’ better). Use EV/EBITDA + P/B if available.\n",
    "            val = winsorize(pd.Series([\n",
    "                -f.get(\"EVToEBITDA\", np.nan),  # negative because lower is better\n",
    "                -f.get(\"PriceToBookRatio\", np.nan),\n",
    "                -f.get(\"TrailingPE\", np.nan)\n",
    "            ], dtype=float), 0.05, 0.95).mean()\n",
    "\n",
    "            components = [x for x in [prof, growth, val] if not pd.isna(x)]\n",
    "            if len(components) == 0:\n",
    "                raw = np.nan\n",
    "            else:\n",
    "                raw = np.nanmean(components)\n",
    "\n",
    "            # Rank across universe\n",
    "            row[\"Q_raw\"] = raw\n",
    "            row[\"Q_detail\"] = {\"prof\": float(prof) if not pd.isna(prof) else None,\n",
    "                               \"growth\": float(growth) if not pd.isna(growth) else None,\n",
    "                               \"val\": float(val) if not pd.isna(val) else None,\n",
    "                               \"beta\": f.get(\"Beta\", None)}\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df[\"Q_raw\"].notna().sum() >= 2:\n",
    "        # Convert to 0..100 via percentile rank\n",
    "        vals = df[\"Q_raw\"].fillna(df[\"Q_raw\"].median())\n",
    "        ranks = vals.rank(pct=True)\n",
    "        df[\"Q\"] = (ranks * 100).clip(0, 100)\n",
    "    else:\n",
    "        df[\"Q\"] = 50.0\n",
    "\n",
    "    # Keep columns\n",
    "    df = df[[\"symbol\", \"Q\", \"Q_raw\", \"Q_detail\"]]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fc4a1a",
   "metadata": {},
   "source": [
    "## 5. News Analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a456ad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def google_news_rss(query, hours=72, lang=\"en\"):\n",
    "    # Build Google News RSS query (no API key required)\n",
    "    url = f\"https://news.google.com/rss/search?q={requests.utils.quote(query)}&hl={lang}\"\n",
    "    try:\n",
    "        d = feedparser.parse(url)\n",
    "        cutoff = ts_now_utc() - timedelta(hours=hours)\n",
    "        items = []\n",
    "        for e in d.entries:\n",
    "            # Parse published date if present\n",
    "            pub = e.get(\"published\", \"\") or e.get(\"updated\", \"\")\n",
    "            when = parse_when(pub) or ts_now_utc()\n",
    "            if when < cutoff:\n",
    "                continue\n",
    "            link = e.get(\"link\", \"\")\n",
    "            title = e.get(\"title\", \"\")\n",
    "            summary = e.get(\"summary\", \"\")\n",
    "            items.append({\"title\": title, \"summary\": summary, \"link\": link, \"published\": when})\n",
    "        return items\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Google RSS error for {query}: {e}\")\n",
    "        return []\n",
    "\n",
    "def newsapi_search(query, hours=72):\n",
    "    if not NEWSAPI_KEY:\n",
    "        return []\n",
    "    cutoff = ts_now_utc() - timedelta(hours=hours)\n",
    "    url = \"https://newsapi.org/v2/everything\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"sortBy\": \"publishedAt\",\n",
    "        \"language\": \"en\",\n",
    "        \"pageSize\": 100,\n",
    "        \"apiKey\": NEWSAPI_KEY,\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        out = []\n",
    "        for a in data.get(\"articles\", []):\n",
    "            when = parse_when(a.get(\"publishedAt\")) or ts_now_utc()\n",
    "            if when < cutoff: \n",
    "                continue\n",
    "            out.append({\n",
    "                \"title\": a.get(\"title\", \"\"),\n",
    "                \"summary\": a.get(\"description\", \"\"),\n",
    "                \"link\": a.get(\"url\", \"\"),\n",
    "                \"published\": when\n",
    "            })\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"NewsAPI error for {query}: {e}\")\n",
    "        return []\n",
    "\n",
    "def tag_news_event(text):\n",
    "    t = text.lower()\n",
    "    tags = []\n",
    "    if any(k in t for k in [\"guidance cut\", \"profit warning\", \"probe\", \"fraud\", \"litigation\", \"default\", \"insolvency\", \"bankruptcy\", \"pledge shares\", \"pledged shares\"]):\n",
    "        tags.append((\"NEG_REGULATORY\", -30))\n",
    "    if any(k in t for k in [\"downgrade\", \"cut to\", \"revised down\", \"misses estimates\"]):\n",
    "        tags.append((\"NEG_ANALYST\", -15))\n",
    "    if any(k in t for k in [\"upgrade\", \"raises price target\", \"beats estimates\", \"record revenue\"]):\n",
    "        tags.append((\"POS_ANALYST\", +12))\n",
    "    if any(k in t for k in [\"merger\", \"acquisition\", \"stake buy\", \"promoter buying\"]):\n",
    "        tags.append((\"MNA\", +10))\n",
    "    if any(k in t for k in [\"mgmt change\", \"resigns\", \"ceo resigns\", \"cfo resigns\"]):\n",
    "        tags.append((\"MGMT_CHANGE\", -5))\n",
    "    if any(k in t for k in [\"pledge\", \"pledged\"]):\n",
    "        tags.append((\"PLEDGE\", -10))\n",
    "    return tags\n",
    "\n",
    "def macro_blackout_multiplier(events, hours=MACRO_BLACKOUT_HOURS, region=REGION):\n",
    "    # Reduce N score if close to macro events\n",
    "    now = ts_now_utc()\n",
    "    for (reg, name, when_str, tz) in events:\n",
    "        if region and reg != region: \n",
    "            continue\n",
    "        when = parse_when(when_str)\n",
    "        if not when:\n",
    "            continue\n",
    "        if abs((when - now).total_seconds()) <= hours * 3600:\n",
    "            return 0.7  # 30% penalty\n",
    "    return 1.0\n",
    "\n",
    "def news_score_for_ticker(ticker, hours=72):\n",
    "    # Query building\n",
    "    q1 = f\"{ticker} stock\"\n",
    "    q2 = ticker.replace(\".NS\", \"\")  # try bare name for Indian tickers\n",
    "\n",
    "    items = []\n",
    "    items += google_news_rss(q1, hours=hours)\n",
    "    items += google_news_rss(q2, hours=hours)\n",
    "    items += newsapi_search(q1, hours=hours)\n",
    "    items += newsapi_search(q2, hours=hours)\n",
    "\n",
    "    # Deduplicate by title hash\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for it in items:\n",
    "        h = hashlib.md5((it[\"title\"] or \"\").encode(\"utf-8\")).hexdigest()\n",
    "        if h not in seen:\n",
    "            seen.add(h)\n",
    "            uniq.append(it)\n",
    "\n",
    "    # Sentiment via FinBERT (headline-level)\n",
    "    s_score = 0.0\n",
    "    n = 0\n",
    "    tags = []\n",
    "    texts = []\n",
    "    for it in uniq:\n",
    "        title = it.get(\"title\") or \"\"\n",
    "        summary = it.get(\"summary\") or \"\"\n",
    "        texts.append(title + \". \" + summary)\n",
    "        tags.extend(tag_news_event(title + \" \" + summary))\n",
    "    # Sentiment model run\n",
    "    if len(texts) > 0:\n",
    "        s = classify_finbert(texts)  # returns list in -1..+1\n",
    "        if len(s) > 0:\n",
    "            s_score = float(np.mean(s))\n",
    "            n = len(s)\n",
    "    # Base N score 50 + 40*sentiment, then apply tag adjustments and macro penalty\n",
    "    base = 50.0 + 40.0 * s_score\n",
    "    for (tag, delta) in tags:\n",
    "        base += delta\n",
    "    base = float(np.clip(base, 0, 100))\n",
    "\n",
    "    # Macro blackout penalty\n",
    "    mult = macro_blackout_multiplier(MANUAL_MACRO_EVENTS, hours=MACRO_BLACKOUT_HOURS, region=REGION)\n",
    "    base *= mult\n",
    "    base = float(np.clip(base, 0, 100))\n",
    "\n",
    "    detail = {\n",
    "        \"headline_count\": int(n),\n",
    "        \"avg_headline_sent\": float(s_score),\n",
    "        \"tags\": tags,\n",
    "        \"macro_mult\": mult\n",
    "    }\n",
    "    return base, detail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad2e4da",
   "metadata": {},
   "source": [
    "## 6. Sentiment Analyst (Finance + Social)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93568b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_FINBERT = {\"tok\": None, \"model\": None}\n",
    "_TWEET  = {\"tok\": None, \"model\": None}\n",
    "_VADER  = {\"sid\": None}\n",
    "\n",
    "def _load_finbert():\n",
    "    if _FINBERT[\"tok\"] is None and TRANSFORMERS_AVAILABLE:\n",
    "        _FINBERT[\"tok\"] = AutoTokenizer.from_pretrained(MODEL_FINBERT)\n",
    "        _FINBERT[\"model\"] = AutoModelForSequenceClassification.from_pretrained(MODEL_FINBERT)\n",
    "    return _FINBERT[\"tok\"], _FINBERT[\"model\"]\n",
    "\n",
    "def _load_tweet():\n",
    "    if _TWEET[\"tok\"] is None and TRANSFORMERS_AVAILABLE:\n",
    "        _TWEET[\"tok\"] = AutoTokenizer.from_pretrained(MODEL_TWEET)\n",
    "        _TWEET[\"model\"] = AutoModelForSequenceClassification.from_pretrained(MODEL_TWEET)\n",
    "    return _TWEET[\"tok\"], _TWEET[\"model\"]\n",
    "\n",
    "def _load_vader():\n",
    "    if _VADER[\"sid\"] is None and HAVE_VADER:\n",
    "        _VADER[\"sid\"] = SentimentIntensityAnalyzer()\n",
    "    return _VADER[\"sid\"]\n",
    "\n",
    "def softmax(x):\n",
    "    e = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e / np.sum(e, axis=-1, keepdims=True)\n",
    "\n",
    "def classify_finbert(texts):\n",
    "    # Returns list of sentiment scores in [-1, +1] using FinBERT (pos-neg)\n",
    "    try:\n",
    "        tok, model = _load_finbert()\n",
    "        if tok is None or model is None:\n",
    "            raise RuntimeError(\"Transformers not available; using VADER fallback\")\n",
    "        enc = tok(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        with torch.no_grad():\n",
    "            out = model(**enc).logits.numpy()\n",
    "        probs = softmax(out)\n",
    "        # FinBERT labels: 0=negative, 1=neutral, 2=positive (ProsusAI/finbert)\n",
    "        score = probs[:,2] - probs[:,0]\n",
    "        return score.tolist()\n",
    "    except Exception as e:\n",
    "        # Fallback to VADER compound\n",
    "        sid = _load_vader()\n",
    "        if sid is None:\n",
    "            return [0.0] * len(texts)\n",
    "        out = []\n",
    "        for t in texts:\n",
    "            out.append(sid.polarity_scores(t).get(\"compound\", 0.0))\n",
    "        return out\n",
    "\n",
    "def classify_tweet_roberta(texts):\n",
    "    # Returns [-1,+1] using cardiffnlp/twitter-roberta-base-sentiment-latest\n",
    "    try:\n",
    "        tok, model = _load_tweet()\n",
    "        if tok is None or model is None:\n",
    "            raise RuntimeError(\"Transformers not available; using VADER fallback\")\n",
    "        enc = tok(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        with torch.no_grad():\n",
    "            out = model(**enc).logits.numpy()\n",
    "        probs = softmax(out)\n",
    "        # labels: 0=negative, 1=neutral, 2=positive\n",
    "        score = probs[:,2] - probs[:,0]\n",
    "        return score.tolist()\n",
    "    except Exception as e:\n",
    "        sid = _load_vader()\n",
    "        if sid is None:\n",
    "            return [0.0] * len(texts)\n",
    "        out = []\n",
    "        for t in texts:\n",
    "            out.append(sid.polarity_scores(t).get(\"compound\", 0.0))\n",
    "        return out\n",
    "\n",
    "def reddit_search_posts(sub, query, hours=72, limit=50):\n",
    "    # Public JSON search; may rate-limit. Use headers.\n",
    "    url = f\"https://www.reddit.com/r/{sub}/search.json\"\n",
    "    params = {\"q\": query, \"restrict_sr\": \"1\", \"sort\": \"new\", \"t\": \"week\", \"limit\": str(limit)}\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 TRS-Agent/1.0\"}\n",
    "    try:\n",
    "        r = requests.get(url, params=params, headers=headers, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        js = r.json()\n",
    "        out = []\n",
    "        cutoff = ts_now_utc() - timedelta(hours=hours)\n",
    "        for c in js.get(\"data\", {}).get(\"children\", []):\n",
    "            d = c.get(\"data\", {})\n",
    "            created = datetime.fromtimestamp(d.get(\"created_utc\", time.time()), tz=timezone.utc)\n",
    "            if created < cutoff: \n",
    "                continue\n",
    "            title = d.get(\"title\", \"\")\n",
    "            selftext = d.get(\"selftext\", \"\")\n",
    "            score = d.get(\"score\", 0)\n",
    "            num_comments = d.get(\"num_comments\", 0)\n",
    "            out.append({\"title\": title, \"body\": selftext, \"created\": created, \"score\": score, \"num_comments\": num_comments})\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Reddit error for r/{sub} {query}: {e}\")\n",
    "        return []\n",
    "\n",
    "REDDIT_SUBS = [\"IndianStreetBets\", \"IndianStockMarket\", \"stocks\"]\n",
    "\n",
    "def social_sentiment_for_ticker(ticker, hours=72):\n",
    "    q = ticker.replace(\".NS\",\"\")\n",
    "    texts = []\n",
    "    for sub in REDDIT_SUBS:\n",
    "        posts = reddit_search_posts(sub, q, hours=hours, limit=40)\n",
    "        for p in posts:\n",
    "            t = (p[\"title\"] + \" \" + (p[\"body\"] or \"\")).strip()\n",
    "            if len(t) > 0:\n",
    "                texts.append(t)\n",
    "    if len(texts) == 0:\n",
    "        return 0.0, {\"posts\": 0, \"avg\": 0.0}\n",
    "    s = classify_tweet_roberta(texts)\n",
    "    if len(s) == 0:\n",
    "        return 0.0, {\"posts\": 0, \"avg\": 0.0}\n",
    "    # Weighted by engagement proxy (we don't have engagement here -> simple mean)\n",
    "    avg = float(np.mean(s))\n",
    "    return avg, {\"posts\": int(len(s)), \"avg\": avg}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768e52cd",
   "metadata": {},
   "source": [
    "## 7. Price/TA & TRS Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "572d9d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_price(ticker, period=HISTORY_PERIOD, interval=PRICE_INTERVAL):\n",
    "    try:\n",
    "        df = yf.download(ticker, period=period, interval=interval, auto_adjust=False, progress=False, multi_level_index=False)\n",
    "        if df is None or df.empty:\n",
    "            return None\n",
    "        df = df.rename_axis(\"Date\").reset_index()\n",
    "        if \"Adj Close\" not in df.columns:\n",
    "            df[\"Adj Close\"] = df[\"Close\"]\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Price fetch error {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "def enrich_ta(df):\n",
    "    df = df.copy()\n",
    "    df[\"SMA50\"] = df[\"Close\"].rolling(50).mean()\n",
    "    df[\"SMA200\"] = df[\"Close\"].rolling(200).mean()\n",
    "    df[\"ATR14\"] = atr(df.set_index(\"Date\"), 14).values\n",
    "    return df\n",
    "\n",
    "def map_sentiment_to_percentile_series(scores_series, window_days=ROLLING_SENT_PERCENTILE_DAYS):\n",
    "    # Map raw sentiment (-1..+1) to 0..100 using rolling percentile on its own history\n",
    "    # If not enough history, linearly map [-1..+1] -> [0..100].\n",
    "    if len(scores_series) < 5:\n",
    "        return 50.0 + 50.0 * scores_series\n",
    "    rp = rolling_percentile(scores_series, window=window_days)\n",
    "    return rp\n",
    "\n",
    "def compute_trs_row(sym, q_score, s_raw, n_score, price_df):\n",
    "    # s_raw is -1..+1, convert to percentile 0..100 using history if available\n",
    "    s_prime = 50.0 + 50.0 * s_raw  # fallback\n",
    "    # If we had a time series of S, we'd do rolling percentile; here we do static map.\n",
    "    trs = W_Q*q_score + W_S*s_prime + W_N*n_score\n",
    "    trs = float(np.clip(trs, 0, 100))\n",
    "\n",
    "    latest = price_df.iloc[-1]\n",
    "    close = float(latest[\"Close\"])\n",
    "    sma50 = float(latest[\"SMA50\"]) if not pd.isna(latest[\"SMA50\"]) else np.nan\n",
    "    sma200 = float(latest[\"SMA200\"]) if not pd.isna(latest[\"SMA200\"]) else np.nan\n",
    "    atr14 = float(latest[\"ATR14\"]) if not pd.isna(latest[\"ATR14\"]) else np.nan\n",
    "\n",
    "    bias_up = (not pd.isna(sma50) and close > sma50) and (not pd.isna(sma200) and close > sma200)\n",
    "\n",
    "    # Entry suggestion & risk\n",
    "    direction = \"LONG\" if trs >= 70 and bias_up else (\"AVOID/SHORT\" if trs <= 35 else \"WATCH\")\n",
    "    stop = close - ATR_MULT_STOP * atr14 if direction == \"LONG\" else (close + ATR_MULT_STOP*atr14 if direction==\"AVOID/SHORT\" else np.nan)\n",
    "    tgt  = close + ATR_MULT_TARGET * atr14 if direction == \"LONG\" else (close - ATR_MULT_TARGET*atr14 if direction==\"AVOID/SHORT\" else np.nan)\n",
    "    shares = position_size(CAPITAL, RISK_PER_TRADE, abs(close - stop)) if not pd.isna(stop) else 0\n",
    "\n",
    "    return {\n",
    "        \"symbol\": sym,\n",
    "        \"TRS\": trs,\n",
    "        \"Q\": float(q_score),\n",
    "        \"S_prime\": float(s_prime),\n",
    "        \"S_raw\": float(s_raw),\n",
    "        \"N\": float(n_score),\n",
    "        \"Close\": close,\n",
    "        \"SMA50\": sma50,\n",
    "        \"SMA200\": sma200,\n",
    "        \"ATR14\": atr14,\n",
    "        \"BiasUp\": bool(bias_up),\n",
    "        \"Direction\": direction,\n",
    "        \"Entry\": close,\n",
    "        \"Stop\": float(stop) if not pd.isna(stop) else np.nan,\n",
    "        \"Target\": float(tgt) if not pd.isna(tgt) else np.nan,\n",
    "        \"PositionSize\": int(shares)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef53fff9",
   "metadata": {},
   "source": [
    "## 8. Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c3fbf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 00:02:09,213 | INFO | Processing RELIANCE.NS\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-09-10 00:02:15,090 | INFO | Processing TCS.NS\n",
      "2025-09-10 00:02:17,299 | INFO | Processing INFY.NS\n",
      "2025-09-10 00:02:19,415 | INFO | Processing HDFCBANK.NS\n",
      "2025-09-10 00:02:21,398 | INFO | Processing BAJFINANCE.NS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>TRS</th>\n",
       "      <th>Q</th>\n",
       "      <th>S_prime</th>\n",
       "      <th>S_raw</th>\n",
       "      <th>N</th>\n",
       "      <th>Close</th>\n",
       "      <th>SMA50</th>\n",
       "      <th>SMA200</th>\n",
       "      <th>ATR14</th>\n",
       "      <th>BiasUp</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Entry</th>\n",
       "      <th>Stop</th>\n",
       "      <th>Target</th>\n",
       "      <th>PositionSize</th>\n",
       "      <th>NewsDetail</th>\n",
       "      <th>SocialDetail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCS.NS</td>\n",
       "      <td>53.928422</td>\n",
       "      <td>50.0</td>\n",
       "      <td>61.224063</td>\n",
       "      <td>0.224481</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3049.399902</td>\n",
       "      <td>3149.572002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.435686</td>\n",
       "      <td>False</td>\n",
       "      <td>WATCH</td>\n",
       "      <td>3049.399902</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>{'headline_count': 0, 'avg_headline_sent': 0.0...</td>\n",
       "      <td>{'posts': 4, 'avg': 0.2244812697172165}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFY.NS</td>\n",
       "      <td>51.809056</td>\n",
       "      <td>50.0</td>\n",
       "      <td>55.168732</td>\n",
       "      <td>0.103375</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1504.300049</td>\n",
       "      <td>1520.967993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.128575</td>\n",
       "      <td>False</td>\n",
       "      <td>WATCH</td>\n",
       "      <td>1504.300049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>{'headline_count': 0, 'avg_headline_sent': 0.0...</td>\n",
       "      <td>{'posts': 1, 'avg': 0.10337463021278381}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HDFCBANK.NS</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>965.150024</td>\n",
       "      <td>989.074999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.385725</td>\n",
       "      <td>False</td>\n",
       "      <td>WATCH</td>\n",
       "      <td>965.150024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>{'headline_count': 0, 'avg_headline_sent': 0.0...</td>\n",
       "      <td>{'posts': 0, 'avg': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RELIANCE.NS</td>\n",
       "      <td>47.413168</td>\n",
       "      <td>50.0</td>\n",
       "      <td>42.609052</td>\n",
       "      <td>-0.147819</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1376.199951</td>\n",
       "      <td>1424.033997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.978568</td>\n",
       "      <td>False</td>\n",
       "      <td>WATCH</td>\n",
       "      <td>1376.199951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>{'headline_count': 0, 'avg_headline_sent': 0.0...</td>\n",
       "      <td>{'posts': 8, 'avg': -0.14781895140185952}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BAJFINANCE.NS</td>\n",
       "      <td>45.354219</td>\n",
       "      <td>50.0</td>\n",
       "      <td>36.726341</td>\n",
       "      <td>-0.265473</td>\n",
       "      <td>50.0</td>\n",
       "      <td>948.400024</td>\n",
       "      <td>907.013998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.975002</td>\n",
       "      <td>False</td>\n",
       "      <td>WATCH</td>\n",
       "      <td>948.400024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>{'headline_count': 0, 'avg_headline_sent': 0.0...</td>\n",
       "      <td>{'posts': 1, 'avg': -0.2654731869697571}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          symbol        TRS     Q    S_prime     S_raw     N        Close  \\\n",
       "0         TCS.NS  53.928422  50.0  61.224063  0.224481  50.0  3049.399902   \n",
       "1        INFY.NS  51.809056  50.0  55.168732  0.103375  50.0  1504.300049   \n",
       "2    HDFCBANK.NS  50.000000  50.0  50.000000  0.000000  50.0   965.150024   \n",
       "3    RELIANCE.NS  47.413168  50.0  42.609052 -0.147819  50.0  1376.199951   \n",
       "4  BAJFINANCE.NS  45.354219  50.0  36.726341 -0.265473  50.0   948.400024   \n",
       "\n",
       "         SMA50  SMA200      ATR14  BiasUp Direction        Entry  Stop  \\\n",
       "0  3149.572002     NaN  49.435686   False     WATCH  3049.399902   NaN   \n",
       "1  1520.967993     NaN  31.128575   False     WATCH  1504.300049   NaN   \n",
       "2   989.074999     NaN  12.385725   False     WATCH   965.150024   NaN   \n",
       "3  1424.033997     NaN  20.978568   False     WATCH  1376.199951   NaN   \n",
       "4   907.013998     NaN  16.975002   False     WATCH   948.400024   NaN   \n",
       "\n",
       "   Target  PositionSize                                         NewsDetail  \\\n",
       "0     NaN             0  {'headline_count': 0, 'avg_headline_sent': 0.0...   \n",
       "1     NaN             0  {'headline_count': 0, 'avg_headline_sent': 0.0...   \n",
       "2     NaN             0  {'headline_count': 0, 'avg_headline_sent': 0.0...   \n",
       "3     NaN             0  {'headline_count': 0, 'avg_headline_sent': 0.0...   \n",
       "4     NaN             0  {'headline_count': 0, 'avg_headline_sent': 0.0...   \n",
       "\n",
       "                                SocialDetail  \n",
       "0    {'posts': 4, 'avg': 0.2244812697172165}  \n",
       "1   {'posts': 1, 'avg': 0.10337463021278381}  \n",
       "2                   {'posts': 0, 'avg': 0.0}  \n",
       "3  {'posts': 8, 'avg': -0.14781895140185952}  \n",
       "4   {'posts': 1, 'avg': -0.2654731869697571}  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def run_trs_pipeline(symbols):\n",
    "    # Fundamentals\n",
    "    dfQ = fundamentals_score(symbols)\n",
    "    qmap = {r.symbol: r.Q for r in dfQ.itertuples(index=False)}\n",
    "\n",
    "    rows = []\n",
    "    for sym in symbols:\n",
    "        logger.info(f\"Processing {sym}\")\n",
    "        # Price\n",
    "        pr = fetch_price(sym)\n",
    "        if pr is None or pr.empty:\n",
    "            logger.warning(f\"No price data for {sym}; skipping.\")\n",
    "            continue\n",
    "        pr = enrich_ta(pr)\n",
    "\n",
    "        # Sentiment (news + social)\n",
    "        n_score, n_detail = news_score_for_ticker(sym, hours=SENTIMENT_LOOKBACK_HOURS)\n",
    "        s_social, s_detail = social_sentiment_for_ticker(sym, hours=SENTIMENT_LOOKBACK_HOURS)\n",
    "\n",
    "        # Fuse S: combine news headline sentiment and social (simple average)\n",
    "        # Note: news_score_for_ticker returns N, but also has avg headline sentiment in [-1,+1]\n",
    "        # We can recompute a news-only sentiment by re-running classify on titles; here we mix social only.\n",
    "        # For robustness, you can fetch the news texts from news_score_for_ticker again.\n",
    "        s_raw = s_social  # keep it simple; news contributes via N\n",
    "\n",
    "        # Q score\n",
    "        q = qmap.get(sym, 50.0)\n",
    "\n",
    "        # TRS row\n",
    "        row = compute_trs_row(sym, q, s_raw, n_score, pr)\n",
    "        # Add details\n",
    "        row[\"NewsDetail\"] = n_detail\n",
    "        row[\"SocialDetail\"] = s_detail\n",
    "        rows.append(row)\n",
    "\n",
    "    out = pd.DataFrame(rows).sort_values(\"TRS\", ascending=False).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "signals = run_trs_pipeline(TICKERS)\n",
    "signals.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf89234",
   "metadata": {},
   "source": [
    "## 9. Save signals & show Topâ€‘N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ca7a3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to trs_signals.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>TRS</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Entry</th>\n",
       "      <th>Stop</th>\n",
       "      <th>Target</th>\n",
       "      <th>PositionSize</th>\n",
       "      <th>Q</th>\n",
       "      <th>S_prime</th>\n",
       "      <th>N</th>\n",
       "      <th>SMA50</th>\n",
       "      <th>SMA200</th>\n",
       "      <th>ATR14</th>\n",
       "      <th>BiasUp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCS.NS</td>\n",
       "      <td>53.928422</td>\n",
       "      <td>WATCH</td>\n",
       "      <td>3049.399902</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>61.224063</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3149.572002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.435686</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFY.NS</td>\n",
       "      <td>51.809056</td>\n",
       "      <td>WATCH</td>\n",
       "      <td>1504.300049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>55.168732</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1520.967993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.128575</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HDFCBANK.NS</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>WATCH</td>\n",
       "      <td>965.150024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>989.074999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.385725</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RELIANCE.NS</td>\n",
       "      <td>47.413168</td>\n",
       "      <td>WATCH</td>\n",
       "      <td>1376.199951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>42.609052</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1424.033997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.978568</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BAJFINANCE.NS</td>\n",
       "      <td>45.354219</td>\n",
       "      <td>WATCH</td>\n",
       "      <td>948.400024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>36.726341</td>\n",
       "      <td>50.0</td>\n",
       "      <td>907.013998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.975002</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          symbol        TRS Direction        Entry  Stop  Target  \\\n",
       "0         TCS.NS  53.928422     WATCH  3049.399902   NaN     NaN   \n",
       "1        INFY.NS  51.809056     WATCH  1504.300049   NaN     NaN   \n",
       "2    HDFCBANK.NS  50.000000     WATCH   965.150024   NaN     NaN   \n",
       "3    RELIANCE.NS  47.413168     WATCH  1376.199951   NaN     NaN   \n",
       "4  BAJFINANCE.NS  45.354219     WATCH   948.400024   NaN     NaN   \n",
       "\n",
       "   PositionSize     Q    S_prime     N        SMA50  SMA200      ATR14  BiasUp  \n",
       "0             0  50.0  61.224063  50.0  3149.572002     NaN  49.435686   False  \n",
       "1             0  50.0  55.168732  50.0  1520.967993     NaN  31.128575   False  \n",
       "2             0  50.0  50.000000  50.0   989.074999     NaN  12.385725   False  \n",
       "3             0  50.0  42.609052  50.0  1424.033997     NaN  20.978568   False  \n",
       "4             0  50.0  36.726341  50.0   907.013998     NaN  16.975002   False  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "signals.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Saved to {OUTPUT_CSV}\")\n",
    "\n",
    "topn = signals.head(TOP_N).copy()\n",
    "display_cols = [\"symbol\",\"TRS\",\"Direction\",\"Entry\",\"Stop\",\"Target\",\"PositionSize\",\"Q\",\"S_prime\",\"N\",\"SMA50\",\"SMA200\",\"ATR14\",\"BiasUp\"]\n",
    "topn[display_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b5ea62",
   "metadata": {},
   "source": [
    "## 10. (Optional) Quick ATR backtest on latest signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cecd3595",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A very simple forward-simulation template:\n",
    "# - Enter on next day open (not executed here)\n",
    "# - Exit at stop/target if hit intraday, or after max_hold days\n",
    "# This is just a template; for realistic testing, you need walk-forward TRS recomputation.\n",
    "\n",
    "def quick_backtest_template(top_df, hold_days=10):\n",
    "    results = []\n",
    "    for r in top_df.itertuples(index=False):\n",
    "        sym = r.symbol\n",
    "        df = fetch_price(sym, period=\"6mo\", interval=\"1d\")\n",
    "        if df is None or len(df) < 50:\n",
    "            continue\n",
    "        # Entry assumed at last available Close for illustration\n",
    "        entry_idx = df.index[-1]\n",
    "        entry_price = float(df.loc[entry_idx, \"Close\"])\n",
    "        atr14 = float(atr(df.set_index(\"Date\"), 14).iloc[-1])\n",
    "        if math.isnan(atr14) or atr14 <= 0:\n",
    "            continue\n",
    "        stop = entry_price - ATR_MULT_STOP*atr14 if r.Direction==\"LONG\" else entry_price + ATR_MULT_STOP*atr14\n",
    "        tgt  = entry_price + ATR_MULT_TARGET*atr14 if r.Direction==\"LONG\" else entry_price - ATR_MULT_TARGET*atr14\n",
    "\n",
    "        # Simulate next 'hold_days' bars (imperfect because we need future data)\n",
    "        # Here, we simply compute hypothetical exit if we had that data. Template only.\n",
    "        results.append({\n",
    "            \"symbol\": sym, \"entry\": entry_price, \"stop\": stop, \"target\": tgt,\n",
    "            \"hypo_R\": (tgt - entry_price) / (entry_price - stop) if r.Direction==\"LONG\" else (entry_price - tgt) / (stop - entry_price)\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example run (commented to avoid confusion; this is illustrative only)\n",
    "# bt = quick_backtest_template(signals.head(TOP_N))\n",
    "# bt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528851a7",
   "metadata": {},
   "source": [
    "\n",
    "## Tips & Next Steps\n",
    "- **Improve fundamentals:** If you trade Indian equities, Alpha Vantage coverage can be sparse. Consider paid sources (FMP, TickerTape/Trendlyne APIs) or broker research dumps to populate Q metrics more reliably.\n",
    "- **Better news features:** Keep a per-ticker headline cache and compute a **rolling S' percentile** over 90 days to stabilize sentiment.\n",
    "- **Macro calendar:** Wire a real macro API (TradingEconomics, etc.) or maintain a small CSV of key **RBI/MoSPI/Fed/ECB** dates to drive the blackout multiplier.\n",
    "- **Execution & slippage:** Integrate your brokerâ€™s API (Groww/Kite/etc.) and include slippage/fees in the sizing + backtest.\n",
    "- **Model ensembling:** Blend FinBERT with Loughranâ€“McDonald lexicon deltas and Tweetâ€‘RoBERTa; calibrate weights on validation PnL, not just AUC.\n",
    "- **Walk-forward backtest:** Recompute TRS daily from **only data available at that time** and evaluate out-of-sample performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".talib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
