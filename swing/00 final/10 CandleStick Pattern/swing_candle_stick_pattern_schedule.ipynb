{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02853d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:44:41 | INFO | Universe size: 500\n",
      "2025-10-17 22:47:59 | INFO | Historical EOD: using 2016-01-11\n",
      "2025-10-17 22:47:59 | ERROR | \n",
      "1 Failed download:\n",
      "2025-10-17 22:47:59 | ERROR | ['^CNX500']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "2025-10-17 22:48:00 | INFO | Using benchmark: ^CRSLDX\n",
      "2025-10-17 22:48:01 | INFO | Positions -> portfolio_positions.csv | Trades -> trades_log.csv | Cash=500000.00\n",
      "2025-10-17 22:48:01 | INFO | Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, json, math, logging, warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "try:\n",
    "    import yfinance as yf\n",
    "except Exception:\n",
    "    yf = None\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "except Exception:\n",
    "    requests = None\n",
    "\n",
    "# -------------------- LOGGING --------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"daily_scheduler\")\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Data\n",
    "    start_date: str = \"2015-01-01\"\n",
    "    end_date: Optional[str] = None              # None -> through today\n",
    "    static_symbols: Optional[List[str]] = None\n",
    "    static_symbols_path: Optional[str] = \"nifty500.txt\"\n",
    "    cache_dir: str = \"cache\"\n",
    "    out_dir: str = \"outputs\"\n",
    "\n",
    "    # Historical run\n",
    "    override_trade_date: Optional[str] = \"2016-01-11\" #None   # e.g. \"2025-10-16\"\n",
    "\n",
    "    # Toggles (mirror backtest)\n",
    "    use_trend_fast_slow: bool = False\n",
    "    use_htf_trend: bool       = False\n",
    "    use_rsi_confirm: bool   = True\n",
    "    use_macd_confirm: bool  = False\n",
    "    use_adx_confirm: bool   = False\n",
    "    use_sma50_confirm: bool = False\n",
    "    use_bbmid_confirm: bool = False\n",
    "\n",
    "    use_basic_liquidity: bool = False\n",
    "    min_price_inr: float = 50.0\n",
    "    min_avg_vol_20d: float = 50_000.0\n",
    "\n",
    "    use_52w_filter: bool     = True\n",
    "    use_volar_ranking: bool  = True\n",
    "    use_mvo_sizing: bool     = True\n",
    "\n",
    "    ema_fast: int = 10\n",
    "    ema_slow: int = 20\n",
    "    ema_htf:  int = 200\n",
    "\n",
    "    stop_loss_pct: float = 0.05\n",
    "    target_pct:    float = 0.10\n",
    "\n",
    "    apply_fees: bool       = True\n",
    "    initial_capital: float = 500_000.0\n",
    "    max_concurrent_positions: int = 5\n",
    "    deploy_cash_frac: float = 0.25\n",
    "    top_k_daily: int = 300\n",
    "\n",
    "    benchmark_try: Tuple[str,...] = (\"^CNX500\",\"^CRSLDX\",\"^NSE500\",\"^NIFTY500\",\"^BSE500\",\"^NSEI\")\n",
    "    volar_lookback: int = 252\n",
    "    filter_52w_window: int = 252\n",
    "    within_pct_of_52w_high: float = 0.50\n",
    "\n",
    "    positions_csv: str = \"portfolio_positions.csv\"\n",
    "    trades_log_csv: str = \"trades_log.csv\"\n",
    "    account_state_path: str = \"account_state.json\"\n",
    "\n",
    "    # Telegram (optional)\n",
    "    enable_telegram: bool = False\n",
    "    telegram_bot_token: str = \"\"\n",
    "    telegram_chat_id: str = \"\"\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "# -------------------- FEES --------------------\n",
    "APPLY_FEES = True\n",
    "def calc_fees(turnover_buy: float, turnover_sell: float) -> float:\n",
    "    if not APPLY_FEES: return 0.0\n",
    "    BROKER_PCT = 0.001; BROKER_MIN = 5.0; BROKER_CAP = 20.0\n",
    "    STT_PCT = 0.001; STAMP_BUY_PCT = 0.00015\n",
    "    EXCH_PCT = 0.0000297; SEBI_PCT = 0.000001; IPFT_PCT = 0.000001\n",
    "    GST_PCT = 0.18; DP_SELL = 20.0 if turnover_sell >= 100 else 0.0\n",
    "    def _broker(turnover):\n",
    "        if turnover <= 0: return 0.0\n",
    "        fee = turnover * BROKER_PCT\n",
    "        return max(BROKER_MIN, min(fee, BROKER_CAP))\n",
    "    br_buy = _broker(turnover_buy); br_sell = _broker(turnover_sell)\n",
    "    stt = STT_PCT * (turnover_buy + turnover_sell)\n",
    "    stamp = STAMP_BUY_PCT * turnover_buy\n",
    "    exch = EXCH_PCT * (turnover_buy + turnover_sell)\n",
    "    sebi = SEBI_PCT * (turnover_buy + turnover_sell)\n",
    "    ipft = IPFT_PCT * (turnover_buy + turnover_sell)\n",
    "    dp = DP_SELL\n",
    "    gst_base = br_buy + br_sell + dp + exch + sebi + ipft\n",
    "    gst = GST_PCT * gst_base\n",
    "    return float((br_buy + br_sell) + stt + stamp + exch + sebi + ipft + dp + gst)\n",
    "\n",
    "# -------------------- HELPERS --------------------\n",
    "def ensure_dirs(*paths):\n",
    "    for p in paths: os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def today_str():\n",
    "    return pd.Timestamp.today(tz=\"Asia/Kolkata\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def load_static_symbols(static_symbols: Optional[List[str]], static_symbols_path: Optional[str]) -> List[str]:\n",
    "    if static_symbols and len(static_symbols) > 0:\n",
    "        syms = list(static_symbols)\n",
    "    elif static_symbols_path and os.path.exists(static_symbols_path):\n",
    "        with open(static_symbols_path, \"r\") as f:\n",
    "            syms = [line.strip() for line in f if line.strip()]\n",
    "    else:\n",
    "        raise ValueError(\"Provide CFG.static_symbols or CFG.static_symbols_path.\")\n",
    "    out = []\n",
    "    for s in syms:\n",
    "        s = s.strip().upper()\n",
    "        if not s.endswith(\".NS\"): s = f\"{s}.NS\"\n",
    "        out.append(s)\n",
    "    uniq, seen = [], set()\n",
    "    for s in out:\n",
    "        if s not in seen:\n",
    "            uniq.append(s); seen.add(s)\n",
    "    return uniq\n",
    "\n",
    "def fetch_prices(tickers: List[str], start: str, end: Optional[str], cache_dir: str) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"yfinance end is exclusive -> request end+1d; fix cache freshness vs desired_last.\"\"\"\n",
    "    ensure_dirs(cache_dir)\n",
    "    data = {}\n",
    "    if end:\n",
    "        desired_last = pd.to_datetime(end).normalize()\n",
    "        end_excl = (desired_last + pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    else:\n",
    "        today_ist = pd.Timestamp.today(tz=\"Asia/Kolkata\").normalize()\n",
    "        desired_last = today_ist\n",
    "        end_excl = (today_ist + pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    for ticker in tickers:\n",
    "        cache_path = os.path.join(cache_dir, f\"{ticker.replace('^','_')}.parquet\")\n",
    "        if os.path.exists(cache_path):\n",
    "            try:\n",
    "                df = pd.read_parquet(cache_path)\n",
    "                if len(df):\n",
    "                    last_in_cache = pd.to_datetime(df.index[-1]).normalize()\n",
    "                    if last_in_cache >= desired_last:\n",
    "                        data[ticker] = df; continue\n",
    "            except Exception:\n",
    "                pass\n",
    "        try:\n",
    "            df = yf.download(\n",
    "                ticker, start=start, end=end_excl,\n",
    "                auto_adjust=True, progress=False, multi_level_index=False,\n",
    "            )\n",
    "            if df is None or df.empty: continue\n",
    "            df = df.rename(columns=str.title)[['Open','High','Low','Close','Volume']].dropna()\n",
    "            df.index.name = \"date\"\n",
    "            df.to_parquet(cache_path)\n",
    "            data[ticker] = df\n",
    "        except Exception as e:\n",
    "            log.warning(\"Download failed %s: %s\", ticker, e); continue\n",
    "    return data\n",
    "\n",
    "def send_telegram(text: str, cfg: Config):\n",
    "    if not cfg.enable_telegram: return\n",
    "    if not requests:\n",
    "        log.warning(\"requests not available; cannot send Telegram\"); return\n",
    "    if not cfg.telegram_bot_token or not cfg.telegram_chat_id:\n",
    "        log.warning(\"Telegram token/chat_id missing; skip\"); return\n",
    "    try:\n",
    "        url = f\"https://api.telegram.org/bot{cfg.telegram_bot_token}/sendMessage\"\n",
    "        payload = {\"chat_id\": cfg.telegram_chat_id, \"text\": text, \"parse_mode\": \"HTML\", \"disable_web_page_preview\": True}\n",
    "        requests.post(url, json=payload, timeout=10)\n",
    "    except Exception as e:\n",
    "        log.warning(\"Telegram send failed: %s\", e)\n",
    "\n",
    "# -------------------- INDICATORS & PATTERNS --------------------\n",
    "def sma(series: pd.Series, window: int) -> pd.Series:\n",
    "    return series.rolling(window, min_periods=window).mean()\n",
    "\n",
    "def ema(series: pd.Series, span: int) -> pd.Series:\n",
    "    return series.ewm(span=span, adjust=False, min_periods=span).mean()\n",
    "\n",
    "def rsi(series: pd.Series, length: int = 14) -> pd.Series:\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0.0)).rolling(length).mean()\n",
    "    loss = (-delta.where(delta < 0, 0.0)).rolling(length).mean()\n",
    "    rs = gain / loss.replace(0.0, np.nan)\n",
    "    return (100 - (100 / (1 + rs))).fillna(50.0)\n",
    "\n",
    "def macd(series: pd.Series, fast=12, slow=26, signal=9):\n",
    "    ef = ema(series, fast); es = ema(series, slow)\n",
    "    line = ef - es; sig = ema(line, signal)\n",
    "    return line, sig, line - sig\n",
    "\n",
    "def _true_range(high: pd.Series, low: pd.Series, prev_close: pd.Series) -> pd.Series:\n",
    "    return pd.concat([(high - low).abs(), (high - prev_close).abs(), (low - prev_close).abs()], axis=1).max(axis=1)\n",
    "\n",
    "def adx_plus_minus_di(high: pd.Series, low: pd.Series, close: pd.Series, length: int = 14):\n",
    "    prev_high = high.shift(1); prev_low = low.shift(1); prev_close = close.shift(1)\n",
    "    up_move = high - prev_high; down_move = prev_low - low\n",
    "    plus_dm = up_move.where((up_move > down_move) & (up_move > 0), 0.0)\n",
    "    minus_dm = down_move.where((down_move > up_move) & (down_move > 0), 0.0)\n",
    "    tr = _true_range(high, low, prev_close)\n",
    "    alpha = 1.0 / length\n",
    "    atr = tr.ewm(alpha=alpha, adjust=False, min_periods=length).mean()\n",
    "    plus_di  = 100 * (plus_dm.ewm(alpha=alpha, adjust=False, min_periods=length).mean() / atr)\n",
    "    minus_di = 100 * (minus_dm.ewm(alpha=alpha, adjust=False, min_periods=length).mean() / atr)\n",
    "    dx = 100 * (plus_di - minus_di).abs() / (plus_di + minus_di).replace(0, np.nan)\n",
    "    adx_series = dx.ewm(alpha=alpha, adjust=False, min_periods=length).mean()\n",
    "    return adx_series, plus_di, minus_di\n",
    "\n",
    "def _body(o, c): return (c - o).abs()\n",
    "def _is_bull(o, c): return c > o\n",
    "def _is_bear(o, c): return c < o\n",
    "\n",
    "def patt_bull_engulfing(df: pd.DataFrame) -> pd.Series:\n",
    "    o, c = df[\"Open\"], df[\"Close\"]; op, cp = o.shift(1), c.shift(1)\n",
    "    return (_is_bear(op, cp) & _is_bull(o, c) & (c >= op) & (o <= cp)).fillna(False)\n",
    "\n",
    "def patt_piercing(df: pd.DataFrame) -> pd.Series:\n",
    "    o, h, l, c = df[\"Open\"], df[\"High\"], df[\"Low\"], df[\"Close\"]\n",
    "    op, _, lp, cp = o.shift(1), h.shift(1), l.shift(1), c.shift(1)\n",
    "    mid_prev = (op + cp) / 2.0\n",
    "    return (_is_bear(op, cp) & _is_bull(o, c) & (o < lp) & (c > mid_prev) & (c < op)).fillna(False)\n",
    "\n",
    "def patt_morning_star(df: pd.DataFrame) -> pd.Series:\n",
    "    o, c = df[\"Open\"], df[\"Close\"]; o1, c1 = o.shift(1), c.shift(1); o2, c2 = o.shift(2), c.shift(2)\n",
    "    bear1 = _is_bear(o2, c2); small2 = (_body(o1, c1) <= (_body(o2, c2) * 0.6)); bull3 = _is_bull(o, c)\n",
    "    mid1 = (o2 + c2) / 2.0; retrace = c > mid1\n",
    "    return (bear1 & small2 & bull3 & retrace).fillna(False)\n",
    "\n",
    "def patt_harami_bull(df: pd.DataFrame) -> pd.Series:\n",
    "    o, c = df[\"Open\"], df[\"Close\"]; o1, c1 = o.shift(1), c.shift(1)\n",
    "    prev_bear = _is_bear(o1, c1)\n",
    "    body_small = (_body(o, c) <= _body(o1, c1) * 0.75)\n",
    "    inside = (np.maximum(o, c) <= o1) & (np.minimum(o, c) >= c1)\n",
    "    return (prev_bear & body_small & inside & (c >= o)).fillna(False)\n",
    "\n",
    "def patt_harami_cross_bull(df: pd.DataFrame, doji_pct=0.1) -> pd.Series:\n",
    "    o, c, h, l = df[\"Open\"], df[\"Close\"], df[\"High\"], df[\"Low\"]; o1, c1 = o.shift(1), c.shift(1)\n",
    "    rng = (h - l).replace(0, np.nan); doji = (_body(o, c) <= (rng * doji_pct))\n",
    "    prev_bear = _is_bear(o1, c1); inside = (np.maximum(o, c) <= o1) & (np.minimum(o, c) >= c1)\n",
    "    return (prev_bear & doji & inside).fillna(False)\n",
    "\n",
    "def patt_hammer(df: pd.DataFrame, shadow_mult=2.0) -> pd.Series:\n",
    "    o, c, h, l = df[\"Open\"], df[\"Close\"], df[\"High\"], df[\"Low\"]\n",
    "    body = _body(o, c); lower_shadow = (np.minimum(o, c) - l).abs(); upper_shadow = (h - np.maximum(o, c)).abs()\n",
    "    return ((lower_shadow >= shadow_mult * body) & (upper_shadow <= body) & (c >= o)).fillna(False)\n",
    "\n",
    "def patt_inverted_hammer(df: pd.DataFrame, shadow_mult=2.0) -> pd.Series:\n",
    "    o, c, h, l = df[\"Open\"], df[\"Close\"], df[\"High\"], df[\"Low\"]\n",
    "    body = _body(o, c); upper_shadow = (h - np.maximum(o, c)).abs(); lower_shadow = (np.minimum(o, c) - l).abs()\n",
    "    return ((upper_shadow >= shadow_mult * body) & (lower_shadow <= body) & (c >= o)).fillna(False)\n",
    "\n",
    "BULLISH_PATTERNS = [\"ENGULFING\",\"PIERCING\",\"MORNING_STAR\",\"HARAMI\",\"HARAMI_CROSS\",\"HAMMER\",\"INVERTED_HAMMER\"]\n",
    "\n",
    "def compute_indicators(df: pd.DataFrame, cfg: Config) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"ema_fast\"] = ema(out[\"Close\"], cfg.ema_fast)\n",
    "    out[\"ema_slow\"] = ema(out[\"Close\"], cfg.ema_slow)\n",
    "    out[\"ema_htf\"]  = ema(out[\"Close\"], cfg.ema_htf)\n",
    "    out[\"rsi\"] = rsi(out[\"Close\"], 14)\n",
    "    macd_line, macd_sig, _ = macd(out[\"Close\"], 12, 26, 9)\n",
    "    out[\"macd_line\"] = macd_line; out[\"macd_signal\"] = macd_sig\n",
    "    out[\"sma50\"] = sma(out[\"Close\"], 50); out[\"bb_mid\"] = sma(out[\"Close\"], 20)\n",
    "    adxv, pdi, ndi = adx_plus_minus_di(out[\"High\"], out[\"Low\"], out[\"Close\"], 14)\n",
    "    out[\"adx\"] = adxv; out[\"+di\"] = pdi; out[\"-di\"] = ndi\n",
    "    out[\"avg_vol_20\"] = out[\"Volume\"].rolling(20).mean()\n",
    "    out[\"high_52w\"] = out[\"Close\"].rolling(cfg.filter_52w_window).max()\n",
    "\n",
    "    patt_map = {\n",
    "        \"ENGULFING\":       patt_bull_engulfing(out),\n",
    "        \"PIERCING\":        patt_piercing(out),\n",
    "        \"MORNING_STAR\":    patt_morning_star(out),\n",
    "        \"HARAMI\":          patt_harami_bull(out),\n",
    "        \"HARAMI_CROSS\":    patt_harami_cross_bull(out),\n",
    "        \"HAMMER\":          patt_hammer(out),\n",
    "        \"INVERTED_HAMMER\": patt_inverted_hammer(out),\n",
    "    }\n",
    "    patt_any = None\n",
    "    for name in BULLISH_PATTERNS:\n",
    "        patt_any = patt_map[name] if patt_any is None else (patt_any | patt_map[name])\n",
    "    out[\"bullish_pattern\"] = patt_any.fillna(False)\n",
    "    return out.dropna()\n",
    "\n",
    "def _and_if_enabled(enabled: bool, cond: pd.Series) -> pd.Series:\n",
    "    return cond.astype(bool).fillna(False) if enabled else pd.Series(True, index=cond.index)\n",
    "\n",
    "def basic_liquidity_ok(row: pd.Series, cfg: Config) -> bool:\n",
    "    if not cfg.use_basic_liquidity: return True\n",
    "    if row[\"Close\"] < cfg.min_price_inr: return False\n",
    "    if row[\"avg_vol_20\"] < cfg.min_avg_vol_20d: return False\n",
    "    return True\n",
    "\n",
    "# -------------------- VOLAR & SIZING --------------------\n",
    "def pick_benchmark(benchmarks: Tuple[str,...], start: str, end: Optional[str], cache_dir: str) -> Tuple[str, pd.DataFrame]:\n",
    "    for t in benchmarks:\n",
    "        data = fetch_prices([t], start, end, cache_dir)\n",
    "        df = data.get(t)\n",
    "        if df is not None and not df.empty:\n",
    "            log.info(\"Using benchmark: %s\", t); return t, df\n",
    "    idx = pd.date_range(start=start, end=end or today_str(), freq=\"B\")\n",
    "    df = pd.DataFrame({\"Close\": np.ones(len(idx))}, index=idx)\n",
    "    log.warning(\"No benchmark found; using synthetic flat series.\")\n",
    "    return \"SYNTH_BENCH\", df\n",
    "\n",
    "def compute_volar_scores(end_dt: pd.Timestamp, tickers: List[str], data_map: Dict[str,pd.DataFrame], bench_df: pd.DataFrame, lookback: int) -> Dict[str, float]:\n",
    "    scores = {}\n",
    "    bser = bench_df[\"Close\"].loc[:end_dt].pct_change().dropna().iloc[-lookback:]\n",
    "    for t in tickers:\n",
    "        df = data_map.get(t)\n",
    "        if df is None or df.empty: scores[t] = 0.0; continue\n",
    "        if end_dt not in df.index:\n",
    "            df = df[df.index <= end_dt]\n",
    "            if df.empty: scores[t] = 0.0; continue\n",
    "        r = df[\"Close\"].loc[:end_dt].pct_change().dropna().iloc[-lookback:]\n",
    "        common = pd.concat([r, bser], axis=1, keys=[\"s\",\"b\"]).dropna()\n",
    "        if common.shape[0] < max(20, int(0.4*lookback)):\n",
    "            scores[t] = 0.0; continue\n",
    "        excess = common[\"s\"] - common[\"b\"]; vol = common[\"s\"].std(ddof=0)\n",
    "        scores[t] = 0.0 if vol <= 1e-8 else float((excess.mean() / vol) * math.sqrt(252.0))\n",
    "    return scores\n",
    "\n",
    "def markowitz_long_only(mu: np.ndarray, Sigma: np.ndarray) -> np.ndarray:\n",
    "    n = len(mu); eps = 1e-6\n",
    "    Sigma = Sigma + eps*np.eye(n)\n",
    "    def solve_lambda(lmbd: float, active_mask=None):\n",
    "        if active_mask is None:\n",
    "            A = np.block([[2*lmbd*Sigma, np.ones((n,1))],[np.ones((1,n)), np.zeros((1,1))]])\n",
    "            b = np.concatenate([mu, np.array([1.0])])\n",
    "            try: w = np.linalg.solve(A, b)[:n]\n",
    "            except np.linalg.LinAlgError: w = np.full(n, 1.0/n)\n",
    "            return w\n",
    "        else:\n",
    "            idx = np.where(active_mask)[0]\n",
    "            if len(idx)==0: return np.full(n, 1.0/n)\n",
    "            S = Sigma[np.ix_(idx, idx)]\n",
    "            o = np.ones(len(idx)); m = mu[idx]\n",
    "            A = np.block([[2*lmbd*S, o[:,None]],[o[None,:], np.zeros((1,1))]])\n",
    "            b = np.concatenate([m, np.array([1.0])])\n",
    "            try: w_sub = np.linalg.solve(A, b)[:len(idx)]\n",
    "            except np.linalg.LinAlgError: w_sub = np.full(len(idx), 1.0/len(idx))\n",
    "            w = np.zeros(n); w[idx] = w_sub; return w\n",
    "    best_w = np.full(n, 1.0/n); best_sr = -1e9\n",
    "    for lmbd in np.logspace(-3, 3, 31):\n",
    "        active = np.ones(n, dtype=bool); w = None\n",
    "        for _ in range(n):\n",
    "            w = solve_lambda(lmbd, active_mask=active)\n",
    "            if not (w < 0).any(): break\n",
    "            active[np.argmin(w)] = False\n",
    "        if w is None: continue\n",
    "        w = np.clip(w, 0, None)\n",
    "        if w.sum() <= 0: continue\n",
    "        w = w / w.sum()\n",
    "        mu_p = float(mu @ w); vol_p = float(np.sqrt(w @ Sigma @ w))\n",
    "        if vol_p <= 1e-8: continue\n",
    "        sr = mu_p / vol_p\n",
    "        if sr > best_sr: best_w = w.copy(); best_sr = sr\n",
    "    return best_w\n",
    "\n",
    "# -------------------- DATE UTIL --------------------\n",
    "def next_bar_date_for_ticker(df: pd.DataFrame, run_dt: pd.Timestamp) -> Optional[pd.Timestamp]:\n",
    "    if df is None or df.empty: return None\n",
    "    idx = df.index\n",
    "    pos = idx.searchsorted(run_dt, side=\"right\")\n",
    "    return idx[pos] if pos < len(idx) else None\n",
    "\n",
    "# -------------------- SIGNALS (NO FILTERS/CAPS HERE) --------------------\n",
    "def build_eod_signals_for_date(scan_dt: pd.Timestamp,\n",
    "                               symbols: List[str],\n",
    "                               data_map: Dict[str, pd.DataFrame],\n",
    "                               cfg: Config) -> pd.DataFrame:\n",
    "    \"\"\"Emit ALL valid signals on scan_dt. Do NOT 52w-filter or slot-cap here.\"\"\"\n",
    "    recs = []\n",
    "    for tkr in symbols:\n",
    "        df = data_map.get(tkr)\n",
    "        if df is None or df.empty or scan_dt not in df.index: continue\n",
    "        d = compute_indicators(df.loc[:scan_dt], cfg)\n",
    "        if d.empty or scan_dt not in d.index: continue\n",
    "        row = d.loc[scan_dt]\n",
    "        patt_ok = bool(row[\"bullish_pattern\"])\n",
    "        conds = []\n",
    "        conds.append(_and_if_enabled(cfg.use_trend_fast_slow, (d[\"ema_fast\"] > d[\"ema_slow\"])))\n",
    "        conds.append(_and_if_enabled(cfg.use_htf_trend, (d[\"Close\"] > d[\"ema_htf\"])))\n",
    "        conds.append(_and_if_enabled(cfg.use_rsi_confirm,  (d[\"rsi\"] > 50.0)))\n",
    "        conds.append(_and_if_enabled(cfg.use_macd_confirm, (d[\"macd_line\"] > d[\"macd_signal\"])))\n",
    "        conds.append(_and_if_enabled(cfg.use_adx_confirm,  ((d[\"adx\"] > 20.0) & (d[\"+di\"] > d[\"-di\"]))))\n",
    "        conds.append(_and_if_enabled(cfg.use_sma50_confirm,(d[\"Close\"] > d[\"sma50\"])))\n",
    "        conds.append(_and_if_enabled(cfg.use_bbmid_confirm,(d[\"Close\"] > d[\"bb_mid\"])))\n",
    "        if cfg.use_basic_liquidity:\n",
    "            liq_ok = bool(basic_liquidity_ok(row, cfg))\n",
    "            conds.append(pd.Series(liq_ok, index=d.index))\n",
    "        all_ok = True\n",
    "        for c in conds: all_ok = all_ok & bool(c.loc[scan_dt])\n",
    "        if not (patt_ok and all_ok): continue\n",
    "\n",
    "        patt_map = {\n",
    "            \"ENGULFING\":       patt_bull_engulfing(d),\n",
    "            \"PIERCING\":        patt_piercing(d),\n",
    "            \"MORNING_STAR\":    patt_morning_star(d),\n",
    "            \"HARAMI\":          patt_harami_bull(d),\n",
    "            \"HARAMI_CROSS\":    patt_harami_cross_bull(d),\n",
    "            \"HAMMER\":          patt_hammer(d),\n",
    "            \"INVERTED_HAMMER\": patt_inverted_hammer(d),\n",
    "        }\n",
    "        patterns = [nm for nm, ser in patt_map.items() if bool(ser.loc[scan_dt])]\n",
    "        patt_str = \" + \".join(patterns) if patterns else \"BullishPattern\"\n",
    "\n",
    "        sigs = []\n",
    "        if cfg.use_trend_fast_slow and bool((d[\"ema_fast\"] > d[\"ema_slow\"]).loc[scan_dt]): sigs.append(\"EMAfast>EMAslow\")\n",
    "        if cfg.use_htf_trend and bool((d[\"Close\"] > d[\"ema_htf\"]).loc[scan_dt]):           sigs.append(\"Close>EMA200\")\n",
    "        if cfg.use_rsi_confirm and bool((d[\"rsi\"] > 50).loc[scan_dt]):                     sigs.append(\"RSI>50\")\n",
    "        if cfg.use_macd_confirm and bool((d[\"macd_line\"] > d[\"macd_signal\"]).loc[scan_dt]):sigs.append(\"MACD>Signal\")\n",
    "        if cfg.use_adx_confirm and bool(((d[\"adx\"] > 20) & (d[\"+di\"] > d[\"-di\"])).loc[scan_dt]): sigs.append(\"ADX>20 & +DI>-DI\")\n",
    "        if cfg.use_sma50_confirm and bool((d[\"Close\"] > d[\"sma50\"]).loc[scan_dt]):         sigs.append(\"Close>SMA50\")\n",
    "        if cfg.use_bbmid_confirm and bool((d[\"Close\"] > d[\"bb_mid\"]).loc[scan_dt]):        sigs.append(\"Close>BBmid\")\n",
    "        if cfg.use_basic_liquidity and bool(basic_liquidity_ok(row, cfg)):                 sigs.append(\"LiquidityOK\")\n",
    "\n",
    "        reason = \"; \".join([f\"Entry: {patt_str} + TogglesOK\", f\"Conf=[{', '.join(sigs) if sigs else 'pattern_only'}]\"])\n",
    "\n",
    "        recs.append({\"date\": scan_dt, \"ticker\": tkr, \"pattern\": patt_str, \"reason\": reason})\n",
    "\n",
    "    recs_df = pd.DataFrame(recs)\n",
    "    if recs_df.empty: return recs_df\n",
    "    return recs_df.reset_index(drop=True)\n",
    "\n",
    "# -------------------- POSITIONS / ACCOUNT I/O --------------------\n",
    "POS_COLS = [\n",
    "    \"ticker\",\"status\",\"planned_entry_date\",\"entry_date\",\"entry_px\",\"qty\",\n",
    "    \"stop_px\",\"tgt_px\",\"reason\",\"volar\",\"mvo_weight\",\"alloc_inr\",\"buy_fee\",\"notes\"\n",
    "]\n",
    "\n",
    "def load_positions(path: str) -> pd.DataFrame:\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        for c in POS_COLS:\n",
    "            if c not in df.columns: df[c] = np.nan\n",
    "        df = df[POS_COLS]\n",
    "        df[\"planned_entry_date\"] = pd.to_datetime(df[\"planned_entry_date\"], errors=\"coerce\")\n",
    "        df[\"entry_date\"] = pd.to_datetime(df[\"entry_date\"], errors=\"coerce\")\n",
    "        return df\n",
    "    return pd.DataFrame(columns=POS_COLS)\n",
    "\n",
    "def save_positions(df: pd.DataFrame, path: str):\n",
    "    df[POS_COLS].to_csv(path, index=False)\n",
    "\n",
    "def append_trades_log(path: str, rows: List[dict]):\n",
    "    cols = [\"ticker\",\"entry_date\",\"entry_px\",\"qty\",\"exit_date\",\"exit_px\",\"exit_reason\",\n",
    "            \"gross_pnl_inr\",\"fees_total_inr\",\"net_pnl_inr\",\"days_held\",\"notes\"]\n",
    "    if not rows: return\n",
    "    new_df = pd.DataFrame(rows, columns=cols)\n",
    "    if os.path.exists(path):\n",
    "        old = pd.read_csv(path); out = pd.concat([old, new_df], ignore_index=True)\n",
    "    else:\n",
    "        out = new_df\n",
    "    out.to_csv(path, index=False)\n",
    "\n",
    "def load_account_state(path: str, initial_cash: float) -> dict:\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {\"cash\": float(initial_cash)}\n",
    "\n",
    "def save_account_state(path: str, state: dict):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(state, f, indent=2)\n",
    "\n",
    "# -------------------- DATE HELPERS --------------------\n",
    "def find_latest_trade_date_from_map(data_map: Dict[str, pd.DataFrame]) -> Optional[pd.Timestamp]:\n",
    "    for df in data_map.values():\n",
    "        if df is not None and not df.empty:\n",
    "            return df.index[-1]\n",
    "    return None\n",
    "\n",
    "def find_trade_date_at_or_before(target: pd.Timestamp, data_map: Dict[str, pd.DataFrame]) -> Optional[pd.Timestamp]:\n",
    "    dates = []\n",
    "    for df in data_map.values():\n",
    "        if df is None or df.empty: continue\n",
    "        dts = df.index[df.index <= target]\n",
    "        if len(dts): dates.append(dts[-1])\n",
    "    return max(dates) if dates else None\n",
    "\n",
    "# -------------------- DAILY PIPELINE --------------------\n",
    "def run_daily(cfg: Config):\n",
    "    ensure_dirs(cfg.cache_dir, cfg.out_dir)\n",
    "    global APPLY_FEES\n",
    "    APPLY_FEES = bool(cfg.apply_fees)\n",
    "\n",
    "    # Universe + Data\n",
    "    symbols = load_static_symbols(cfg.static_symbols, cfg.static_symbols_path)\n",
    "    log.info(\"Universe size: %d\", len(symbols))\n",
    "    data_map = fetch_prices(symbols, cfg.start_date, cfg.end_date, cfg.cache_dir)\n",
    "    if not data_map:\n",
    "        log.error(\"No data fetched. Abort.\"); return\n",
    "\n",
    "    # Determine run date\n",
    "    if cfg.override_trade_date:\n",
    "        target = pd.to_datetime(cfg.override_trade_date)\n",
    "        run_dt = find_trade_date_at_or_before(target, data_map)\n",
    "        if run_dt is None:\n",
    "            log.error(\"No trading day on/before %s found in data.\", target.date()); return\n",
    "        log.info(\"Historical EOD: using %s\", run_dt.date())\n",
    "    else:\n",
    "        run_dt = find_latest_trade_date_from_map(data_map)\n",
    "        if run_dt is None:\n",
    "            log.error(\"Could not determine latest trading date.\"); return\n",
    "        log.info(\"Live EOD: latest trading date %s\", run_dt.date())\n",
    "\n",
    "    # Benchmark (single instance for the run)\n",
    "    _, bench_df = pick_benchmark(cfg.benchmark_try, cfg.start_date, cfg.end_date, cfg.cache_dir)\n",
    "\n",
    "    # Load positions & account\n",
    "    pos = load_positions(cfg.positions_csv)\n",
    "    acct = load_account_state(cfg.account_state_path, cfg.initial_capital)\n",
    "    cash = float(acct.get(\"cash\", cfg.initial_capital))\n",
    "\n",
    "    # 1) SELL first (TP > SL)\n",
    "    exits_to_log = []\n",
    "    open_mask = (pos[\"status\"]==\"open\")\n",
    "    for i, r in pos[open_mask].iterrows():\n",
    "        tkr = r[\"ticker\"]; df = data_map.get(tkr)\n",
    "        if df is None or df.empty or run_dt not in df.index: continue\n",
    "        bar = df.loc[run_dt]\n",
    "        low = float(bar[\"Low\"]); high = float(bar[\"High\"])\n",
    "        entry_px = float(r[\"entry_px\"]); qty = int(r[\"qty\"])\n",
    "        stop_px = float(r[\"stop_px\"]);  tgt_px = float(r[\"tgt_px\"])\n",
    "        entry_fee = float(r.get(\"buy_fee\", 0.0)) if not pd.isna(r.get(\"buy_fee\", np.nan)) else 0.0\n",
    "\n",
    "        exit_reason = None; exit_px = None\n",
    "        if (low <= stop_px) and (high >= tgt_px):\n",
    "            exit_reason = \"TakeProfit hit\"; exit_px = tgt_px\n",
    "        elif low <= stop_px:\n",
    "            exit_reason = \"StopLoss hit\"; exit_px = stop_px\n",
    "        elif high >= tgt_px:\n",
    "            exit_reason = \"TakeProfit hit\"; exit_px = tgt_px\n",
    "        else:\n",
    "            d = compute_indicators(df.loc[:run_dt], cfg)\n",
    "            if run_dt in d.index and bool((d.loc[run_dt, \"ema_fast\"] < d.loc[run_dt, \"ema_slow\"])):\n",
    "                notes = str(r.get(\"notes\",\"\"))\n",
    "                if \"exit_pending_next_open\" not in notes:\n",
    "                    pos.loc[i, \"notes\"] = (notes + \" | exit_pending_next_open\").strip(\" |\")\n",
    "                continue\n",
    "\n",
    "        if exit_reason is not None:\n",
    "            # SELL proceeds & fees\n",
    "            turn_sell = exit_px * qty\n",
    "            sell_fee = calc_fees(0.0, turn_sell)\n",
    "            cash += (turn_sell - sell_fee)  # add proceeds net of SELL fee\n",
    "            gross = (exit_px - entry_px) * qty\n",
    "            total_fees = entry_fee + sell_fee\n",
    "            net = gross - total_fees\n",
    "            days_held = int((pd.Timestamp(run_dt) - pd.Timestamp(r[\"entry_date\"])).days)\n",
    "            exits_to_log.append({\n",
    "                \"ticker\": tkr, \"entry_date\": pd.Timestamp(r[\"entry_date\"]).strftime(\"%Y-%m-%d\"),\n",
    "                \"entry_px\": float(entry_px), \"qty\": int(qty),\n",
    "                \"exit_date\": pd.Timestamp(run_dt).strftime(\"%Y-%m-%d\"),\n",
    "                \"exit_px\": float(exit_px), \"exit_reason\": exit_reason,\n",
    "                \"gross_pnl_inr\": float(gross), \"fees_total_inr\": float(total_fees), \"net_pnl_inr\": float(net),\n",
    "                \"days_held\": int(days_held), \"notes\": str(r.get(\"reason\",\"\"))\n",
    "            })\n",
    "            pos = pos.drop(index=i)\n",
    "            send_telegram(f\"üîî <b>EXIT</b> {tkr}\\n{exit_reason} @ {exit_px:.2f} on {run_dt.date()}\", cfg)\n",
    "\n",
    "    if exits_to_log:\n",
    "        append_trades_log(cfg.trades_log_csv, exits_to_log)\n",
    "        log.info(\"Logged %d exits | cash=%.2f\", len(exits_to_log), cash)\n",
    "\n",
    "    # 2) Delayed exits (trend invalidation ‚Üí next open)\n",
    "    delayed_mask = (pos[\"status\"]==\"open\") & (pos[\"notes\"].fillna(\"\").str.contains(\"exit_pending_next_open\"))\n",
    "    delayed_rows = pos[delayed_mask].copy()\n",
    "    delayed_logs = []\n",
    "    for i, r in delayed_rows.iterrows():\n",
    "        tkr = r[\"ticker\"]; df = data_map.get(tkr)\n",
    "        if df is None or df.empty or run_dt not in df.index: continue\n",
    "        open_px = float(df.loc[run_dt, \"Open\"])\n",
    "        entry_px = float(r[\"entry_px\"]); qty = int(r[\"qty\"])\n",
    "        entry_fee = float(r.get(\"buy_fee\", 0.0)) if not pd.isna(r.get(\"buy_fee\", np.nan)) else 0.0\n",
    "        turn_sell = open_px * qty\n",
    "        sell_fee = calc_fees(0.0, turn_sell)\n",
    "        cash += (turn_sell - sell_fee)\n",
    "        gross = (open_px - entry_px) * qty\n",
    "        net = gross - (entry_fee + sell_fee)\n",
    "        days_held = int((pd.Timestamp(run_dt) - pd.Timestamp(r[\"entry_date\"])).days)\n",
    "        delayed_logs.append({\n",
    "            \"ticker\": tkr, \"entry_date\": pd.Timestamp(r[\"entry_date\"]).strftime(\"%Y-%m-%d\"),\n",
    "            \"entry_px\": float(entry_px), \"qty\": int(qty),\n",
    "            \"exit_date\": pd.Timestamp(run_dt).strftime(\"%Y-%m-%d\"),\n",
    "            \"exit_px\": float(open_px), \"exit_reason\": \"Trend invalidation -> exited at next open\",\n",
    "            \"gross_pnl_inr\": float(gross), \"fees_total_inr\": float(entry_fee + sell_fee), \"net_pnl_inr\": float(net),\n",
    "            \"days_held\": int(days_held), \"notes\": str(r.get(\"reason\",\"\"))\n",
    "        })\n",
    "        pos = pos.drop(index=i)\n",
    "        send_telegram(f\"‚ö†Ô∏è <b>EXIT @ NEXT OPEN</b>\\n{tkr} @ {open_px:.2f} on {run_dt.date()}\", cfg)\n",
    "\n",
    "    if delayed_logs:\n",
    "        append_trades_log(cfg.trades_log_csv, delayed_logs)\n",
    "        log.info(\"Logged %d delayed exits | cash=%.2f\", len(delayed_logs), cash)\n",
    "\n",
    "    # 3) Confirm pending BUYs (planned_date <= run_dt)\n",
    "    pend = pos[(pos[\"status\"]==\"pending_entry\") & (pos[\"planned_entry_date\"]<=pd.Timestamp(run_dt))].copy()\n",
    "    if not pend.empty:\n",
    "        # Exclude already-open\n",
    "        already_open = set(pos.loc[pos[\"status\"]==\"open\",\"ticker\"])\n",
    "        pend = pend[~pend[\"ticker\"].isin(already_open)].copy()\n",
    "\n",
    "        tickers = pend[\"ticker\"].tolist()\n",
    "        volar_map = compute_volar_scores(pd.Timestamp(run_dt), tickers, data_map, bench_df, CFG.volar_lookback) if CFG.use_volar_ranking else {t: np.nan for t in tickers}\n",
    "\n",
    "        filt_rows = []\n",
    "        for _, rr in pend.iterrows():\n",
    "            t = rr[\"ticker\"]; df = data_map.get(t)\n",
    "            if df is None or df.empty or run_dt not in df.index: continue\n",
    "            # 52w filter ON ENTRY DAY\n",
    "            hist = df[\"Close\"].loc[:run_dt]\n",
    "            window = hist.iloc[-CFG.filter_52w_window:] if len(hist)>=CFG.filter_52w_window else hist\n",
    "            h52 = float(window.max()) if len(window) else np.nan\n",
    "            close_val = float(df.loc[run_dt, \"Close\"])\n",
    "            if CFG.use_52w_filter and not (h52>0 and close_val >= CFG.within_pct_of_52w_high * h52):\n",
    "                continue\n",
    "            rr = rr.copy(); rr[\"volar\"] = float(volar_map.get(t, np.nan))\n",
    "            filt_rows.append(rr)\n",
    "\n",
    "        pend2 = pd.DataFrame(filt_rows)\n",
    "        if not pend2.empty and CFG.use_volar_ranking:\n",
    "            pend2 = pend2.sort_values(\"volar\", ascending=False)\n",
    "\n",
    "        # Slots and top_k_daily cap (match backtest)\n",
    "        open_now = pos[pos[\"status\"]==\"open\"].shape[0]\n",
    "        slots = max(0, CFG.max_concurrent_positions - open_now)\n",
    "        cap = min(CFG.top_k_daily, slots)\n",
    "        to_confirm = pend2.head(cap) if not pend2.empty and cap>0 else pd.DataFrame(columns=pend2.columns)\n",
    "\n",
    "        # MVO sizing among selected (like backtest)\n",
    "        weights = np.array([])\n",
    "        if not to_confirm.empty and CFG.use_mvo_sizing and to_confirm.shape[0] >= 2:\n",
    "            names = to_confirm[\"ticker\"].tolist()\n",
    "            R = []\n",
    "            for t in names:\n",
    "                ser = data_map[t][\"Close\"].loc[:run_dt].pct_change().dropna().iloc[-CFG.volar_lookback:]\n",
    "                R.append(ser)\n",
    "            R = pd.concat(R, axis=1); R.columns = names; R = R.dropna()\n",
    "            if R.shape[0] >= max(20, int(0.4*CFG.volar_lookback)):\n",
    "                mu = R.mean().values; Sigma = R.cov().values\n",
    "                weights = markowitz_long_only(mu, Sigma)\n",
    "            else:\n",
    "                weights = np.full(len(names), 1.0/len(names))\n",
    "        elif not to_confirm.empty:\n",
    "            weights = np.full(len(to_confirm), 1.0/len(to_confirm))\n",
    "\n",
    "        # Cash-based sizing & fee-aware admission\n",
    "        deploy_cash = max(0.0, float(cash)) * float(CFG.deploy_cash_frac)\n",
    "        if not to_confirm.empty and deploy_cash <= 0:\n",
    "            log.info(\"No deployable cash (cap=%.0f%%). Skipping confirmations.\", 100*CFG.deploy_cash_frac)\n",
    "            to_confirm = pd.DataFrame(columns=to_confirm.columns)\n",
    "\n",
    "        for k, (_, rr) in enumerate(to_confirm.iterrows()):\n",
    "            t = rr[\"ticker\"]; df = data_map.get(t)\n",
    "            if df is None or df.empty or run_dt not in df.index: continue\n",
    "            open_px = float(df.loc[run_dt, \"Open\"])\n",
    "            w = float(weights[k]) if len(weights) else 1.0\n",
    "            alloc_inr = float((weights.sum() and deploy_cash * w) if len(weights) else deploy_cash)\n",
    "            if alloc_inr <= 0:\n",
    "                continue\n",
    "            qty = int(math.floor(alloc_inr / open_px))\n",
    "            if qty <= 0:\n",
    "                continue\n",
    "            turn_buy = qty * open_px\n",
    "            buy_fee = calc_fees(turn_buy, 0.0)\n",
    "            total_cost = turn_buy + buy_fee\n",
    "            if total_cost > cash:\n",
    "                # shrink qty to fit cash\n",
    "                qty = int(math.floor((cash - buy_fee) / open_px))\n",
    "                if qty <= 0:\n",
    "                    continue\n",
    "                turn_buy = qty * open_px\n",
    "                buy_fee = calc_fees(turn_buy, 0.0)\n",
    "                total_cost = turn_buy + buy_fee\n",
    "                if total_cost > cash:\n",
    "                    continue\n",
    "\n",
    "            # Admit & deduct cash\n",
    "            cash -= total_cost\n",
    "            stop_px = float(open_px * (1 - CFG.stop_loss_pct))\n",
    "            tgt_px  = float(open_px * (1 + CFG.target_pct))\n",
    "\n",
    "            idx_in_pos = pos.index[(pos[\"ticker\"]==t) & (pos[\"status\"]==\"pending_entry\")].tolist()\n",
    "            if not idx_in_pos: continue\n",
    "            j = idx_in_pos[0]\n",
    "            pos.loc[j, \"status\"] = \"open\"\n",
    "            pos.loc[j, \"entry_date\"] = run_dt\n",
    "            pos.loc[j, \"entry_px\"] = open_px\n",
    "            pos.loc[j, \"qty\"] = qty\n",
    "            pos.loc[j, \"stop_px\"] = stop_px\n",
    "            pos.loc[j, \"tgt_px\"] = tgt_px\n",
    "            pos.loc[j, \"mvo_weight\"] = (float(weights[k]) if len(weights) else np.nan)\n",
    "            pos.loc[j, \"alloc_inr\"] = float(alloc_inr)\n",
    "            pos.loc[j, \"buy_fee\"] = float(buy_fee)\n",
    "            pos.loc[j, \"notes\"] = (str(pos.loc[j, \"notes\"]) + \" | confirmed@open\").strip(\" |\")\n",
    "\n",
    "            send_telegram(\n",
    "                f\"‚úÖ <b>ENTRY CONFIRMED</b>\\n{t} @ {open_px:.2f} on {run_dt.date()}\\n\"\n",
    "                f\"Qty: {qty} | SL: {stop_px:.2f} | TP: {tgt_px:.2f}\\n{pos.loc[j, 'reason']}\", CFG\n",
    "            )\n",
    "\n",
    "    # 4) New signals today ‚Üí plan all for next bar (NO caps/filters/sizing here)\n",
    "    recos = build_eod_signals_for_date(run_dt, symbols, data_map, CFG)\n",
    "    held = set(pos[\"ticker\"].tolist())\n",
    "    if not recos.empty:\n",
    "        recos = recos[~recos[\"ticker\"].isin(held)].reset_index(drop=True)\n",
    "\n",
    "    # Plan per-ticker next available bar (no caps)\n",
    "    new_rows = []\n",
    "    if not recos.empty:\n",
    "        for _, r in recos.iterrows():\n",
    "            t = r[\"ticker\"]; df_t = data_map.get(t)\n",
    "            nxt_dt = next_bar_date_for_ticker(df_t, run_dt)\n",
    "            if nxt_dt is None:\n",
    "                log.info(\"Skip planning %s (no next bar after %s)\", t, run_dt.date()); continue\n",
    "            new_rows.append({\n",
    "                \"ticker\": t, \"status\": \"pending_entry\",\n",
    "                \"planned_entry_date\": pd.Timestamp(nxt_dt),\n",
    "                \"entry_date\": pd.NaT, \"entry_px\": np.nan, \"qty\": np.nan,\n",
    "                \"stop_px\": np.nan, \"tgt_px\": np.nan,\n",
    "                \"reason\": str(r[\"reason\"]), \"volar\": np.nan,\n",
    "                \"mvo_weight\": np.nan, \"alloc_inr\": np.nan, \"buy_fee\": np.nan, \"notes\": \"planned\"\n",
    "            })\n",
    "\n",
    "        stamp = pd.Timestamp(run_dt).strftime(\"%Y%m%d\")\n",
    "        recos_out = os.path.join(CFG.out_dir, f\"eod_recos_{stamp}.csv\")\n",
    "        recos.to_csv(recos_out, index=False)\n",
    "        log.info(\"Wrote recommendations -> %s\", recos_out)\n",
    "\n",
    "        lines = [f\"üìù <b>EOD Recos {run_dt.date()}</b> (planned for next bar)\"]\n",
    "        for _, rr in recos.iterrows():\n",
    "            lines.append(f\"‚Ä¢ {rr['ticker']}: {rr['pattern']} | {rr['reason']}\")\n",
    "        send_telegram(\"\\n\".join(lines[:50]), CFG)\n",
    "\n",
    "    if new_rows:\n",
    "        add_df = pd.DataFrame(new_rows, columns=POS_COLS)\n",
    "        pos = pd.concat([pos, add_df], ignore_index=True)\n",
    "\n",
    "    # Save positions & account cash\n",
    "    ensure_dirs(CFG.out_dir)\n",
    "    save_positions(pos, CFG.positions_csv)\n",
    "    save_account_state(CFG.account_state_path, {\"cash\": float(cash)})\n",
    "    log.info(\"Positions -> %s | Trades -> %s | Cash=%.2f\", CFG.positions_csv, CFG.trades_log_csv, cash)\n",
    "    log.info(\"Done.\")\n",
    "\n",
    "def main():\n",
    "    global APPLY_FEES\n",
    "    APPLY_FEES = bool(CFG.apply_fees)\n",
    "    if yf is None:\n",
    "        log.error(\"yfinance not available.\"); return\n",
    "    run_daily(CFG)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".talib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
