{"cells":[{"cell_type":"markdown","id":"dfc57bc2-40f5-498f-92f3-b8ecbf948988","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"0b84fb3b-06cc-4ccb-a758-1de054d33751","metadata":{},"outputs":[],"source":["<h1>Image Datasets and Transforms</h1> \n"]},{"cell_type":"markdown","id":"43d406b9-932d-45b9-b01e-51d499494d85","metadata":{},"outputs":[],"source":["<h2>Objective</h2><ul><li> How to build a image dataset object.</li><li> How to perform pre-build transforms from Torchvision Transforms to the dataset. .</li></ul> \n"]},{"cell_type":"markdown","id":"60b83a94-40a6-4d0d-a5a6-02a2262a9792","metadata":{},"outputs":[],"source":["<h2>Table of Contents</h2>\n","<p>In this lab, you will build a dataset objects for images; many of the processes can be applied to a larger dataset. Then you will apply pre-build transforms from Torchvision Transforms to that dataset.</p>\n","<ul>\n","    <li><a href=\"#auxiliary\"> Auxiliary Functions </a></li>\n","    <li><a href=\"#Dataset\"> Datasets</a></li>\n","    <li><a href=\"#Torchvision\">Torchvision Transforms</a></li>\n","</ul>\n","<p>Estimated Time Needed: <strong>25 min</strong></p>\n","\n","<hr>\n"]},{"cell_type":"markdown","id":"310a2ea4-0a52-4176-97ce-ee855d3c096a","metadata":{},"outputs":[],"source":["<h2>Preparation</h2>\n"]},{"cell_type":"markdown","id":"ebac07fc-0287-4f1b-b329-b2448fe1ec68","metadata":{},"outputs":[],"source":["Download the dataset and unzip the files in your data directory, **to download faster this dataset has only 100 samples**:\n"]},{"cell_type":"code","id":"c5fa8cdb-f7ec-435b-bc5a-72603190759e","metadata":{},"outputs":[],"source":["! wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/labs/Week1/data/img.tar.gz -P /resources/data\n"]},{"cell_type":"code","id":"007df4d9-f125-444a-a527-2076a7054a38","metadata":{},"outputs":[],"source":["!tar -xf /resources/data/img.tar.gz "]},{"cell_type":"code","id":"8e8444e7-4ce2-4af7-b71c-47f81f046c04","metadata":{},"outputs":[],"source":["!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/labs/Week1/data/index.csv "]},{"cell_type":"markdown","id":"b156db03-b9c0-4a16-a00e-f5212bfca272","metadata":{},"outputs":[],"source":["We will use this function in the lab:\n"]},{"cell_type":"code","id":"6158cf4f-dbe0-4108-b5f2-4dd616c88ec9","metadata":{},"outputs":[],"source":["def show_data(data_sample, shape = (28, 28)):\n    plt.imshow(data_sample[0].numpy().reshape(shape), cmap='gray')\n    plt.title('y = ' + data_sample[1])"]},{"cell_type":"markdown","id":"263e77c7-513b-4778-a2b8-da5d981a7362","metadata":{},"outputs":[],"source":["The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it.\n"]},{"cell_type":"code","id":"3a9848f8-4ccf-476f-819e-de14b8de9342","metadata":{},"outputs":[],"source":["%%time\n%pip install matplotlib numpy pandas\n%pip install torch==2.8.0+cpu torchvision==0.23.0+cpu torchaudio==2.8.0+cpu \\\n    --index-url https://download.pytorch.org/whl/cpu"]},{"cell_type":"code","id":"c6d85f13-a288-47b5-916f-90e77b679d90","metadata":{},"outputs":[],"source":["# These are the libraries will be used for this lab.\n\nimport torch \nimport matplotlib.pylab as plt\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\ntorch.manual_seed(0)"]},{"cell_type":"code","id":"dc5e2418-05d8-4820-a037-bcaf6930d5be","metadata":{},"outputs":[],"source":["from matplotlib.pyplot import imshow\nimport matplotlib.pylab as plt\nfrom PIL import Image\nimport pandas as pd\nimport os"]},{"cell_type":"markdown","id":"c60977b7-f9ba-4f38-aba0-b96c45721704","metadata":{},"outputs":[],"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"791b78f6-3cee-404b-9fb2-509cb4435d7c","metadata":{},"outputs":[],"source":["<h2 id=\"auxiliary\">Auxiliary Functions</h2>\n"]},{"cell_type":"markdown","id":"9a667205-5a5a-4a5c-ac94-e9a762ab5c71","metadata":{},"outputs":[],"source":["You will use the following function as components of a dataset object, in this section, you will review each of the components independently.\n"]},{"cell_type":"markdown","id":"5e661b3f-6541-43d3-864d-7b9723b0a506","metadata":{},"outputs":[],"source":[" The path to the csv file with the labels for each image.\n"]},{"cell_type":"code","id":"8736f46d-f317-464e-b607-3f2e52b789fc","metadata":{},"outputs":[],"source":["# Read CSV file from the URL and print out the first five samples\ndirectory=\"\"\ncsv_file ='index.csv'\ncsv_path=os.path.join(directory,csv_file)"]},{"cell_type":"markdown","id":"890516a0-e308-4ee0-93ce-eeee42efe90e","metadata":{},"outputs":[],"source":["You can load the CSV file and convert it into a dataframe , using the Pandas function <code>read_csv()</code> . You can view the dataframe using the method head.\n"]},{"cell_type":"code","id":"7ac5186b-6b82-4571-889a-4f215c61d932","metadata":{},"outputs":[],"source":["data_name = pd.read_csv(csv_path)\ndata_name.head()"]},{"cell_type":"markdown","id":"9ac89a91-7730-42b3-8bc1-fb647987359e","metadata":{},"outputs":[],"source":["The first column of the dataframe corresponds to the type of clothing. The second column is the name of the image file corresponding to the clothing. You can obtain the path of the first file by using the method  <code> <i>DATAFRAME</i>.iloc[0, 1]</code>. The first argument corresponds to the sample number, and the second input corresponds to the column index. \n"]},{"cell_type":"code","id":"5c6c843e-a35b-4a27-8b15-2401261e4f5e","metadata":{},"outputs":[],"source":["# Get the value on location row 0, column 1 (Notice that index starts at 0)\n#rember this dataset has only 100 samples to make the download faster  \nprint('File name:', data_name.iloc[0, 1])"]},{"cell_type":"markdown","id":"bcf1ba80-8a17-4b5c-8bf2-c0b3787089ef","metadata":{},"outputs":[],"source":["As the class of the sample is in the first column, you can also obtain the class value as follows.\n"]},{"cell_type":"code","id":"31f95445-4bde-442d-b541-005d9e48f212","metadata":{},"outputs":[],"source":["# Get the value on location row 0, column 0 (Notice that index starts at 0.)\n\nprint('y:', data_name.iloc[0, 0])"]},{"cell_type":"markdown","id":"39f9e739-a848-43dc-9a42-58b2849e50f9","metadata":{},"outputs":[],"source":["Similarly, You can obtain the file name of the second image file and class type:\n"]},{"cell_type":"code","id":"6d5989e7-f847-4687-8eb1-d36b6e68cb97","metadata":{},"outputs":[],"source":["# Print out the file name and the class number of the element on row 1 (the second row)\n\nprint('File name:', data_name.iloc[1, 1])\nprint('class or y:', data_name.iloc[1, 0])"]},{"cell_type":"markdown","id":"75382110-b8e4-4235-aaaf-2076c6cfd6cd","metadata":{},"outputs":[],"source":["The number of samples corresponds to the number of rows in a dataframe. You can obtain the number of rows using the following lines of code. This will correspond the data attribute <code>len</code>.\n"]},{"cell_type":"code","id":"c3368779-6a4d-4efa-a3bc-3defe3fa8e54","metadata":{},"outputs":[],"source":["# Print out the total number of rows in traing dataset\n\nprint('The number of rows: ', data_name.shape[0])"]},{"cell_type":"markdown","id":"6e3fa274-bca4-49dd-a316-0c5cf028f8d7","metadata":{},"outputs":[],"source":["<h2 id=\"load_image\">Load Image</h2>\n"]},{"cell_type":"markdown","id":"26f4ff17-aabc-445f-b046-06d4b117692b","metadata":{},"outputs":[],"source":["To load the image, you need the directory and the image name. You can concatenate the variable <code>train_data_dir</code> with the name of the image stored in a Dataframe. Finally, you will store the result in the variable <code>image_name</code>\n"]},{"cell_type":"code","id":"d74224b6-dc05-414c-94fe-1cedd83857ef","metadata":{},"outputs":[],"source":["# Combine the directory path with file name\n\nimage_name =data_name.iloc[1, 1]\nimage_name"]},{"cell_type":"markdown","id":"5518f90d-9814-4bf8-b453-e3295c612d9f","metadata":{},"outputs":[],"source":["we can find the image path:\n"]},{"cell_type":"code","id":"3ed82bce-0079-4c46-bf30-6f95a3bdba01","metadata":{},"outputs":[],"source":["image_path=os.path.join(directory,image_name)\nimage_path"]},{"cell_type":"markdown","id":"2c0b61a3-bcdd-4276-ad1b-8448d0eaaaf7","metadata":{},"outputs":[],"source":["You can then use the function <code>Image.open</code> to store the image to the variable <code>image</code> and display the image and class .\n"]},{"cell_type":"code","id":"42fe75b4-69a3-4a35-9e74-0a9d2186f234","metadata":{},"outputs":[],"source":["# Plot the second training image\n\nimage = Image.open(image_path)\nplt.imshow(image,cmap='gray', vmin=0, vmax=255)\nplt.title(data_name.iloc[1, 0])\nplt.show()"]},{"cell_type":"markdown","id":"1339eb2c-4d56-4db8-a955-587ce883ca8a","metadata":{},"outputs":[],"source":["You can repeat the process for the 20th image.\n"]},{"cell_type":"code","id":"45314ad6-91d4-4086-849b-d6aab065684e","metadata":{},"outputs":[],"source":["# Plot the 20th image\n\nimage_name = data_name.iloc[19, 1]\nimage_path=os.path.join(directory,image_name)\nimage = Image.open(image_path)\nplt.imshow(image,cmap='gray', vmin=0, vmax=255)\nplt.title(data_name.iloc[19, 0])\nplt.show()"]},{"cell_type":"markdown","id":"aa1c9d1b-e7b7-48a5-bf96-80ef613b02f5","metadata":{},"outputs":[],"source":["<hr>\n"]},{"cell_type":"markdown","id":"fbfb7c15-de75-44ec-97b5-5a0e7e6902e9","metadata":{},"outputs":[],"source":[" Create the dataset object.\n"]},{"cell_type":"markdown","id":"c1790772-3a56-4af5-ab73-1357f0fb39be","metadata":{},"outputs":[],"source":["<h2 id=\"data_class\">Create a Dataset Class</h2>\n"]},{"cell_type":"markdown","id":"b9573470-0e15-409d-912d-e81eaf2a7925","metadata":{},"outputs":[],"source":["In this section, we will use the components in the last section to build a dataset class and then create an object.\n"]},{"cell_type":"code","id":"d0bb2b7f-195d-401d-8a9a-f5a2770f381c","metadata":{},"outputs":[],"source":["# Create your own dataset object\n\nclass Dataset(Dataset):\n\n    # Constructor\n    def __init__(self, csv_file, data_dir, transform=None):\n        \n        # Image directory\n        self.data_dir=data_dir\n        \n        # The transform is goint to be used on image\n        self.transform = transform\n        data_dircsv_file=os.path.join(self.data_dir,csv_file)\n        # Load the CSV file contians image info\n        self.data_name= pd.read_csv(data_dircsv_file)\n        \n        # Number of images in dataset\n        self.len=self.data_name.shape[0] \n    \n    # Get the length\n    def __len__(self):\n        return self.len\n    \n    # Getter\n    def __getitem__(self, idx):\n        \n        # Image file path\n        img_name=os.path.join(self.data_dir,self.data_name.iloc[idx, 1])\n        # Open image file\n        image = Image.open(img_name)\n        \n        # The class label for the image\n        y = self.data_name.iloc[idx, 0]\n        \n        # If there is any transform method, apply it onto the image\n        if self.transform:\n            image = self.transform(image)\n\n        return image, y"]},{"cell_type":"code","id":"0ee00f66-92c5-4ee8-834c-5ede66aaf76a","metadata":{},"outputs":[],"source":["# Create the dataset objects\n\ndataset = Dataset(csv_file=csv_file, data_dir=directory)"]},{"cell_type":"markdown","id":"2e5039bb-3b48-4598-be65-e1624db6aa2a","metadata":{},"outputs":[],"source":["Each sample of the image and the class y is stored in a tuple <code> dataset[sample]</code> . The image is the first element in the tuple <code> dataset[sample][0]</code> the label or class is the second element in the tuple <code> dataset[sample][1]</code>. For example you can plot the first image and class.\n"]},{"cell_type":"code","id":"2244736c-e341-4e37-a9fc-fb15f7231b90","metadata":{},"outputs":[],"source":["image=dataset[0][0]\ny=dataset[0][1]\n\nplt.imshow(image,cmap='gray', vmin=0, vmax=255)\nplt.title(y)\nplt.show()"]},{"cell_type":"code","id":"7a7e47d2-2763-4913-b6b6-4f347d533db1","metadata":{},"outputs":[],"source":["y"]},{"cell_type":"markdown","id":"47084013-033a-4dfd-bd1d-6df8917f1f7c","metadata":{},"outputs":[],"source":["Similarly, you can plot the second image: \n"]},{"cell_type":"code","id":"a387c505-b2ca-46a8-b498-7b3bef256b5f","metadata":{},"outputs":[],"source":["image=dataset[9][0]\ny=dataset[9][1]\n\nplt.imshow(image,cmap='gray', vmin=0, vmax=255)\nplt.title(y)\nplt.show()"]},{"cell_type":"markdown","id":"34eb3a75-da4d-4da8-8cf4-b3d6ad065060","metadata":{},"outputs":[],"source":["<h2 id=\"Torchvision\"> Torchvision Transforms  </h2>\n"]},{"cell_type":"markdown","id":"e3ff3119-1f2c-4a80-8b8a-75f584219756","metadata":{},"outputs":[],"source":[" \n","You will focus on the following libraries:\n"]},{"cell_type":"code","id":"dc3f3659-9e8e-4be2-942d-e25aa07c926a","metadata":{},"outputs":[],"source":["import torchvision.transforms as transforms"]},{"cell_type":"markdown","id":"8d94ede7-9a92-4645-ae8b-2c98269692f7","metadata":{},"outputs":[],"source":["We can apply some image transform functions on the dataset object. The iamge can be cropped and converted to a tensor. We can use <code>transform.Compose</code> we learned from the previous lab to combine the two transform functions.\n"]},{"cell_type":"code","id":"da76fdce-43af-46e1-a4e3-9104659a62d7","metadata":{},"outputs":[],"source":["# Combine two transforms: crop and convert to tensor. Apply the compose to MNIST dataset\n\ncroptensor_data_transform = transforms.Compose([transforms.CenterCrop(20), transforms.ToTensor()])\ndataset = Dataset(csv_file=csv_file , data_dir=directory,transform=croptensor_data_transform )\nprint(\"The shape of the first element tensor: \", dataset[0][0].shape)\n"]},{"cell_type":"markdown","id":"a959dbda-a372-4977-891f-6093529995e8","metadata":{},"outputs":[],"source":["We can see the image is now 20 x 20\n"]},{"cell_type":"markdown","id":"b398bebf-9142-457b-b024-c123fe3dd255","metadata":{},"outputs":[],"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"823fee8a-68a8-445b-8037-3cc1f1d7986c","metadata":{},"outputs":[],"source":["Let us plot the first image again. Notice we see less of the shoe.\n"]},{"cell_type":"code","id":"ea46d4fc-4eb9-4533-85c4-0787feb5fb78","metadata":{},"outputs":[],"source":["# Plot the first element in the dataset\n\nshow_data(dataset[0],shape = (20, 20))"]},{"cell_type":"code","id":"bd24d824-c544-4c48-99bc-9c0854d036b2","metadata":{},"outputs":[],"source":["# Plot the second element in the dataset\n\nshow_data(dataset[1],shape = (20, 20))"]},{"cell_type":"markdown","id":"a7cbdc1e-84f5-41bf-be7b-a73037291880","metadata":{},"outputs":[],"source":["In the below example, we Vertically flip the image, and then convert it to a tensor. Use <code>transforms.Compose()</code> to combine these two transform functions. Plot the flipped image.\n"]},{"cell_type":"code","id":"0c7d648c-5e4d-4d7a-a463-8c933b936b8b","metadata":{},"outputs":[],"source":["# Construct the compose. Apply it on MNIST dataset. Plot the image out.\n\nfliptensor_data_transform = transforms.Compose([transforms.RandomVerticalFlip(p=1),transforms.ToTensor()])\ndataset = Dataset(csv_file=csv_file , data_dir=directory,transform=fliptensor_data_transform )\nshow_data(dataset[1])"]},{"cell_type":"markdown","id":"37ead45e-e08d-4b58-9bf2-f777143570de","metadata":{},"outputs":[],"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"1b747057-53e2-4581-9e27-175934b2adbd","metadata":{},"outputs":[],"source":["<h3>Practice</h3>\n"]},{"cell_type":"markdown","id":"223b511a-f5b5-4245-91d4-cfcb12546dd2","metadata":{},"outputs":[],"source":["Try to use the <code>RandomVerticalFlip</code> (vertically flip the image) with horizontally flip and convert to tensor as a compose. Apply the compose on image. Use <code>show_data()</code> to plot the second image (the image as <b>2</b>).\n"]},{"cell_type":"code","id":"7ac32859-f2bc-440b-86cf-81f0b930ed2b","metadata":{},"outputs":[],"source":["# Practice: Combine vertical flip, horizontal flip and convert to tensor as a compose. Apply the compose on image. Then plot the image\n\n# Type your code here"]},{"cell_type":"markdown","id":"b1c41901-6d01-4571-a737-5f486fdf3941","metadata":{},"outputs":[],"source":["Double-click __here__ for the solution.\n","<!-- \n","my_data_transform = transforms.Compose([transforms.RandomVerticalFlip(p = 1), transforms.RandomHorizontalFlip(p = 1), transforms.ToTensor()])\n","dataset = Dataset(csv_file=csv_file , data_dir=directory,transform=fliptensor_data_transform )\n","show_data(dataset[1])\n"," -->\n"]},{"cell_type":"markdown","id":"a88d9464-de59-47cc-a839-d145138ebd20","metadata":{},"outputs":[],"source":["<a href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork&context=cpdaas&apps=data_science_experience%2Cwatson_machine_learning\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"></a>\n"]},{"cell_type":"markdown","id":"f5f9dc47-658d-4d70-93b7-a53fff077ee8","metadata":{},"outputs":[],"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"6f9d77e4-0c49-466d-803b-65a6952c3f41","metadata":{},"outputs":[],"source":["<h2>About the Authors:</h2> \n","\n","<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"]},{"cell_type":"markdown","id":"a48b36ce-a915-431a-8a37-544b88c80f7c","metadata":{},"outputs":[],"source":["Other contributors: <a href=\"https://www.linkedin.com/in/michelleccarey/\">Michelle Carey</a>, <a href=\"www.linkedin.com/in/jiahui-mavis-zhou-a4537814a\">Mavis Zhou</a> \n"]},{"cell_type":"markdown","id":"2952c649-350c-4b59-89d4-55f329759bb2","metadata":{},"outputs":[],"source":["<!--\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-21  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","\n","-->\n"]},{"cell_type":"markdown","id":"38b3ae5c-9cad-4807-9287-7af8d20e008b","metadata":{},"outputs":[],"source":["<hr>\n"]},{"cell_type":"markdown","id":"6bc9b615-f4bb-4cf6-bd72-c18b12dd8dc8","metadata":{},"outputs":[],"source":["## <h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.12.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"d47a1a90c019192a625d1e70dc0e23af9a4be425797837e42d099a056b628654"},"nbformat":4,"nbformat_minor":4}