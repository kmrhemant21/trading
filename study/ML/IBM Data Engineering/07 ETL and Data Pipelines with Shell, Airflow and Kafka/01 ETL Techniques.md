# ETL Techniques

## Key Terms

**ETL Tools:** Software applications that facilitate the ETL process, providing features like automation, ease of use, and support for complex transformations.

**Data Extraction:** The first stage of the ETL process where data is acquired from various source systems, which can include raw, unstructured, or streaming data.

**Data Loading:** The final phase of the ETL process where the transformed data is written to a target system, such as a database or data warehouse.

**Data Transformation:** The stage where data is cleaned and processed to ensure reliability and compatibility with the target system, often performed in a staging area.

**ETL:** ETL stands for Extract, Transform, Load, which is a process for curating data from multiple sources and loading it into a target system.

> **Note:** This content was generated by AI, so please check for any mistakes.

## Overview

ETL stands for Extract, Transform, and Load, and refers to the process of curating data from multiple sources, conforming it to a unified data format or structure, and loading the transformed data into its new environment.

![ETL Process](./images/etl-process.png)
*Figure 1: ETL is an acronym used to describe the main processes behind a data pipeline design methodology that stands for Extract-Transform-Load. Data is extracted from disparate sources to an intermediate staging area where it is integrated and prepared for loading into a destination such as a data warehouse.*

## The ETL Process

### Extract

Data extraction is the first stage of the ETL process, where data is acquired from various source systems. The data may be:

- Completely raw (e.g., sensor data from IoT devices)
- Unstructured data (e.g., scanned medical documents or company emails)
- Streaming data (e.g., social media feeds)
- Near real-time data (e.g., stock market transactions)
- Existing enterprise databases and data warehouses

### Transform

The transformation stage is where rules and processes are applied to the data to prepare it for loading into the target system. This is normally done in an intermediate working environment called a "staging area." Here, the data are cleaned to ensure reliability and conformed to ensure compatibility with the target system.

Common transformations include:

- **Cleaning:** Fixing any errors or missing values
- **Filtering:** Selecting only what is needed
- **Joining:** Merging disparate data sources
- **Normalizing:** Converting data to common units
- **Data Structuring:** Converting one data format to another (JSON, XML, CSV to database tables)
- **Feature Engineering:** Creating KPIs for dashboards or machine learning
- **Anonymizing and Encrypting:** Ensuring privacy and security
- **Sorting:** Ordering the data to improve search performance
- **Aggregating:** Summarizing granular data

### Load

The load phase involves writing the transformed data to a target system. The target can be:

- Simple files (e.g., comma-separated files)
- Databases
- Data warehouses
- Data marts
- Data lakes
- Other unified, centralized data stores

In most cases, as data is being loaded into a database, the constraints defined by its schema must be satisfied for the workflow to run successfully. The schema includes integrity constraints such as:

- Uniqueness
- Referential integrity
- Mandatory fields

These requirements help ensure overall data quality.

## ETL Workflows as Data Pipelines

Generally, an ETL workflow is a well thought out process that is carefully engineered to meet technical and end-user requirements.

Traditionally, the overall accuracy of the ETL workflow has been more important than speed, although efficiency is usually an important factor in minimizing resource costs. To boost efficiency, data is fed through a data pipeline in smaller packets.

![Data Pipeline](./images/data-pipeline.png)
*Figure 2: Data packets being fed in sequence, or "piped" through the ETL data pipeline. Ideally, by the time the third packet is ingested, all three ETL processes are running simultaneously on different packets.*

### Batch Processing

With conventional ETL pipelines, data is processed in batches, usually on a repeating schedule that ranges from hours to days apart. For example, records accumulating in an Online Transaction Processing System (OLTP) can be moved as a daily batch process to one or more Online Analytics Processing (OLAP) systems.

Batch processing intervals can be triggered by:

- When the source data reaches a certain size
- When an event of interest occurs (e.g., intruder alert)
- On-demand (e.g., web apps like music or video streaming services)

## Staging Areas

ETL pipelines are frequently used to integrate data from disparate and usually siloed systems within the enterprise. These systems can be from different vendors, locations, and divisions of the company.

![ETL Integration](./images/etl-integration.png)
*Figure 3: An ETL data integration pipeline concept for a Cost Accounting OLAP, fed by disparate OLTP systems within the enterprise. The staging area is used to manage change detection of new or modified data from the source systems, data updates, and any transformations required to conform and integrate the data prior to loading to the OLAP.*

## ETL Workflows as DAGs

ETL workflows can involve considerable complexity. By breaking down the workflow into individual tasks and dependencies, workflow orchestration tools such as Apache Airflow provide better control over complexity.

Airflow represents workflows as a directed acyclic graph (DAG). Airflow tasks can be expressed using predefined templates called operators, including:

- **Bash operators:** For running Bash code
- **Python operators:** For running Python code

![Airflow DAG](./images/airflow-dag.png)
*Figure 4: An Apache Airflow DAG representing a workflow. The green boxes represent individual tasks, while the arrows show dependencies between tasks.*

## Popular ETL Tools

Modern enterprise-grade ETL tools typically include:

- **Automation:** Fully automated pipelines
- **Ease of use:** ETL rule recommendations
- **Drag-and-drop interface:** "No-code" rules and data flows
- **Transformation support:** Assistance with complex calculations
- **Security and Compliance:** Data encryption and HIPAA, GDPR compliance

### Tool Comparison

| Tool | Key Features |
|------|--------------|
| **Talend Open Studio** | • Supports big data, data warehousing, and profiling<br>• Collaboration, monitoring, and scheduling<br>• Drag-and-drop GUI<br>• Auto-generates Java code<br>• Open-source |
| **AWS Glue** | • ETL service for analytics<br>• Schema suggestions<br>• Create jobs from AWS Console |
| **IBM InfoSphere DataStage** | • Data integration tool for ETL and ELT jobs<br>• Drag-and-drop graphical interface<br>• Parallel processing<br>• Enterprise connectivity |
| **Alteryx** | • Self-service data analytics platform<br>• Drag-and-drop accessibility<br>• No SQL or coding required |
| **Apache Airflow and Python** | • "Configuration as code" platform<br>• Open-sourced by Airbnb<br>• Programmatically author, schedule, and monitor<br>• Scales to Big Data<br>• Cloud platform integration |
| **Pandas Python Library** | • Versatile open-source programming tool<br>• Based on data frames<br>• Great for ETL, exploration, and prototyping<br>• Limited Big Data scalability |
