# Practice activity: Monitoring deployed models

## Introduction
Want to keep your machine learning models performing at their best even after deployment? Monitoring is essential for ensuring accuracy, reliability, and adaptability in production environments. 

This guide will walk you through the critical steps, best practices, and tools for monitoring deployed models, helping you to maintain long-term success.

By the end of this activity, you will be able to:

- Implement monitoring for a deployed machine learning model.
- Set up alerts and logs to track key metrics and detect potential issues.
- Understand how to use monitoring tools to maintain model performance.

## Step-by-step process for monitoring deployed models
Follow these steps to set up comprehensive monitoring for your deployed machine learning model. Each step will guide you through essential practices to ensure that your model remains accurate, reliable, and responsive in a production environment.

1. Choose the right monitoring tools.
2. Implement data quality checks.
3. Set up performance metrics monitoring.
4. Configure alerts for critical thresholds.
5. Detect model drift.
6. Log and troubleshoot.

### Step 1: Choose the right monitoring tools
Select tools based on your deployment environment and monitoring needs:

- **Azure Monitor**: provides comprehensive monitoring for models deployed in Azure environments, including integration and visualization.
- **Grafana**: an open-source tool for creating real-time dashboards to visualize metrics.
- **Prometheus**: suitable for collecting and storing metrics at both the model and system level.

### Step 2: Implement data quality checks
Run the following code in a Jupyter Notebook. Make sure to use the Python 3.8 - AzureML kernel and monitor incoming data quality to ensure consistency with training data:

```python
import numpy as np
import pandas as pd

# Sample incoming data
incoming_data = pd.DataFrame({'feature1': [1.4, 1.6, 1.8], 'feature2': [3.3, 3.9, 4.2]})

# Training data metrics
training_mean = {'feature1': 1.5, 'feature2': 3.7}
training_std = {'feature1': 0.2, 'feature2': 0.3}

# Calculate statistics for incoming data
incoming_mean = incoming_data.mean()

# Compare data to check for drift
for feature in incoming_data.columns:
    if abs(incoming_mean[feature] - training_mean[feature]) > training_std[feature] * 3:
  print(f"Alert: Significant data deviation detected in {feature}")
```

### Step 3: Set up performance metrics monitoring
One way to monitor performance metrics is by using Prometheus, an open-source cloud monitoring toolkit. 

Continuously track metrics such as accuracy, precision, recall, and F1 score:

```yaml
# Example Prometheus configuration to scrape model metrics
scrape_configs:
    - job_name: 'model_metrics'
        static_configs:
            - targets: ['localhost:9090']
```

### Step 4: Configure alerts for critical thresholds
You can also use Azure to configure alerts and monitoring. Read and understand (but don't run) this example code:

```python
from azure.monitor.query import MetricsQueryClient
from azure.identity import DefaultAzureCredential

credential = DefaultAzureCredential()
client = MetricsQueryClient(credential)

# Alert example
alert_condition = {
    "threshold": 85,
    "metric": "accuracy",
    "operator": "LessThan",
    "alert_action": "EmailNotification"
}

print("Alert condition set for accuracy below 85%.")
```

### Step 5: Detect model drift
Use statistical tests to monitor for model drift caused by changes in data distributions:

```python
from scipy.stats import ks_2samp

# Training data and incoming data samples
training_data_sample = np.random.normal(1.5, 0.2, 100)
incoming_data_sample = incoming_data['feature1']

# Perform KS test
d_stat, p_value = ks_2samp(training_data_sample, incoming_data_sample)
if p_value < 0.05:
    print("Model drift detected for feature1")
```

### Step 6: Log and troubleshoot
Log inputs, outputs, and errors to diagnose issues in production:

```python
import logging

# Configure logging settings
logging.basicConfig(filename='model_performance_logs.log', level=logging.INFO)

# Log prediction request and response
input_data = {'feature1': 1.6, 'feature2': 3.8}
output_prediction = {'prediction': 0.9}

logging.info(f"Input: {input_data}, Output: {output_prediction}")
```
Use Azure Log Analytics for centralized logging and detailed diagnostics.

## Conclusion
In this activity, you learned how to:

- Choose the right tools for monitoring deployed models.
- Implement data quality checks to ensure consistency.
- Track performance metrics continuously.
- Configure alerts to address critical thresholds.
- Detect model drift effectively.
- Utilize logging for troubleshooting and diagnostics.

Effective monitoring ensures that your models perform reliably in dynamic environments. By automating monitoring tasks and using scalable tools, you can maintain model performance and deliver consistent results.
