{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d474ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here‚Äôs what your (fixed) script does, step by step:\n",
    "\n",
    "1. Grab and prep the data\n",
    "\n",
    "* Downloads daily OHLCV data for AAPL from 2020-01-01 to 2024-01-01 with `yfinance`.\n",
    "* Uses only the **Close** column, reshaped to a 2-D array `(N, 1)`.\n",
    "* Scales prices to `[0, 1]` with `MinMaxScaler` (so the LSTM trains stably).\n",
    "\n",
    "2. Turn the series into supervised sequences\n",
    "\n",
    "* With `look_back = 60`, it builds sliding windows: for each day *t*, **X** contains the previous 60 scaled closes `[t-60 ‚Ä¶ t-1]`, and **y** is the scaled close at day *t*.\n",
    "* Shapes after building:\n",
    "\n",
    "  * `X` ‚Üí `(N-60, 60, 1)` (samples, timesteps, features)\n",
    "  * `y` ‚Üí `(N-60,)`\n",
    "\n",
    "3. Train / test split (time-ordered)\n",
    "\n",
    "* Splits the sequences 80/20 into `X_train, y_train` and `X_test, y_test`.\n",
    "* Because the split is done **after** sequence creation and `shuffle=False` during fit, there‚Äôs no temporal shuffling.\n",
    "\n",
    "4. Define the LSTM model\n",
    "\n",
    "* First LSTM layer: `return_sequences=True` so it outputs a sequence for each of the 60 steps (this lets you stack another LSTM).\n",
    "* Second LSTM layer: returns only the final hidden state (summary of the whole 60-day window).\n",
    "* `Dense(1)`: maps that state to a single number ‚Äî the **next day‚Äôs scaled close**.\n",
    "* Optimizer/loss: Adam + MSE for regression on a continuous target.\n",
    "\n",
    "5. Train the model\n",
    "\n",
    "* `model.fit(..., shuffle=False)` keeps chronological order for batches (safer for time series).\n",
    "\n",
    "6. Quick (toy) ‚Äúbacktest‚Äù\n",
    "\n",
    "* Picks one random sample from the test set, runs `model.predict(...)`, then **inverse-scales** both prediction and actual to real prices with the same scaler.\n",
    "* Prints something like: `Predicted: $x.xx, Actual: $y.yy`.\n",
    "\n",
    "What this model is really learning\n",
    "\n",
    "* It‚Äôs a **sequence-to-one regression**: *Given the last 60 closing prices, predict the next closing price.*\n",
    "* Inputs: just the close (no volume, no returns, no exogenous features).\n",
    "* Output: the raw price level (scaled), not the return ‚Äî simple but can be harder to learn than predicting deltas/returns.\n",
    "\n",
    "Common gotchas & easy improvements\n",
    "\n",
    "* **Data leakage:** you currently fit the scaler on *all* data. For a stricter evaluation, fit on **train only**, then transform the test set with that scaler.\n",
    "* **Evaluation:** selecting one random test point isn‚Äôt a real backtest. Better: generate predictions for **every** `X_test` sample, inverse-scale them, and compute MAE/RMSE; plot predicted vs actual.\n",
    "* **Targets:** consider predicting **returns** (or log returns) and reconstructing prices; often easier and more stationary.\n",
    "* **Features:** adding technicals (e.g., returns, moving averages, volatility) or multiple assets/market features can help.\n",
    "* **Walk-forward validation:** retrain periodically on expanding windows to mimic live conditions.\n",
    "\"\"\"\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# ----------------- Reproducibility -----------------\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "TICKERS    = ['NIFTYBEES.NS']  # edit your list\n",
    "START      = '2020-01-01'\n",
    "END        = '2025-08-29'\n",
    "LOOK_BACK  = 60\n",
    "TRAIN_FRAC = 0.8\n",
    "EPOCHS     = 10\n",
    "BATCH_SZ   = 32\n",
    "SHOW_PLOTS = True     # set False to skip plotting\n",
    "SAVE_PLOTS = False    # set True to save PNGs\n",
    "\n",
    "# Pre-compute END+1 calendar day for labeling the forecast\n",
    "END_DT = pd.to_datetime(END)\n",
    "NEXT_FORECAST_DATE = END_DT + pd.Timedelta(days=1)  # e.g., 2024-01-02\n",
    "\n",
    "# ----------------- Helpers -----------------\n",
    "def build_sequences(scaled_series: np.ndarray, look_back: int):\n",
    "    \"\"\"\n",
    "    scaled_series: shape (N, 1)\n",
    "    returns X: (N-look_back, look_back, 1), y: (N-look_back,)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(look_back, len(scaled_series)):\n",
    "        X.append(scaled_series[i - look_back:i, 0])\n",
    "        y.append(scaled_series[i, 0])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "    return X, y\n",
    "\n",
    "def make_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=input_shape),\n",
    "        LSTM(50),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "def mae(a, b):\n",
    "    return float(np.mean(np.abs(a - b)))\n",
    "\n",
    "def rmse(a, b):\n",
    "    return float(np.sqrt(np.mean((a - b) ** 2)))\n",
    "\n",
    "# ----------------- Main -----------------\n",
    "results = []\n",
    "\n",
    "for ticker in TICKERS:\n",
    "    print(f\"\\n=== {ticker} ===\")\n",
    "    df = yf.download(ticker, start=START, end=END, progress=False)\n",
    "    if df.empty or 'Close' not in df.columns:\n",
    "        print(\"No data; skipping.\")\n",
    "        continue\n",
    "\n",
    "    closes = df['Close'].astype(float).values.reshape(-1, 1)\n",
    "    N = len(closes)\n",
    "    if N <= LOOK_BACK + 1:\n",
    "        print(f\"Not enough data after LOOK_BACK={LOOK_BACK}; skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Time-ordered split on raw data (prices)\n",
    "    split_idx = int(TRAIN_FRAC * N)\n",
    "    train_raw = closes[:split_idx]\n",
    "    full_raw  = closes  # keep full series for transform/predictions\n",
    "\n",
    "    # Fit scaler ONLY on train (avoid leakage)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(train_raw)\n",
    "    scaled_all = scaler.transform(full_raw)\n",
    "\n",
    "    # Build sequences over the FULL scaled series; then split by sequence count\n",
    "    X_all, y_all = build_sequences(scaled_all, LOOK_BACK)\n",
    "    M = len(X_all)\n",
    "    if M == 0:\n",
    "        print(\"Sequence build produced no samples; skipping.\")\n",
    "        continue\n",
    "\n",
    "    seq_split = int(TRAIN_FRAC * M)\n",
    "    X_train, y_train = X_all[:seq_split], y_all[:seq_split]\n",
    "    X_test,  y_test  = X_all[seq_split:], y_all[seq_split:]\n",
    "\n",
    "    if len(X_test) == 0:\n",
    "        print(\"Empty test set; reduce TRAIN_FRAC or extend date range. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Dates aligned to each target y\n",
    "    y_dates = df.index[LOOK_BACK:]          # length M\n",
    "    test_dates = y_dates[seq_split:]        # aligns with X_test / y_test\n",
    "\n",
    "    # Build & train model\n",
    "    model = make_model(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SZ, shuffle=False, verbose=1)\n",
    "\n",
    "    # Predict full test set\n",
    "    y_pred_scaled = model.predict(X_test, verbose=0)         # (n_test, 1)\n",
    "    y_true_scaled = y_test.reshape(-1, 1)\n",
    "\n",
    "    # Inverse scale to prices\n",
    "    y_pred_price = scaler.inverse_transform(y_pred_scaled).reshape(-1)\n",
    "    y_true_price = scaler.inverse_transform(y_true_scaled).reshape(-1)\n",
    "\n",
    "    # Metrics\n",
    "    m_mae  = mae(y_true_price, y_pred_price)\n",
    "    m_rmse = rmse(y_true_price, y_pred_price)\n",
    "    print(f\"Test MAE:  {m_mae:.4f}\")\n",
    "    print(f\"Test RMSE: {m_rmse:.4f}\")\n",
    "\n",
    "    # --------- Next-day (t+1) forecast labeled as END + 1 day ----------\n",
    "    # Use the LAST look_back window from the FULL series (scaled by train-scaler)\n",
    "    last_window = scaled_all[-LOOK_BACK:, 0].reshape(1, LOOK_BACK, 1)\n",
    "    next_scaled = model.predict(last_window, verbose=0)       # (1,1)\n",
    "    next_price  = scaler.inverse_transform(next_scaled.reshape(-1, 1))[0, 0]\n",
    "\n",
    "    print(f\"Next-day forecast (for {NEXT_FORECAST_DATE.date()}): ${next_price:.2f}\")\n",
    "\n",
    "    # --------- Plot Actual vs Predicted on Test Set ----------\n",
    "    if SHOW_PLOTS:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(test_dates, y_true_price, label='Actual')\n",
    "        plt.plot(test_dates, y_pred_price, label='Predicted')\n",
    "        plt.title(f'{ticker} ‚Äî Test Set: Actual vs Predicted Close')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Price')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        if SAVE_PLOTS:\n",
    "            fname = f'{ticker}_test_pred_vs_actual.png'\n",
    "            plt.savefig(fname, dpi=150)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    # Collect per-ticker outputs (optional)\n",
    "    out_df = pd.DataFrame({\n",
    "        'Date': test_dates,\n",
    "        'Actual': y_true_price,\n",
    "        'Predicted': y_pred_price\n",
    "    }).set_index('Date')\n",
    "\n",
    "    results.append({\n",
    "        'ticker': ticker,\n",
    "        'mae': m_mae,\n",
    "        'rmse': m_rmse,\n",
    "        'next_date': NEXT_FORECAST_DATE,\n",
    "        'next_price': next_price,\n",
    "        'pred_df': out_df\n",
    "    })\n",
    "\n",
    "# ----------------- Summary print -----------------\n",
    "print(\"\\n=== Summary ===\")\n",
    "for r in results:\n",
    "    print(f\"{r['ticker']}: MAE={r['mae']:.4f}, RMSE={r['rmse']:.4f}, Next({r['next_date'].date()})-> ${r['next_price']:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d25c20ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Tuning LOOK_BACK for INFY.NS over (30, 60, 90, 120, 180) ...\n",
      "[INFY.NS] LB= 30  val_loss=0.000291  test_RMSE=23.6727  (n_test=577)\n",
      "[INFY.NS] LB= 60  val_loss=0.000291  test_RMSE=23.3814  (n_test=577)\n",
      "[INFY.NS] LB= 90  val_loss=0.000312  test_RMSE=23.5633  (n_test=577)\n",
      "[INFY.NS] LB=120  val_loss=0.000327  test_RMSE=24.3285  (n_test=577)\n",
      "[INFY.NS] LB=180  val_loss=0.000304  test_RMSE=23.6719  (n_test=577)\n",
      "Chosen LOOK_BACK for INFY.NS: 60 (val_loss=0.000291 test_RMSE=23.3814)\n",
      "\n",
      "=== INFY.NS | LOOK_BACK=60 ===\n",
      "Epoch 1/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - loss: 0.0069 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0027 - val_loss: 4.6738e-04 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.0015 - val_loss: 7.4542e-04 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.0014 - val_loss: 4.0601e-04 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.0016 - val_loss: 3.4135e-04 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 8.9327e-04 - val_loss: 3.3834e-04 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 6.7685e-04 - val_loss: 3.1294e-04 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 6.0965e-04 - val_loss: 3.3989e-04 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 5.9153e-04 - val_loss: 3.1155e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 5.3542e-04 - val_loss: 3.0995e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 5.3594e-04 - val_loss: 3.0608e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 4.9670e-04 - val_loss: 3.0944e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 5.1682e-04 - val_loss: 3.0439e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 4.8675e-04 - val_loss: 3.1442e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 5.0476e-04 - val_loss: 3.0402e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 4.8410e-04 - val_loss: 3.0931e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 5.0183e-04 - val_loss: 3.0484e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 4.7742e-04 - val_loss: 3.0989e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 4.8655e-04 - val_loss: 3.0482e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 4.6460e-04 - val_loss: 3.0226e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 4.7735e-04 - val_loss: 3.0326e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 4.9148e-04 - val_loss: 3.0416e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 4.6633e-04 - val_loss: 3.0611e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 4.9066e-04 - val_loss: 3.0435e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 4.7650e-04 - val_loss: 3.0440e-04 - learning_rate: 1.5625e-05\n",
      "Test MAE=17.0776  RMSE=24.0737  MAPE=1.10%  sMAPE=1.10%  DirAcc=52.17%  n_test=577\n",
      "Next-day forecast (for 2025-08-29): 1521.48\n",
      "\n",
      "=== Summary ===\n",
      "INFY.NS | LB=60 | MAE=17.0776 RMSE=24.0737 MAPE=1.10% sMAPE=1.10% DirAcc=52.17% n_test=577 | Next(2025-08-29) -> 1521.48\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "# ----------------- Reproducibility -----------------\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "TICKERS      = ['INFY.NS']          # put your list here\n",
    "START        = '2010-01-01'              # use more years to \"train on larger dataset\"\n",
    "END          = '2025-08-28'\n",
    "TRAIN_FRAC   = 0.85                       # larger train split if you have more data\n",
    "VAL_FRAC     = 0.12                       # fraction of TRAIN rows (from the tail)\n",
    "EPOCHS       = 50\n",
    "BATCH_SZ     = 128\n",
    "USE_BUSINESS_DAY = True                   # False -> calendar day (END+1), True -> next business day\n",
    "SHOW_PLOTS   = False\n",
    "SAVE_PLOTS   = False\n",
    "\n",
    "# Auto-tuning space for LOOK_BACK (window size the LSTM sees)\n",
    "TUNE_LOOK_BACKS = (30, 60, 90, 120, 180)\n",
    "\n",
    "END_DT = pd.to_datetime(END)\n",
    "FORECAST_DATE = END_DT + (BDay(1) if USE_BUSINESS_DAY else pd.Timedelta(days=1))\n",
    "\n",
    "# ----------------- Small helpers -----------------\n",
    "def tofloat(x):\n",
    "    \"\"\"Safely convert numpy arrays / scalars to Python float.\"\"\"\n",
    "    return float(np.asarray(x).reshape(-1)[0])\n",
    "\n",
    "def mae(a, b): return float(np.mean(np.abs(a - b)))\n",
    "def rmse(a, b): return float(np.sqrt(np.mean((a - b) ** 2)))\n",
    "def mape(a, b): return float(100 * np.mean(np.abs((a - b) / (a + 1e-9))))\n",
    "def smape(a, b): return float(100 * np.mean(2 * np.abs(b - a) / (np.abs(a) + np.abs(b) + 1e-9)))\n",
    "\n",
    "# ----------------- Feature engineering -----------------\n",
    "def compute_rsi(close: pd.Series, period: int = 14) -> pd.Series:\n",
    "    delta = close.diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = (-delta).clip(lower=0)\n",
    "    roll_up = up.rolling(period, min_periods=period).mean()\n",
    "    roll_down = down.rolling(period, min_periods=period).mean()\n",
    "    rs = roll_up / (roll_down + 1e-9)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out['logret'] = np.log(out['Close']).diff()                      # t return\n",
    "    out['ma5_gap']  = out['Close'].rolling(5).mean() / out['Close'] - 1\n",
    "    out['ma10_gap'] = out['Close'].rolling(10).mean() / out['Close'] - 1\n",
    "    out['ma20_gap'] = out['Close'].rolling(20).mean() / out['Close'] - 1\n",
    "    out['rsi14']    = compute_rsi(out['Close'], 14) / 100.0          # scale to 0..1\n",
    "    out['vol20']    = out['logret'].rolling(20).std()                # rolling volatility\n",
    "    out['logvol']   = np.log1p(out['Volume'])\n",
    "    out['volz20']   = (out['logvol'] - out['logvol'].rolling(20).mean()) / (out['logvol'].rolling(20).std() + 1e-9)\n",
    "    # Next-day target: log return at t+1\n",
    "    out['y'] = out['logret'].shift(-1)\n",
    "    return out\n",
    "\n",
    "# ----------------- Sequence builder -----------------\n",
    "def build_sequences(feat_mat: np.ndarray,\n",
    "                    y_vec: np.ndarray,\n",
    "                    closes_vec: np.ndarray,\n",
    "                    dates_idx: pd.DatetimeIndex,\n",
    "                    look_back: int):\n",
    "    \"\"\"\n",
    "    Build samples ending at time t with target = return at t+1.\n",
    "    For a sample ending at index i-1 (sequence uses rows [i-look_back, i)):\n",
    "      y uses y_vec[i-1] (equals logret at i)\n",
    "      prev_close = closes_vec[i-1] (price at t)\n",
    "      target_date = dates_idx[i] (date of t+1)\n",
    "    \"\"\"\n",
    "    X, y, prev_close, target_dates = [], [], [], []\n",
    "    N = len(feat_mat)\n",
    "    for i in range(look_back, N):\n",
    "        X.append(feat_mat[i - look_back:i, :])\n",
    "        y.append(y_vec[i - 1])\n",
    "        prev_close.append(closes_vec[i - 1])\n",
    "        target_dates.append(dates_idx[i])\n",
    "    return np.array(X), np.array(y), np.array(prev_close), pd.to_datetime(target_dates)\n",
    "\n",
    "# ----------------- Model -----------------\n",
    "def make_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        LSTM(64, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# ----------------- Tuner -----------------\n",
    "def tune_look_back(ticker, look_backs=TUNE_LOOK_BACKS):\n",
    "    \"\"\"\n",
    "    Try multiple LOOK_BACK values and pick the best by lowest validation loss\n",
    "    (tie-broken by lowest test RMSE on prices).\n",
    "    Returns: dict with keys {lb, val_loss, test_rmse}\n",
    "    \"\"\"\n",
    "    best = {\"lb\": None, \"val_loss\": np.inf, \"test_rmse\": np.inf}\n",
    "\n",
    "    # Load data\n",
    "    df = yf.download(ticker, start=START, end=END, progress=False, auto_adjust=True)\n",
    "    if df.empty or 'Close' not in df.columns:\n",
    "        print(f\"[{ticker}] No data. Skipping tune.\")\n",
    "        return best\n",
    "\n",
    "    f = add_features(df)\n",
    "    cols = ['logret','ma5_gap','ma10_gap','ma20_gap','rsi14','vol20','volz20','y','Close']\n",
    "    f = f[cols].dropna().copy()\n",
    "    if len(f) < 200:\n",
    "        print(f\"[{ticker}] Too few rows after features: {len(f)}. Skipping tune.\")\n",
    "        return best\n",
    "\n",
    "    n_rows = len(f)\n",
    "    row_split = int(TRAIN_FRAC * n_rows)\n",
    "    val_rows  = int(VAL_FRAC * row_split)\n",
    "    train_rows = row_split - val_rows\n",
    "\n",
    "    feat_cols = ['logret','ma5_gap','ma10_gap','ma20_gap','rsi14','vol20','volz20']\n",
    "    feat_raw  = f[feat_cols].values\n",
    "    y_raw     = f['y'].values\n",
    "    closes    = f['Close'].values.astype(float)\n",
    "    dates     = f.index\n",
    "\n",
    "    # Fit scaler on TRAIN+VAL (no test leakage)\n",
    "    scalerX = StandardScaler().fit(feat_raw[:row_split])\n",
    "    feat_scaled_all = scalerX.transform(feat_raw)\n",
    "\n",
    "    for LB in look_backs:\n",
    "        if train_rows <= LB + 5:\n",
    "            print(f\"[{ticker}] LB={LB:>3} -> train too small. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        X_all, y_all, prev_close_all, _ = build_sequences(\n",
    "            feat_scaled_all, y_raw, closes, dates, LB\n",
    "        )\n",
    "\n",
    "        # Sequence splits so windows don't cross folds\n",
    "        seq_split = max(1, row_split - LB)         # start of TEST sequences\n",
    "        seq_val_split = max(1, train_rows - LB)    # start of VAL sequences\n",
    "\n",
    "        if seq_split <= 1 or seq_val_split <= 1 or len(X_all[seq_split:]) == 0:\n",
    "            print(f\"[{ticker}] LB={LB:>3} -> empty test/val. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        X_train,  y_train  = X_all[:seq_val_split], y_all[:seq_val_split]\n",
    "        X_val,    y_val    = X_all[seq_val_split:seq_split], y_all[seq_val_split:seq_split]\n",
    "        X_test,   y_test   = X_all[seq_split:], y_all[seq_split:]\n",
    "        prev_test = prev_close_all[seq_split:]\n",
    "\n",
    "        model = make_model((X_train.shape[1], X_train.shape[2]))\n",
    "        cb = [\n",
    "            EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5),\n",
    "        ]\n",
    "        hist = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val) if len(X_val) > 0 else None,\n",
    "            epochs=EPOCHS, batch_size=BATCH_SZ, shuffle=False, verbose=0, callbacks=cb\n",
    "        )\n",
    "\n",
    "        # Lowest val_loss seen (fallback to loss if val missing)\n",
    "        vloss_hist = hist.history.get('val_loss', None)\n",
    "        val_loss = np.min(vloss_hist) if vloss_hist else np.min(hist.history['loss'])\n",
    "\n",
    "        # Test RMSE on prices\n",
    "        y_pred_ret = model.predict(X_test, verbose=0).reshape(-1)\n",
    "        y_true_ret = y_test.reshape(-1)\n",
    "        y_pred_price = prev_test * np.exp(y_pred_ret)\n",
    "        y_true_price = prev_test * np.exp(y_true_ret)\n",
    "        test_rmse = rmse(y_true_price, y_pred_price)\n",
    "\n",
    "        print(f\"[{ticker}] LB={LB:>3}  val_loss={val_loss:.6f}  test_RMSE={test_rmse:.4f}  (n_test={len(y_true_price)})\")\n",
    "\n",
    "        better = (val_loss < best[\"val_loss\"]) or \\\n",
    "                 (np.isclose(val_loss, best[\"val_loss\"]) and test_rmse < best[\"test_rmse\"])\n",
    "        if better:\n",
    "            best.update({\"lb\": LB, \"val_loss\": float(val_loss), \"test_rmse\": float(test_rmse)})\n",
    "\n",
    "    return best\n",
    "\n",
    "# ----------------- Train/Eval/Forecast with chosen LOOK_BACK -----------------\n",
    "def train_and_forecast(ticker, look_back):\n",
    "    print(f\"\\n=== {ticker} | LOOK_BACK={look_back} ===\")\n",
    "    df = yf.download(ticker, start=START, end=END, progress=False, auto_adjust=True)\n",
    "    if df.empty or 'Close' not in df.columns:\n",
    "        print(\"No data; skipping.\")\n",
    "        return None\n",
    "\n",
    "    f = add_features(df)\n",
    "    needed = ['logret','ma5_gap','ma10_gap','ma20_gap','rsi14','vol20','volz20','y','Close']\n",
    "    f = f[needed].dropna().copy()\n",
    "    if len(f) <= look_back + 10:\n",
    "        print(\"Not enough rows after features; skipping.\")\n",
    "        return None\n",
    "\n",
    "    n_rows = len(f)\n",
    "    row_split = int(TRAIN_FRAC * n_rows)\n",
    "    val_rows  = int(VAL_FRAC * row_split)\n",
    "    train_rows = row_split - val_rows\n",
    "    if train_rows <= look_back:\n",
    "        print(\"Training span too small given LOOK_BACK; adjust splits.\")\n",
    "        return None\n",
    "\n",
    "    feat_cols = ['logret','ma5_gap','ma10_gap','ma20_gap','rsi14','vol20','volz20']\n",
    "    feat_raw  = f[feat_cols].values\n",
    "    y_raw     = f['y'].values\n",
    "    closes    = f['Close'].values.astype(float)\n",
    "    dates     = f.index\n",
    "\n",
    "    scalerX = StandardScaler().fit(feat_raw[:row_split])     # train+val only\n",
    "    feat_scaled_all = scalerX.transform(feat_raw)\n",
    "\n",
    "    X_all, y_all, prev_close_all, target_dates_all = build_sequences(\n",
    "        feat_scaled_all, y_raw, closes, dates, look_back\n",
    "    )\n",
    "\n",
    "    seq_split = max(1, row_split - look_back)\n",
    "    seq_val_split = max(1, train_rows - look_back)\n",
    "\n",
    "    X_train,  y_train  = X_all[:seq_val_split], y_all[:seq_val_split]\n",
    "    X_val,    y_val    = X_all[seq_val_split:seq_split], y_all[seq_val_split:seq_split]\n",
    "    X_test,   y_test   = X_all[seq_split:], y_all[seq_split:]\n",
    "    prev_test = prev_close_all[seq_split:]\n",
    "    dates_test = target_dates_all[seq_split:]\n",
    "\n",
    "    if len(X_test) == 0:\n",
    "        print(\"Empty test set; adjust dates/splits.\")\n",
    "        return None\n",
    "\n",
    "    # Build & train\n",
    "    model = make_model((X_train.shape[1], X_train.shape[2]))\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5)\n",
    "    ]\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val) if len(X_val) > 0 else None,\n",
    "        epochs=EPOCHS, batch_size=BATCH_SZ, shuffle=False, verbose=1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Predict returns on test, convert to prices\n",
    "    y_pred_ret = model.predict(X_test, verbose=0).reshape(-1)\n",
    "    y_true_ret = y_test.reshape(-1)\n",
    "    y_pred_price = prev_test * np.exp(y_pred_ret)\n",
    "    y_true_price = prev_test * np.exp(y_true_ret)\n",
    "\n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        \"mae\": mae(y_true_price, y_pred_price),\n",
    "        \"rmse\": rmse(y_true_price, y_pred_price),\n",
    "        \"mape\": mape(y_true_price, y_pred_price),\n",
    "        \"smape\": smape(y_true_price, y_pred_price),\n",
    "        \"dir_acc\": float(np.mean(np.sign(y_true_ret) == np.sign(y_pred_ret))),\n",
    "        \"n_test\": int(len(y_true_price))\n",
    "    }\n",
    "    print(f\"Test MAE={metrics['mae']:.4f}  RMSE={metrics['rmse']:.4f}  \"\n",
    "          f\"MAPE={metrics['mape']:.2f}%  sMAPE={metrics['smape']:.2f}%  \"\n",
    "          f\"DirAcc={metrics['dir_acc']*100:.2f}%  n_test={metrics['n_test']}\")\n",
    "\n",
    "    # Next-day (END+1) forecast\n",
    "    last_feat_rows = feat_scaled_all[-look_back:, :]\n",
    "    last_close     = tofloat(closes[-1])\n",
    "    next_ret       = tofloat(model.predict(last_feat_rows.reshape(1, look_back, len(feat_cols)), verbose=0))\n",
    "    next_price     = tofloat(last_close * np.exp(next_ret))\n",
    "    print(f\"Next-day forecast (for {FORECAST_DATE.date()}): {next_price:.2f}\")\n",
    "\n",
    "    # Plot if enabled\n",
    "    if SHOW_PLOTS:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(dates_test, y_true_price, label='Actual Close (Test)')\n",
    "        plt.plot(dates_test, y_pred_price, label='Predicted Close')\n",
    "        plt.title(f'{ticker} ‚Äî Test Set: Actual vs Predicted Close (LB={look_back})')\n",
    "        plt.xlabel('Date'); plt.ylabel('Price'); plt.legend(); plt.tight_layout()\n",
    "        if SAVE_PLOTS: plt.savefig(f'{ticker}_pred_vs_actual_LB{look_back}.png', dpi=150)\n",
    "        plt.show(); plt.close()\n",
    "\n",
    "    return {\n",
    "        \"ticker\": ticker,\n",
    "        \"look_back\": int(look_back),\n",
    "        \"metrics\": metrics,\n",
    "        \"forecast_date\": FORECAST_DATE,\n",
    "        \"next_price\": next_price\n",
    "    }\n",
    "\n",
    "# ----------------- Main -----------------\n",
    "all_results = []\n",
    "for tk in TICKERS:\n",
    "    print(f\"\\n>>> Tuning LOOK_BACK for {tk} over {TUNE_LOOK_BACKS} ...\")\n",
    "    best = tune_look_back(tk, TUNE_LOOK_BACKS)\n",
    "    chosen_lb = best[\"lb\"] if best[\"lb\"] is not None else 60\n",
    "    print(f\"Chosen LOOK_BACK for {tk}: {chosen_lb} \"\n",
    "          f\"(val_loss={best['val_loss']:.6f} test_RMSE={best['test_rmse']:.4f})\" if best[\"lb\"] else\n",
    "          f\"No valid LB found; falling back to {chosen_lb}\")\n",
    "\n",
    "    res = train_and_forecast(tk, chosen_lb)\n",
    "    if res: all_results.append(res)\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "for r in all_results:\n",
    "    m = r[\"metrics\"]\n",
    "    print(f\"{r['ticker']} | LB={r['look_back']} | \"\n",
    "          f\"MAE={m['mae']:.4f} RMSE={m['rmse']:.4f} MAPE={m['mape']:.2f}% sMAPE={m['smape']:.2f}% \"\n",
    "          f\"DirAcc={m['dir_acc']*100:.2f}% n_test={m['n_test']} | \"\n",
    "          f\"Next({r['forecast_date'].date()}) -> {r['next_price']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c276b48",
   "metadata": {},
   "source": [
    "Absolutely ‚Äî here‚Äôs what your script does, step by step, and why each part exists. I‚Äôll keep the order exactly as the code flows.\n",
    "\n",
    "---\n",
    "\n",
    "# High-level overview\n",
    "\n",
    "You‚Äôre building a **next-day return** predictor for a single stock using:\n",
    "1. TA-Lib + pandas to create technical features (shifted to avoid leakage)\n",
    "\t‚Ä¢\tThey use TA-Lib (a technical analysis library) and pandas to compute indicators like SMA, EMA, RSI, Bollinger Bands, MACD, etc.\n",
    "\t‚Ä¢\tFeatures are shifted so that the model doesn‚Äôt ‚Äúpeek into the future‚Äù (data leakage).\n",
    "Example: Instead of using today‚Äôs RSI to predict today‚Äôs return, you shift RSI(ùë°‚àí1) to predict return(ùë°).\n",
    "\n",
    "‚∏ª\n",
    "\n",
    "2. Volatility-scaled target to stabilize variance\n",
    "\t‚Ä¢\tInstead of predicting raw returns, they divide returns by recent volatility (like ATR or rolling std).\n",
    "\t‚Ä¢\tThis makes the target more stable, because raw returns are noisy and heteroscedastic (variance changes over time).\n",
    "\t‚Ä¢\tIt helps the model focus on risk-adjusted returns rather than absolute size.\n",
    "\n",
    "‚∏ª\n",
    "\n",
    "3. Walk-forward (time-series) cross-validation with a small purge gap\n",
    "\t‚Ä¢\tStandard k-fold CV doesn‚Äôt work in time-series (leaks future info).\n",
    "\t‚Ä¢\tSo they use walk-forward CV:\n",
    "\t‚Ä¢\tTrain on 2010‚Äì2015, test on 2016\n",
    "\t‚Ä¢\tTrain on 2010‚Äì2016, test on 2017\n",
    "\t‚Ä¢\t‚Ä¶ and so on.\n",
    "\t‚Ä¢\tA purge gap is added so that overlapping data near the split (which can leak info due to autocorrelation) is excluded.\n",
    "\n",
    "‚∏ª\n",
    "\n",
    "4. An ensemble of base models (ElasticNet, RandomForest, optional XGBoost/LightGBM)\n",
    "\t‚Ä¢\tThey don‚Äôt rely on a single model.\n",
    "\t‚Ä¢\tBase models:\n",
    "\t‚Ä¢\tElasticNet ‚Üí good for linear trends + feature selection.\n",
    "\t‚Ä¢\tRandomForest ‚Üí good for nonlinear patterns.\n",
    "\t‚Ä¢\tXGBoost/LightGBM ‚Üí gradient boosting, powerful on tabular data.\n",
    "\n",
    "‚∏ª\n",
    "\n",
    "5. Stacking (a Ridge meta-model learns how to mix base models)\n",
    "\t‚Ä¢\tAfter training base models, their predictions (meta-features) are combined.\n",
    "\t‚Ä¢\tA Ridge Regression (meta-model) learns the best weights for each base model.\n",
    "\t‚Ä¢\tExample: maybe RF is best in volatile markets, ElasticNet in trending ones.\n",
    "\t‚Ä¢\tThis stacking often outperforms single models.\n",
    "\n",
    "‚∏ª\n",
    "\n",
    "6. A quick OOF backtest (long-only with costs) for sanity\n",
    "\t‚Ä¢\tOOF = Out-Of-Fold predictions (predictions on unseen test folds during CV).\n",
    "\t‚Ä¢\tThey run a simple trading backtest:\n",
    "\t‚Ä¢\tLong if prediction > 0 (model expects positive return).\n",
    "\t‚Ä¢\tFlat otherwise.\n",
    "\t‚Ä¢\tDeduct transaction costs.\n",
    "\t‚Ä¢\tThis is just a sanity check to see if the model has any predictive power.\n",
    "\n",
    "The script prints:\n",
    "\n",
    "* Each base model‚Äôs OOF RMSE,\n",
    "* The stacker‚Äôs OOF RMSE,\n",
    "* A **single next-day forecast** (return and implied price),\n",
    "* Simple OOF backtest stats (total return, hit rate, Sharpe).\n",
    "\n",
    "---\n",
    "\n",
    "# Detailed walkthrough\n",
    "\n",
    "## 1) CONFIG\n",
    "\n",
    "```python\n",
    "TICKER=\"INFY.NS\", START_DATE, END_DATE, SPLITS, GAP_DAYS, USE_VOL_SCALE, FEE_RT, THRESH_LONG\n",
    "```\n",
    "\n",
    "* Controls symbol & date range (**yfinance** daily bars).\n",
    "* `SPLITS`: number of time-series CV folds.\n",
    "* `GAP_DAYS`: drops the first `gap` rows of each validation fold to **purge overlap** (reduces label leakage from overlapping windows).\n",
    "* `USE_VOL_SCALE=True`: model `y_scaled = next_return / prev_vol20`. This stabilizes the target; later you rescale the prediction by the last known `vol20`.\n",
    "* `FEE_RT`: approximate round-turn cost used in the backtest (adjust to your broker).\n",
    "* `THRESH_LONG`: only take a trade if predicted next-day return exceeds this (filters noise).\n",
    "\n",
    "## 2) IMPORTS\n",
    "\n",
    "* `pandas`, `numpy`, `yfinance` for data.\n",
    "* `sklearn` for models, scaling, splitting, metrics.\n",
    "* Optional: `xgboost`, `lightgbm` if installed.\n",
    "\n",
    "## 3) HELPERS\n",
    "\n",
    "### `_as_float1d(x)`\n",
    "\n",
    "TA-Lib needs **1-D contiguous float64 arrays**. This coerces Series/DataFrame to `np.float64` 1-D and contiguous. Prevents the ‚Äúwrong dimensions‚Äù/dtype errors you hit earlier.\n",
    "\n",
    "### `_select_single_ticker_ohlcv(df, ticker)`\n",
    "\n",
    "yfinance sometimes returns **MultiIndex columns** (e.g., multiple tickers). This picks the one ticker you want regardless of whether the layout is `(ticker -> OHLCV)` or `(OHLCV -> ticker)`.\n",
    "It also:\n",
    "\n",
    "* Falls back to `Adj Close` if `Close` isn‚Äôt present,\n",
    "* Ensures you end with exactly `Open, High, Low, Close, Volume`,\n",
    "* Coerces to numeric and drops bad rows,\n",
    "* Returns a clean, single-ticker OHLCV DataFrame.\n",
    "\n",
    "## 4) DOWNLOAD + SHAPE\n",
    "\n",
    "```python\n",
    "raw = yf.download(...)\n",
    "df  = _select_single_ticker_ohlcv(raw, TICKER)\n",
    "```\n",
    "\n",
    "* Pulls daily OHLCV.\n",
    "* Ensures you have at least \\~600 rows (so CV has enough data).\n",
    "* Creates shorthands: `cl, hi, lo, vo` and daily returns `ret = cl.pct_change()`.\n",
    "\n",
    "## 5) TA-LIB SETUP\n",
    "\n",
    "```python\n",
    "try: import talib as ta; HAS_TA=True\n",
    "except: HAS_TA=False\n",
    "```\n",
    "\n",
    "* If TA-Lib isn‚Äôt installed, the script uses **pandas fallbacks** for indicators.\n",
    "\n",
    "## 6) INDICATOR WRAPPERS\n",
    "\n",
    "* `rsi`, `macd`, `stoch`, `bbands`, `adx`, `atr`, `mfi`, `cci`, `willr`, `obv`\n",
    "* Each function:\n",
    "\n",
    "  * If `HAS_TA`, calls the TA-Lib function with `_as_float1d(...)`.\n",
    "  * Else computes a close approximation in pandas.\n",
    "* All feature outputs are aligned to the input index.\n",
    "\n",
    "**Key point:** later you call `.shift(1)` on almost all features to **avoid look-ahead** (use only information known at the close of day *t* to predict day *t+1*).\n",
    "\n",
    "## 7) FEATURES (no leakage)\n",
    "\n",
    "```python\n",
    "rsi14 = rsi(cl, 14).shift(1)\n",
    "macd_line, macd_sig, macd_hist = macd(cl); then shift(1)\n",
    "stoch_k, stoch_d = stoch(...); shift(1)\n",
    "bb_up, bb_mid, bb_low = bbands(cl); shift(1)\n",
    "bb_width = (bb_up - bb_low)/bb_mid  # width; already shifted via inputs\n",
    "...\n",
    "obv_val = obv(cl, vo).pct_change().shift(1)\n",
    "```\n",
    "\n",
    "* You compute a diverse set: momentum (RSI/MACD/Stoch), volatility (ATR, BB width), trend/strength (ADX, CCI, Williams %R), volume (MFI, OBV ROC), lag/rolling stats (recent returns, mean & std), and weekday dummies.\n",
    "* Everything is `shift(1)` or derived from shifted values, so **all features are known at prediction time**.\n",
    "\n",
    "`X` is then assembled from these features and infinities are replaced with NaN (to be dropped later).\n",
    "\n",
    "## 8) TARGET (next-day return) + volatility scaling\n",
    "\n",
    "```python\n",
    "y_next = ret.shift(-1)           # tomorrow‚Äôs return\n",
    "vol20  = ret.rolling(20).std()   # rolling daily vol\n",
    "y      = y_next / vol20.shift(1) # if USE_VOL_SCALE\n",
    "```\n",
    "\n",
    "* **Raw target** is next day‚Äôs simple return.\n",
    "* If `USE_VOL_SCALE`, you **stabilize** the target by dividing by the **last known** 20-day volatility (shifted by 1).\n",
    "  This reduces heteroscedasticity (wide swings in variance), often improving fit.\n",
    "* Finally, `data = concat(X, y, y_next).dropna()` ensures rows have complete features & target.\n",
    "\n",
    "## 9) TIME-SERIES CV + BASE MODELS\n",
    "\n",
    "### `ts_purged_splits(...)`\n",
    "\n",
    "* Wraps `TimeSeriesSplit(n_splits=SPLITS)` but **drops the first `gap` rows of each test fold** to reduce information bleed from overlapping windows (a form of leakage).\n",
    "\n",
    "### Base models\n",
    "\n",
    "```python\n",
    "(\"enet\", ElasticNet(...)), (\"rf\", RandomForestRegressor(...)), (\"xgb\", ...?), (\"lgbm\", ...?)\n",
    "```\n",
    "\n",
    "* **ElasticNet** (linear, L1+L2): good for mostly linear relationships and mild feature selection.\n",
    "* **RandomForest**: nonlinear, robust to outliers; captures interactions.\n",
    "* **XGB/LGBM** (if installed): gradient boosted trees; strong tabular baselines.\n",
    "\n",
    "**OOF predictions:**\n",
    "\n",
    "* For each fold:\n",
    "\n",
    "  * Fit models on the training slice.\n",
    "  * Predict the validation slice.\n",
    "  * Store predictions into `oof_preds[name][te]`.\n",
    "  * Store ground truth into `oof_truth[te]`.\n",
    "\n",
    "**Why OOF?** Out-of-fold predictions simulate ‚Äúunseen‚Äù data and are needed to **train the stacker without leakage**.\n",
    "\n",
    "### OOF RMSE printout\n",
    "\n",
    "```python\n",
    "rmse = RMSE(oof_truth[m], oof_preds[name][m])\n",
    "```\n",
    "\n",
    "* Your sklearn version didn‚Äôt support `squared=False`, so you use a small `RMSE()` wrapper.\n",
    "* This gives **walk-forward** error per base model on the (scaled) target `y`.\n",
    "\n",
    "## 10) STACKED META-MODEL\n",
    "\n",
    "```python\n",
    "Z = DataFrame({name: oof_preds[name]}, index=X.index)\n",
    "mask_all_series = Series(~np.isnan(oof_truth), index=X.index)\n",
    "mask = Z.notna().all(axis=1) & mask_all_series\n",
    "meta = Ridge(alpha=1.0)\n",
    "meta.fit(Z.loc[mask].values, oof_truth[mask])\n",
    "```\n",
    "\n",
    "* `Z` collects the base models‚Äô OOF predictions as columns; **indexed by X.index** to align perfectly.\n",
    "* `mask` selects rows where **all** base OOF preds exist **and** the truth exists.\n",
    "* `Ridge` is trained on these OOF predictions to learn **the optimal blend** (stacking).\n",
    "* You print the stacker‚Äôs OOF RMSE ‚Äî this is usually **lower** than individual base models.\n",
    "\n",
    "*(You previously fixed an ‚Äúunalignable boolean Series‚Äù error by forcing all these objects to share the same index, then assigning by `valid_idx`.)*\n",
    "\n",
    "## 11) FIT ALL DATA & PRODUCE NEXT-DAY FORECAST\n",
    "\n",
    "* Refit all base models on **all available data**:\n",
    "\n",
    "  * ElasticNet uses **scaled** features (`StandardScaler`) because linear models are sensitive to scale.\n",
    "  * Tree/boosted models use raw features (they‚Äôre scale-invariant).\n",
    "* Build `base_next` by predicting the **last row** with each base model.\n",
    "* Feed `base_next` into the `meta` stacker to get `y_hat_scaled`.\n",
    "* If `USE_VOL_SCALE`, **rescale** the stacker‚Äôs output by the **last known** `vol20` to get a raw return forecast `y_hat`.\n",
    "* Convert to predicted price:\n",
    "\n",
    "  ```python\n",
    "  pred_next_close = last_close * (1 + y_hat)\n",
    "  ```\n",
    "* Print the last close, predicted return, predicted next close.\n",
    "\n",
    "**Interpretation:** `y_hat` is the model‚Äôs expected simple return for the *next* daily bar. It‚Äôs often a small number (bps). It‚Äôs not a certainty ‚Äî treat it like a noisy signal.\n",
    "\n",
    "## 12) QUICK OOF BACKTEST (long-only)\n",
    "\n",
    "```python\n",
    "oof_meta = Series(NaN, index=X.index)\n",
    "valid_idx = mask[mask].index\n",
    "oof_meta.loc[valid_idx] = meta.predict(Z.loc[valid_idx].values)\n",
    "```\n",
    "\n",
    "* Recomputes the **stacker predictions** using only OOF rows (never in-sample for that fold).\n",
    "\n",
    "**Unscale (if needed):**\n",
    "\n",
    "```python\n",
    "pred_unscaled = oof_meta * vol20   # if USE_VOL_SCALE\n",
    "```\n",
    "\n",
    "**Signals & PnL:**\n",
    "\n",
    "* **Entry rule:** go long if predicted next-day return > `THRESH_LONG`.\n",
    "* **Turnover & fees:** pay a half round-turn each time you switch position (simple approximation).\n",
    "* **PnL:** yesterday‚Äôs signal \\* today‚Äôs realized next-day return ‚àí fees.\n",
    "* **Equity:** cumulative product of (1 + pnl).\n",
    "* Prints **Total Return (after fees)**, **Directional Accuracy**, and **daily Sharpe**.\n",
    "\n",
    "> This is a **sanity check**, not a production backtest:\n",
    ">\n",
    "> * It‚Äôs **OOF only**, which is good (no in-sample cheating), but it‚Äôs very simple (no position sizing, no risk controls).\n",
    "> * If this can‚Äôt beat costs out-of-fold, the signal is likely too weak to trade as-is.\n",
    "\n",
    "---\n",
    "\n",
    "# What the printed numbers mean\n",
    "\n",
    "* **‚ÄúWalk-forward CV (OOF) ‚Ä¶ RMSE‚Äù**\n",
    "  Error of each base model on the *scaled* target (if enabled). Lower is better.\n",
    "\n",
    "* **‚Äústack RMSE (on OOF)‚Äù**\n",
    "  Error of the **blended** prediction. Should be ‚â§ best base model RMSE if stacking helps.\n",
    "\n",
    "* **‚ÄúNext-day Forecast‚Äù**\n",
    "  The single forecast for the next daily bar:\n",
    "  last close, predicted next-day **return**, and implied **next close**.\n",
    "\n",
    "* **Backtest block**\n",
    "\n",
    "  * **Total return (after fees)** on OOF predictions applying a simple long-only rule.\n",
    "  * **Directional accuracy:** fraction of times the sign of predicted return matched the realized return.\n",
    "  * **Sharpe (daily):** annualized (‚àö252) mean/std of daily pnl.\n",
    "\n",
    "---\n",
    "\n",
    "# Knobs you can tune\n",
    "\n",
    "* **Features:** add/remove indicators; try longer/shorter windows; add regime flags (e.g., SMA cross).\n",
    "* **Target:** stick with `USE_VOL_SCALE=True`. You can also try Huber loss in boosters (robust to outliers).\n",
    "* **Models:** enable XGB/LGBM if installed; increase estimators mildly; or run Optuna tuning.\n",
    "* **CV:** increase `SPLITS` (more folds) and adjust `GAP_DAYS` (‚â• 1 is good for dailies).\n",
    "* **Threshold & fees:** calibrate `THRESH_LONG` to maximize OOF Sharpe net of **your** costs.\n",
    "\n",
    "---\n",
    "\n",
    "# Common pitfalls (and how this script avoids them)\n",
    "\n",
    "* **Data leakage:** all features are `.shift(1)`; target uses `.shift(-1)`; CV is time-ordered with a purge gap.\n",
    "* **TA-Lib crashes:** `_as_float1d` forces the exact dtype/shape it expects.\n",
    "* **yfinance MultiIndex:** `_select_single_ticker_ohlcv` normalizes layout and columns.\n",
    "* **Sklearn version differences:** `RMSE()` helper avoids reliance on `squared=False`.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can:\n",
    "\n",
    "* Add an **intraday** version (India 9:15‚Äì15:30) with VWAP/OR range features and a 15:25 exit,\n",
    "* Plug in **Optuna** for automated hyper-parameter tuning,\n",
    "* Or export the **OOF predictions** and **feature importances** so you can inspect where the model is winning/losing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87631b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Walk-forward CV (OOF) on target y (scaled units) ===\n",
      "enet    RMSE: 1.185571\n",
      "rf      RMSE: 1.188595\n",
      "huber   RMSE: 1.172782\n",
      "stack   RMSE: 1.143632\n",
      "\n",
      "OOF RMSE in RAW return units: 0.016721\n",
      "\n",
      "=== Tuned OOF Backtest (net of fees) ===\n",
      "Chosen threshold: 0.0 (sigma units)\n",
      "Chosen sizing gain (SIZE_K): 0.5\n",
      "Total return:  16.65%\n",
      "Sharpe (daily): 0.50\n",
      "Max drawdown:   -4.84%\n",
      "Directional acc: 52.39%\n",
      "\n",
      "=== Next-day Forecast ===\n",
      "Last close: 1500.10\n",
      "Predicted next-day return: 0.000864\n",
      "Predicted next close: 1501.40\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# CONFIG\n",
    "# =========================================\n",
    "TICKER        = \"INFY.NS\"\n",
    "START_DATE    = \"2015-01-01\"\n",
    "END_DATE      = None\n",
    "\n",
    "SPLITS        = 6\n",
    "GAP_DAYS      = 1\n",
    "\n",
    "USE_VOL_SCALE    = True       # train target as y/vol and rescale back\n",
    "THRESH_IN_SIGMA  = True       # interpret thresholds in sigma units (y/œÉ) using oof_meta\n",
    "ALLOW_SHORTS     = True       # allow short positions if signal < -threshold\n",
    "\n",
    "# Backtest costs and knobs\n",
    "FEE_RT        = 0.0006        # round-turn fee+slippage\n",
    "INIT_CAPITAL  = 1.0           # normalized\n",
    "\n",
    "# Position sizing mode\n",
    "USE_POSITION_SIZING = True    # if False => fixed 1x positions gated by thresholds\n",
    "SIZE_K_GRID         = [0.5, 0.75, 1.0, 1.25, 1.5]  # tanh gain sweep (only if USE_POSITION_SIZING)\n",
    "\n",
    "# Threshold grid (we'll auto-pick best)\n",
    "# If THRESH_IN_SIGMA=True, these are in sigma units (on oof_meta). Otherwise in raw returns (on pred_unscaled).\n",
    "THRESH_GRID   = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]  # sigma units (‚âà standard deviations)\n",
    "THRESH_GRID_RAW = [0.0, 0.0003, 0.0005, 0.0007, 0.0010, 0.0015]          # raw return units (fallback if THRESH_IN_SIGMA=False)\n",
    "\n",
    "# =========================================\n",
    "# IMPORTS\n",
    "# =========================================\n",
    "import warnings, numpy as np, pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet, Ridge, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Optional boosters\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    HAS_XGB = False\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    HAS_LGBM = True\n",
    "except Exception:\n",
    "    HAS_LGBM = False\n",
    "\n",
    "# =========================================\n",
    "# HELPERS\n",
    "# =========================================\n",
    "def RMSE(y_true, y_pred):\n",
    "    # sklearn<0.22 compatibility (no squared=False)\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def max_drawdown(equity: pd.Series):\n",
    "    peak = equity.cummax()\n",
    "    dd = (equity - peak) / peak\n",
    "    return float(dd.min()) if len(dd) else 0.0\n",
    "\n",
    "def _as_float1d(x):\n",
    "    if isinstance(x, pd.DataFrame):\n",
    "        x = x.squeeze()\n",
    "    x = pd.to_numeric(pd.Series(x), errors=\"coerce\")\n",
    "    a = np.asarray(x, dtype=np.float64).reshape(-1)\n",
    "    return np.ascontiguousarray(a, dtype=np.float64)\n",
    "\n",
    "def _select_single_ticker_ohlcv(df, ticker):\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        lev0 = df.columns.get_level_values(0).unique()\n",
    "        lev1 = df.columns.get_level_values(1).unique()\n",
    "        if ticker in lev0:\n",
    "            sub = df[ticker].copy()\n",
    "        elif ticker in lev1:\n",
    "            sub = df.xs(ticker, axis=1, level=1).copy()\n",
    "        else:\n",
    "            raise ValueError(f\"TICKER '{ticker}' not found in downloaded columns.\")\n",
    "    else:\n",
    "        sub = df.copy()\n",
    "\n",
    "    cols = list(sub.columns)\n",
    "    if \"Close\" not in cols and \"Adj Close\" in cols:\n",
    "        sub[\"Close\"] = sub[\"Adj Close\"]\n",
    "\n",
    "    required = [\"Open\", \"High\", \"Low\", \"Close\"]\n",
    "    for c in required:\n",
    "        if c not in sub.columns:\n",
    "            raise ValueError(f\"Missing required column '{c}'. Have: {list(sub.columns)}\")\n",
    "\n",
    "    if \"Volume\" not in sub.columns:\n",
    "        sub[\"Volume\"] = 0.0\n",
    "\n",
    "    keep = [c for c in [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"] if c in sub.columns]\n",
    "    sub[keep] = sub[keep].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    sub = sub.dropna(subset=[\"Open\",\"High\",\"Low\",\"Close\"]).sort_index()\n",
    "    return sub\n",
    "\n",
    "def ts_purged_splits(X, n_splits=5, gap=0):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    for tr, te in tscv.split(X):\n",
    "        if gap > 0:\n",
    "            te = te[gap:]\n",
    "        if len(te) == 0:\n",
    "            continue\n",
    "        yield tr, te\n",
    "\n",
    "# =========================================\n",
    "# DATA\n",
    "# =========================================\n",
    "raw = yf.download(TICKER, start=START_DATE, end=END_DATE, auto_adjust=True, progress=False)\n",
    "if raw is None or raw.empty:\n",
    "    raise ValueError(\"No data downloaded. Check ticker/date range or connection.\")\n",
    "\n",
    "df = _select_single_ticker_ohlcv(raw, TICKER)\n",
    "if len(df) < 600:\n",
    "    raise ValueError(\"Not enough rows after cleaning. Increase history.\")\n",
    "\n",
    "cl, hi, lo, vo = df[\"Close\"], df[\"High\"], df[\"Low\"], df[\"Volume\"]\n",
    "ret = cl.pct_change()\n",
    "\n",
    "# =========================================\n",
    "# TA INDICATORS (TA-Lib if available)\n",
    "# =========================================\n",
    "try:\n",
    "    import talib as ta\n",
    "    HAS_TA = True\n",
    "except Exception:\n",
    "    HAS_TA = False\n",
    "\n",
    "def rsi(series, n=14):\n",
    "    if HAS_TA:\n",
    "        return pd.Series(ta.RSI(_as_float1d(series), timeperiod=n), index=series.index)\n",
    "    delta = series.diff()\n",
    "    gain  = delta.clip(lower=0).rolling(n).mean()\n",
    "    loss  = -delta.clip(upper=0).rolling(n).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100/(1+rs))\n",
    "\n",
    "def macd(series, fast=12, slow=26, sig=9):\n",
    "    if HAS_TA:\n",
    "        m, s, h = ta.MACD(_as_float1d(series), fastperiod=fast, slowperiod=slow, signalperiod=sig)\n",
    "        idx = series.index\n",
    "        return pd.Series(m, idx), pd.Series(s, idx), pd.Series(h, idx)\n",
    "    ema_f = series.ewm(span=fast, adjust=False).mean()\n",
    "    ema_s = series.ewm(span=slow, adjust=False).mean()\n",
    "    line = ema_f - ema_s\n",
    "    signal = line.ewm(span=sig, adjust=False).mean()\n",
    "    hist = line - signal\n",
    "    return line, signal, hist\n",
    "\n",
    "def stoch(high, low, close, k=14, d=3):\n",
    "    if HAS_TA:\n",
    "        sk, sd = ta.STOCH(_as_float1d(high), _as_float1d(low), _as_float1d(close),\n",
    "                          fastk_period=k, slowk_period=d, slowd_period=d)\n",
    "        idx = close.index\n",
    "        return pd.Series(sk, idx), pd.Series(sd, idx)\n",
    "    ll = low.rolling(k).min(); hh = high.rolling(k).max()\n",
    "    fastk = 100 * (close - ll) / (hh - ll)\n",
    "    slowk = fastk.rolling(d).mean(); slowd = slowk.rolling(d).mean()\n",
    "    return slowk, slowd\n",
    "\n",
    "def bbands(series, n=20, nbdev=2):\n",
    "    if HAS_TA:\n",
    "        up, mid, low = ta.BBANDS(_as_float1d(series), timeperiod=n, nbdevup=nbdev, nbdevdn=nbdev, matype=0)\n",
    "        idx = series.index\n",
    "        return pd.Series(up, idx), pd.Series(mid, idx), pd.Series(low, idx)\n",
    "    m = series.rolling(n).mean(); s = series.rolling(n).std()\n",
    "    up, low = m + nbdev*s, m - nbdev*s\n",
    "    return up, m, low\n",
    "\n",
    "def adx(high, low, close, n=14):\n",
    "    if HAS_TA:\n",
    "        return pd.Series(ta.ADX(_as_float1d(high), _as_float1d(low), _as_float1d(close), timeperiod=n),\n",
    "                         index=close.index)\n",
    "    tr = np.maximum(high-low, np.maximum((high-close.shift()).abs(), (low-close.shift()).abs()))\n",
    "    dm_pos = (high - high.shift()).clip(lower=0); dm_neg = (low.shift() - low).clip(lower=0)\n",
    "    atr = tr.rolling(n).mean()\n",
    "    di_pos = 100 * (dm_pos.ewm(span=n, adjust=False).mean() / atr)\n",
    "    di_neg = 100 * (dm_neg.ewm(span=n, adjust=False).mean() / atr)\n",
    "    dx = 100 * (di_pos - di_neg).abs() / (di_pos + di_neg)\n",
    "    return dx.ewm(span=n, adjust=False).mean()\n",
    "\n",
    "def atr(high, low, close, n=14):\n",
    "    if HAS_TA:\n",
    "        return pd.Series(ta.ATR(_as_float1d(high), _as_float1d(low), _as_float1d(close), timeperiod=n),\n",
    "                         index=close.index)\n",
    "    tr = np.maximum(high-low, np.maximum((high-close.shift()).abs(), (low-close.shift()).abs()))\n",
    "    return tr.rolling(n).mean()\n",
    "\n",
    "def mfi(high, low, close, vol, n=14):\n",
    "    if HAS_TA:\n",
    "        return pd.Series(ta.MFI(_as_float1d(high), _as_float1d(low), _as_float1d(close), _as_float1d(vol),\n",
    "                                timeperiod=n), index=close.index)\n",
    "    tp = (high+low+close)/3\n",
    "    pmf = (tp.diff().clip(lower=0) * vol); nmf = (-tp.diff().clip(upper=0) * vol)\n",
    "    mr = pmf.rolling(n).sum() / nmf.rolling(n).sum()\n",
    "    return 100 - (100/(1+mr))\n",
    "\n",
    "def cci(high, low, close, n=20):\n",
    "    if HAS_TA:\n",
    "        return pd.Series(ta.CCI(_as_float1d(high), _as_float1d(low), _as_float1d(close), timeperiod=n),\n",
    "                         index=close.index)\n",
    "    tp = (high+low+close)/3; sma = tp.rolling(n).mean()\n",
    "    md = (tp - sma).abs().rolling(n).mean()\n",
    "    return (tp - sma) / (0.015 * md)\n",
    "\n",
    "def willr(high, low, close, n=14):\n",
    "    if HAS_TA:\n",
    "        return pd.Series(ta.WILLR(_as_float1d(high), _as_float1d(low), _as_float1d(close), timeperiod=n),\n",
    "                         index=close.index)\n",
    "    ll = low.rolling(n).min(); hh = high.rolling(n).max()\n",
    "    return -100 * (hh - close) / (hh - ll)\n",
    "\n",
    "def obv(close, vol):\n",
    "    if HAS_TA:\n",
    "        return pd.Series(ta.OBV(_as_float1d(close), _as_float1d(vol)), index=close.index)\n",
    "    return (np.sign(close.diff().fillna(0)) * vol).fillna(0).cumsum()\n",
    "\n",
    "# =========================================\n",
    "# FEATURES (shifted to avoid leakage)\n",
    "# =========================================\n",
    "rsi14 = rsi(cl, 14).shift(1)\n",
    "m_line, m_sig, m_hist = macd(cl); m_line, m_sig, m_hist = m_line.shift(1), m_sig.shift(1), m_hist.shift(1)\n",
    "sk, sd = stoch(hi, lo, cl); sk, sd = sk.shift(1), sd.shift(1)\n",
    "bb_up, bb_mid, bb_low = bbands(cl); bb_up, bb_mid, bb_low = bb_up.shift(1), bb_mid.shift(1), bb_low.shift(1)\n",
    "bb_width = (bb_up - bb_low) / bb_mid\n",
    "adx14 = adx(hi, lo, cl, 14).shift(1)\n",
    "atr14 = atr(hi, lo, cl, 14).shift(1)\n",
    "mfi14 = mfi(hi, lo, cl, vo, 14).shift(1)\n",
    "cci20 = cci(hi, lo, cl, 20).shift(1)\n",
    "will14 = willr(hi, lo, cl, 14).shift(1)\n",
    "obv_roc = obv(cl, vo).pct_change().shift(1)\n",
    "\n",
    "X = pd.DataFrame({\n",
    "    \"ret_1\": ret.shift(1), \"ret_2\": ret.shift(2), \"ret_3\": ret.shift(3),\n",
    "    \"ret_5\": ret.shift(5), \"ret_10\": ret.shift(10),\n",
    "    \"mean_5\": ret.rolling(5).mean().shift(1), \"std_5\": ret.rolling(5).std().shift(1),\n",
    "    \"mean_20\": ret.rolling(20).mean().shift(1), \"std_20\": ret.rolling(20).std().shift(1),\n",
    "    \"rsi14\": rsi14, \"macd\": m_line, \"macd_sig\": m_sig, \"macd_hist\": m_hist,\n",
    "    \"stoch_k\": sk, \"stoch_d\": sd, \"bb_width\": bb_width,\n",
    "    \"adx14\": adx14, \"atr14\": atr14, \"mfi14\": mfi14, \"cci20\": cci20, \"will14\": will14,\n",
    "    \"obv_roc\": obv_roc,\n",
    "    \"dow_0\": (df.index.dayofweek==0).astype(int),\n",
    "    \"dow_1\": (df.index.dayofweek==1).astype(int),\n",
    "    \"dow_2\": (df.index.dayofweek==2).astype(int),\n",
    "    \"dow_3\": (df.index.dayofweek==3).astype(int),\n",
    "    \"dow_4\": (df.index.dayofweek==4).astype(int),\n",
    "}).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# =========================================\n",
    "# TARGET (next-day return) + scaling\n",
    "# =========================================\n",
    "y_next = ret.shift(-1)\n",
    "vol20  = ret.rolling(20).std()\n",
    "y      = (y_next / (vol20.shift(1) + 1e-12)) if USE_VOL_SCALE else y_next\n",
    "\n",
    "data = pd.concat([X, y.rename(\"y\"), y_next.rename(\"y_raw\")], axis=1).dropna()\n",
    "X, y, y_raw = data[X.columns], data[\"y\"], data[\"y_raw\"]\n",
    "\n",
    "# =========================================\n",
    "# MODELS (with robust options)\n",
    "# =========================================\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "base_models = []\n",
    "base_models.append((\"enet\",  ElasticNet(alpha=0.001, l1_ratio=0.15, max_iter=5000, random_state=0)))\n",
    "base_models.append((\"rf\",    RandomForestRegressor(n_estimators=500, max_depth=9, min_samples_leaf=5,\n",
    "                                                   random_state=0, n_jobs=-1)))\n",
    "# Robust linear model\n",
    "base_models.append((\"huber\", HuberRegressor(epsilon=1.5, alpha=0.0005)))  # needs scaled X\n",
    "\n",
    "if HAS_XGB:\n",
    "    base_models.append((\"xgb\", XGBRegressor(\n",
    "        n_estimators=700, max_depth=5, learning_rate=0.03, subsample=0.9, colsample_bytree=0.9,\n",
    "        reg_lambda=6.0, random_state=0, tree_method=\"hist\", n_jobs=-1)))\n",
    "if HAS_LGBM:\n",
    "    # Try huber objective if supported, else default\n",
    "    try:\n",
    "        base_models.append((\"lgbm\", LGBMRegressor(\n",
    "            n_estimators=1000, num_leaves=48, learning_rate=0.02, subsample=0.9,\n",
    "            colsample_bytree=0.9, reg_lambda=6.0, objective=\"huber\", random_state=0, n_jobs=-1)))\n",
    "    except Exception:\n",
    "        base_models.append((\"lgbm\", LGBMRegressor(\n",
    "            n_estimators=1000, num_leaves=48, learning_rate=0.02, subsample=0.9,\n",
    "            colsample_bytree=0.9, reg_lambda=6.0, random_state=0, n_jobs=-1)))\n",
    "\n",
    "# =========================================\n",
    "# OOF PREDICTIONS (walk-forward w/ purge gap)\n",
    "# =========================================\n",
    "oof_preds = {name: np.full(len(X), np.nan) for name, _ in base_models}\n",
    "oof_truth = np.full(len(X), np.nan)\n",
    "\n",
    "for tr, te in ts_purged_splits(X, n_splits=SPLITS, gap=GAP_DAYS):\n",
    "    Xtr, Xte = X.iloc[tr], X.iloc[te]\n",
    "    ytr, yte = y.iloc[tr], y.iloc[te]\n",
    "    Xtr_s = scaler.fit_transform(Xtr); Xte_s = scaler.transform(Xte)\n",
    "\n",
    "    for name, model in base_models:\n",
    "        if name in (\"enet\", \"huber\"):   # needs scaling\n",
    "            model.fit(Xtr_s, ytr)\n",
    "            pred = model.predict(Xte_s)\n",
    "        else:\n",
    "            model.fit(Xtr, ytr)\n",
    "            pred = model.predict(Xte)\n",
    "        oof_preds[name][te] = pred\n",
    "    oof_truth[te] = yte.values\n",
    "\n",
    "# Print scaled RMSE per base\n",
    "print(\"=== Walk-forward CV (OOF) on target y (scaled units) ===\")\n",
    "mask_all = ~np.isnan(oof_truth)\n",
    "for name, _ in base_models:\n",
    "    m = ~np.isnan(oof_preds[name]) & mask_all\n",
    "    rmse_scaled = RMSE(oof_truth[m], oof_preds[name][m])\n",
    "    print(f\"{name:6s}  RMSE: {rmse_scaled:.6f}\")\n",
    "\n",
    "# Stack meta on OOF preds\n",
    "Z = pd.DataFrame({name: oof_preds[name] for name, _ in base_models}, index=X.index)\n",
    "mask_series = pd.Series(mask_all, index=X.index)\n",
    "mask = Z.notna().all(axis=1) & mask_series\n",
    "\n",
    "meta = Ridge(alpha=1.0)\n",
    "meta.fit(Z.loc[mask].values, oof_truth[mask])\n",
    "stack_rmse_scaled = RMSE(oof_truth[mask], meta.predict(Z.loc[mask].values))\n",
    "print(f\"stack   RMSE: {stack_rmse_scaled:.6f}\")\n",
    "\n",
    "# =========================================\n",
    "# Convert OOF preds to RAW return units for interpretability\n",
    "# =========================================\n",
    "oof_meta = pd.Series(np.nan, index=X.index)\n",
    "valid_idx = mask[mask].index\n",
    "oof_meta.loc[valid_idx] = meta.predict(Z.loc[valid_idx].values)          # in scaled units if USE_VOL_SCALE\n",
    "\n",
    "if USE_VOL_SCALE:\n",
    "    vol_aligned = vol20.reindex(oof_meta.index)\n",
    "    pred_unscaled = oof_meta * vol_aligned                               # -> raw return units\n",
    "else:\n",
    "    pred_unscaled = oof_meta\n",
    "\n",
    "# Raw RMSE (OOF)\n",
    "rmse_raw = RMSE(y_raw.loc[valid_idx], pred_unscaled.loc[valid_idx])\n",
    "print(f\"\\nOOF RMSE in RAW return units: {rmse_raw:.6f}\")\n",
    "\n",
    "# =========================================\n",
    "# TUNED BACKTEST (threshold & sizing)\n",
    "# =========================================\n",
    "def backtest_from_preds(pred_raw: pd.Series, pred_sigma: pd.Series, realized_raw: pd.Series,\n",
    "                        fees_rt: float, threshold: float, thresh_in_sigma: bool,\n",
    "                        use_sizing: bool, size_k: float, allow_shorts: bool):\n",
    "    \"\"\"\n",
    "    pred_raw: predicted next-day return (raw units)\n",
    "    pred_sigma: predicted next-day return in sigma units (oof_meta)\n",
    "    realized_raw: realized next-day return (raw)\n",
    "    \"\"\"\n",
    "    idx = pred_raw.index.intersection(realized_raw.index)\n",
    "    pr  = pred_raw.loc[idx].copy()\n",
    "    ps  = pred_sigma.loc[idx].copy()\n",
    "    rr  = realized_raw.loc[idx].copy()\n",
    "\n",
    "    if thresh_in_sigma:\n",
    "        gate_long  = ps >  threshold\n",
    "        gate_short = ps < -threshold if allow_shorts else pd.Series(False, index=idx)\n",
    "    else:\n",
    "        gate_long  = pr >  threshold\n",
    "        gate_short = pr < -threshold if allow_shorts else pd.Series(False, index=idx)\n",
    "\n",
    "    if use_sizing:\n",
    "        # Confidence-based sizing using sigma-units (stable): pos ‚àà [-1,1]\n",
    "        # Gain parameter size_k controls aggressiveness.\n",
    "        pos = np.tanh(ps / (size_k + 1e-12))\n",
    "        if not allow_shorts:\n",
    "            pos = pos.clip(lower=0)  # long-only sizing\n",
    "        # Apply gates: zero out small/conflicted signals\n",
    "        mask_gate = gate_long | gate_short\n",
    "        pos = pos.where(mask_gate, 0.0)\n",
    "    else:\n",
    "        # Fixed 1x positions with threshold gating\n",
    "        pos = pd.Series(0.0, index=idx)\n",
    "        pos[gate_long]  =  1.0\n",
    "        if allow_shorts:\n",
    "            pos[gate_short] = -1.0\n",
    "\n",
    "    # Fees on changes in position (half round-turn per change)\n",
    "    turnover = pos.diff().abs().fillna(0.0)\n",
    "    fees = turnover * (fees_rt / 2.0)\n",
    "\n",
    "    pnl = pos.shift(1).fillna(0.0) * rr - fees\n",
    "    equity = (INIT_CAPITAL * (1.0 + pnl.fillna(0.0))).cumprod()\n",
    "\n",
    "    ann_factor = 252.0\n",
    "    daily_ret  = pnl.fillna(0.0)\n",
    "    sharpe = (np.sqrt(ann_factor) * daily_ret.mean() / (daily_ret.std() + 1e-12)) if daily_ret.std() > 0 else 0.0\n",
    "    total_ret = equity.iloc[-1] / equity.iloc[0] - 1.0 if len(equity) else 0.0\n",
    "    mdd = max_drawdown(equity)\n",
    "\n",
    "    hit = (np.sign(pr) == np.sign(rr)).mean()  # directional accuracy based on raw preds\n",
    "    return {\n",
    "        \"equity\": equity, \"pnl\": daily_ret,\n",
    "        \"total_return\": float(total_ret), \"sharpe\": float(sharpe),\n",
    "        \"mdd\": float(mdd), \"hit_rate\": float(hit)\n",
    "    }\n",
    "\n",
    "# Sweep thresholds (and optionally sizing gain) to maximize Sharpe (OOF)\n",
    "def tune_threshold_and_size(pred_raw, pred_sigma, realized_raw, fees_rt,\n",
    "                            thresh_grid_sigma, thresh_grid_raw, use_sigma, use_sizing, size_k_grid, allow_shorts):\n",
    "    best = {\"sharpe\": -9e9}\n",
    "    if use_sigma:\n",
    "        tg = thresh_grid_sigma\n",
    "    else:\n",
    "        tg = thresh_grid_raw\n",
    "    if not tg:\n",
    "        tg = [0.0]\n",
    "\n",
    "    for th in tg:\n",
    "        if use_sizing:\n",
    "            for k in size_k_grid:\n",
    "                res = backtest_from_preds(pred_raw, pred_sigma, realized_raw, fees_rt, th, use_sigma, True, k, allow_shorts)\n",
    "                if res[\"sharpe\"] > best[\"sharpe\"]:\n",
    "                    best = {\"threshold\": th, \"size_k\": k, **res}\n",
    "        else:\n",
    "            res = backtest_from_preds(pred_raw, pred_sigma, realized_raw, fees_rt, th, use_sigma, False, 1.0, allow_shorts)\n",
    "            if res[\"sharpe\"] > best[\"sharpe\"]:\n",
    "                best = {\"threshold\": th, \"size_k\": 1.0, **res}\n",
    "    return best\n",
    "\n",
    "best = tune_threshold_and_size(\n",
    "    pred_unscaled.dropna(), oof_meta.dropna(), y_raw, FEE_RT,\n",
    "    THRESH_GRID, THRESH_GRID_RAW, THRESH_IN_SIGMA, USE_POSITION_SIZING, SIZE_K_GRID, ALLOW_SHORTS\n",
    ")\n",
    "\n",
    "print(\"\\n=== Tuned OOF Backtest (net of fees) ===\")\n",
    "unit = \"sigma units\" if THRESH_IN_SIGMA else \"raw return\"\n",
    "print(f\"Chosen threshold: {best['threshold']} ({unit})\")\n",
    "if USE_POSITION_SIZING:\n",
    "    print(f\"Chosen sizing gain (SIZE_K): {best['size_k']}\")\n",
    "print(f\"Total return:  {best['total_return']*100:.2f}%\")\n",
    "print(f\"Sharpe (daily): {best['sharpe']:.2f}\")\n",
    "print(f\"Max drawdown:   {best['mdd']*100:.2f}%\")\n",
    "print(f\"Directional acc: {best['hit_rate']*100:.2f}%\")\n",
    "\n",
    "# =========================================\n",
    "# TRAIN ON ALL DATA & NEXT-DAY FORECAST\n",
    "# =========================================\n",
    "# Fit base learners on full data\n",
    "X_s = scaler.fit_transform(X)\n",
    "for name, model in base_models:\n",
    "    if name in (\"enet\", \"huber\"):\n",
    "        model.fit(X_s, y)\n",
    "    else:\n",
    "        model.fit(X, y)\n",
    "\n",
    "# Current last-row prediction (stacked)\n",
    "last_row_raw = X.iloc[[-1]]\n",
    "last_row_s   = scaler.transform(last_row_raw)\n",
    "\n",
    "base_next = []\n",
    "for name, model in base_models:\n",
    "    if name in (\"enet\", \"huber\"):\n",
    "        base_next.append(model.predict(last_row_s)[0])\n",
    "    else:\n",
    "        base_next.append(model.predict(last_row_raw)[0])\n",
    "\n",
    "y_hat_scaled = float(meta.predict(np.array(base_next).reshape(1, -1))[0])\n",
    "if USE_VOL_SCALE:\n",
    "    last_vol = float(vol20.reindex(X.index).iloc[-1])\n",
    "    y_hat = y_hat_scaled * (last_vol if np.isfinite(last_vol) else 0.0)\n",
    "else:\n",
    "    y_hat = y_hat_scaled\n",
    "\n",
    "last_close = float(cl.reindex(X.index).iloc[-1])\n",
    "pred_next_close = last_close * (1 + y_hat)\n",
    "\n",
    "print(\"\\n=== Next-day Forecast ===\")\n",
    "print(f\"Last close: {last_close:.2f}\")\n",
    "print(f\"Predicted next-day return: {y_hat:.6f}\")\n",
    "print(f\"Predicted next close: {pred_next_close:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "091f215d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== INFY.NS ===\n",
      "Scaled OOF RMSE: {'enet': '1.1856', 'rf': '1.1881', 'huber': '1.1728'} | stack: 1.1436\n",
      "OOF RMSE (raw): 0.016721\n",
      "Tuned Sharpe: 0.49 | TotalRet: 16.52% | MDD: -4.91% | Hit: 52.48%\n",
      "Next-day: last=1500.10 pred_ret=0.000861 pred_close=1501.39\n",
      "\n",
      "=== TCS.NS ===\n",
      "Scaled OOF RMSE: {'enet': '1.1733', 'rf': '1.1530', 'huber': '1.1714'} | stack: 1.1273\n",
      "OOF RMSE (raw): 0.014959\n",
      "Tuned Sharpe: 0.55 | TotalRet: 3.93% | MDD: -1.37% | Hit: 51.35%\n",
      "Next-day: last=3093.70 pred_ret=0.000461 pred_close=3095.13\n",
      "\n",
      "=== RELIANCE.NS ===\n",
      "Scaled OOF RMSE: {'enet': '1.1846', 'rf': '1.1728', 'huber': '1.1809'} | stack: 1.1448\n",
      "OOF RMSE (raw): 0.017414\n",
      "Tuned Sharpe: 0.80 | TotalRet: 11.74% | MDD: -3.35% | Hit: 51.85%\n",
      "Next-day: last=1385.90 pred_ret=0.000853 pred_close=1387.08\n",
      "\n",
      "================ PORTFOLIO (Equal-weight OOF) ================\n",
      "Symbols used: 3 / 3\n",
      "Total return:  10.77%\n",
      "Sharpe (daily): 0.79\n",
      "Max drawdown:   -2.24%\n",
      "Directional acc: 51.89%\n",
      "\n",
      "================ NEXT-DAY FORECASTS =================\n",
      "   INFY.NS | last=1500.10  pred_ret=0.000861  pred_close=1501.39\n",
      "    TCS.NS | last=3093.70  pred_ret=0.000461  pred_close=3095.13\n",
      "RELIANCE.NS | last=1385.90  pred_ret=0.000853  pred_close=1387.08\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# CONFIG\n",
    "# =========================================\n",
    "TICKERS       = [\"INFY.NS\", \"TCS.NS\", \"RELIANCE.NS\"]  # <- put your list here\n",
    "START_DATE    = \"2015-01-01\"\n",
    "END_DATE      = None\n",
    "\n",
    "SPLITS        = 6\n",
    "GAP_DAYS      = 1\n",
    "\n",
    "USE_VOL_SCALE    = True        # train y/vol and rescale back\n",
    "THRESH_IN_SIGMA  = True        # threshold on sigma-units (stable); else raw-return\n",
    "ALLOW_SHORTS     = True        # allow short positions if signal < -threshold\n",
    "\n",
    "# Backtest costs and knobs\n",
    "FEE_RT        = 0.0006         # round-turn fee+slippage per symbol\n",
    "INIT_CAPITAL  = 1.0            # normalized (per-symbol and portfolio)\n",
    "\n",
    "# Position sizing mode\n",
    "USE_POSITION_SIZING = True     # False => fixed 1x when gated\n",
    "SIZE_K_GRID         = [0.5, 0.75, 1.0, 1.25, 1.5]  # tanh gain sweep\n",
    "\n",
    "# Threshold grid (auto-picked for max OOF Sharpe)\n",
    "THRESH_GRID_SIGMA = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]  # sigma units\n",
    "THRESH_GRID_RAW   = [0.0, 0.0003, 0.0005, 0.0007, 0.0010, 0.0015]            # raw return units\n",
    "\n",
    "MIN_ROWS_PER_SYMBOL = 600      # skip too-short histories\n",
    "VERBOSE_PER_SYMBOL  = True     # print per-symbol summary\n",
    "\n",
    "# =========================================\n",
    "# IMPORTS\n",
    "# =========================================\n",
    "import warnings, numpy as np, pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet, Ridge, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Optional boosters\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    HAS_XGB = False\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    HAS_LGBM = True\n",
    "except Exception:\n",
    "    HAS_LGBM = False\n",
    "\n",
    "# =========================================\n",
    "# UTILS\n",
    "# =========================================\n",
    "def RMSE(y_true, y_pred):\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def max_drawdown(equity: pd.Series):\n",
    "    peak = equity.cummax()\n",
    "    dd = (equity - peak) / (peak + 1e-12)\n",
    "    return float(dd.min()) if len(dd) else 0.0\n",
    "\n",
    "def _as_float1d(x):\n",
    "    if isinstance(x, pd.DataFrame):\n",
    "        x = x.squeeze()\n",
    "    x = pd.to_numeric(pd.Series(x), errors=\"coerce\")\n",
    "    a = np.asarray(x, dtype=np.float64).reshape(-1)\n",
    "    return np.ascontiguousarray(a, dtype=np.float64)\n",
    "\n",
    "def _select_single_ticker_ohlcv(df, ticker):\n",
    "    \"\"\"Handle single or multi-index columns; return clean OHLCV for one ticker.\"\"\"\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        lev0 = df.columns.get_level_values(0).unique()\n",
    "        lev1 = df.columns.get_level_values(1).unique()\n",
    "        if ticker in lev0:\n",
    "            sub = df[ticker].copy()\n",
    "        elif ticker in lev1:\n",
    "            sub = df.xs(ticker, axis=1, level=1).copy()\n",
    "        else:\n",
    "            raise ValueError(f\"TICKER '{ticker}' not found in downloaded columns.\")\n",
    "    else:\n",
    "        sub = df.copy()\n",
    "\n",
    "    cols = list(sub.columns)\n",
    "    if \"Close\" not in cols and \"Adj Close\" in cols:\n",
    "        sub[\"Close\"] = sub[\"Adj Close\"]\n",
    "\n",
    "    for c in [\"Open\", \"High\", \"Low\", \"Close\"]:\n",
    "        if c not in sub.columns:\n",
    "            raise ValueError(f\"Missing required column '{c}'. Have: {list(sub.columns)}\")\n",
    "\n",
    "    if \"Volume\" not in sub.columns:\n",
    "        sub[\"Volume\"] = 0.0\n",
    "\n",
    "    keep = [c for c in [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"] if c in sub.columns]\n",
    "    sub[keep] = sub[keep].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    sub = sub.dropna(subset=[\"Open\",\"High\",\"Low\",\"Close\"]).sort_index()\n",
    "    return sub\n",
    "\n",
    "def ts_purged_splits(X, n_splits=5, gap=0):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    for tr, te in tscv.split(X):\n",
    "        if gap > 0:\n",
    "            te = te[gap:]\n",
    "        if len(te) == 0:\n",
    "            continue\n",
    "        yield tr, te\n",
    "\n",
    "# =========================================\n",
    "# INDICATORS (TA-Lib with fallback)\n",
    "# =========================================\n",
    "try:\n",
    "    import talib as ta\n",
    "    HAS_TA = True\n",
    "except Exception:\n",
    "    HAS_TA = False\n",
    "\n",
    "def rsi(series, n=14):\n",
    "    if HAS_TA:\n",
    "        return pd.Series(ta.RSI(_as_float1d(series), timeperiod=n), index=series.index)\n",
    "    delta = series.diff()\n",
    "    gain  = delta.clip(lower=0).rolling(n).mean()\n",
    "    loss  = -delta.clip(upper=0).rolling(n).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100/(1+rs))\n",
    "\n",
    "def macd(series, fast=12, slow=26, sig=9):\n",
    "    if HAS_TA:\n",
    "        m, s, h = ta.MACD(_as_float1d(series), fastperiod=fast, slowperiod=slow, signalperiod=sig)\n",
    "        idx = series.index\n",
    "        return pd.Series(m, idx), pd.Series(s, idx), pd.Series(h, idx)\n",
    "    ema_f = series.ewm(span=fast, adjust=False).mean()\n",
    "    ema_s = series.ewm(span=slow, adjust=False).mean()\n",
    "    line = ema_f - ema_s\n",
    "    signal = line.ewm(span=sig, adjust=False).mean()\n",
    "    hist = line - signal\n",
    "    return line, signal, hist\n",
    "\n",
    "def stoch(high, low, close, k=14, d=3):\n",
    "    if HAS_TA:\n",
    "        sk, sd = ta.STOCH(_as_float1d(high), _as_float1d(low), _as_float1d(close),\n",
    "                          fastk_period=k, slowk_period=d, slowd_period=d)\n",
    "        idx = close.index\n",
    "        return pd.Series(sk, idx), pd.Series(sd, idx)\n",
    "    ll = low.rolling(k).min(); hh = high.rolling(k).max()\n",
    "    fastk = 100 * (close - ll) / (hh - ll)\n",
    "    slowk = fastk.rolling(d).mean(); slowd = slowk.rolling(d).mean()\n",
    "    return slowk, slowd\n",
    "\n",
    "def bbands(series, n=20, nbdev=2):\n",
    "    if HAS_TA:\n",
    "        up, mid, low = ta.BBANDS(_as_float1d(series), timeperiod=n, nbdevup=nbdev, nbdevdn=nbdev, matype=0)\n",
    "        idx = series.index\n",
    "        return pd.Series(up, idx), pd.Series(mid, idx), pd.Series(low, idx)\n",
    "    m = series.rolling(n).mean(); s = series.rolling(n).std()\n",
    "    up, low = m + nbdev*s, m - nbdev*s\n",
    "    return up, m, low\n",
    "\n",
    "def adx(high, low, close, n=14):\n",
    "    if HAS_TA:\n",
    "        return pd.Series(ta.ADX(_as_float1d(high), _as_float1d(low), _as_float1d(close), timeperiod=n),\n",
    "                         index=close.index)\n",
    "    tr = np.maximum(high-low, np.maximum((high-close.shift()).abs(), (low-close.shift()).abs()))\n",
    "    dm_pos = (high - high.shift()).clip(lower=0); dm_neg = (low.shift() - low).clip(lower=0)\n",
    "    atr = tr.rolling(n).mean()\n",
    "    di_pos = 100 * (dm_pos.ewm(span=n, adjust=False).mean() / atr)\n",
    "    di_neg = 100 * (dm_neg.ewm(span=n, adjust=False).mean() / atr)\n",
    "    dx = 100 * (di_pos - di_neg).abs() / (di_pos + di_neg)\n",
    "    return dx.ewm(span=n, adjust=False).mean()\n",
    "\n",
    "def atr(high, low, close, n=14):\n",
    "    if HAS_TA:\n",
    "        return pd.Series(ta.ATR(_as_float1d(high), _as_float1d(low), _as_float1d(close), timeperiod=n),\n",
    "                         index=close.index)\n",
    "    tr = np.maximum(high-low, np.maximum((high-close.shift()).abs(), (low-close.shift()).abs()))\n",
    "    return tr.rolling(n).mean()\n",
    "\n",
    "def mfi(high, low, close, vol, n=14):\n",
    "    if HAS_TA:\n",
    "        return pd.Series(ta.MFI(_as_float1d(high), _as_float1d(low), _as_float1d(close), _as_float1d(vol),\n",
    "                                timeperiod=n), index=close.index)\n",
    "    tp = (high+low+close)/3\n",
    "    pmf = (tp.diff().clip(lower=0) * vol); nmf = (-tp.diff().clip(upper=0) * vol)\n",
    "    mr = pmf.rolling(n).sum() / nmf.rolling(n).sum()\n",
    "    return 100 - (100/(1+mr))\n",
    "\n",
    "def cci(high, low, close, n=20):\n",
    "    if HAS_TA:\n",
    "        return pd.Series(ta.CCI(_as_float1d(high), _as_float1d(low), _as_float1d(close), timeperiod=n),\n",
    "                         index=close.index)\n",
    "    tp = (high+low+close)/3; sma = tp.rolling(n).mean()\n",
    "    md = (tp - sma).abs().rolling(n).mean()\n",
    "    return (tp - sma) / (0.015 * md)\n",
    "\n",
    "def willr(high, low, close, n=14):\n",
    "    if HAS_TA:\n",
    "        return pd.Series(ta.WILLR(_as_float1d(high), _as_float1d(low), _as_float1d(close), timeperiod=n),\n",
    "                         index=close.index)\n",
    "    ll = low.rolling(n).min(); hh = high.rolling(n).max()\n",
    "    return -100 * (hh - close) / (hh - ll)\n",
    "\n",
    "def obv(close, vol):\n",
    "    if HAS_TA:\n",
    "        return pd.Series(ta.OBV(_as_float1d(close), _as_float1d(vol)), index=close.index)\n",
    "    return (np.sign(close.diff().fillna(0)) * vol).fillna(0).cumsum()\n",
    "\n",
    "# =========================================\n",
    "# PER-SYMBOL PIPELINE\n",
    "# =========================================\n",
    "def fit_symbol_pipeline(ticker: str):\n",
    "    out = {\"ticker\": ticker, \"skipped\": False, \"error\": None}\n",
    "    try:\n",
    "        raw = yf.download(ticker, start=START_DATE, end=END_DATE, auto_adjust=True, progress=False)\n",
    "        if raw is None or raw.empty:\n",
    "            out[\"skipped\"] = True; out[\"error\"] = \"No data\"\n",
    "            return out\n",
    "        df = _select_single_ticker_ohlcv(raw, ticker)\n",
    "        if len(df) < MIN_ROWS_PER_SYMBOL:\n",
    "            out[\"skipped\"] = True; out[\"error\"] = f\"Too few rows (<{MIN_ROWS_PER_SYMBOL})\"\n",
    "            return out\n",
    "\n",
    "        cl, hi, lo, vo = df[\"Close\"], df[\"High\"], df[\"Low\"], df[\"Volume\"]\n",
    "        ret = cl.pct_change()\n",
    "\n",
    "        # --- features (shifted to avoid leakage)\n",
    "        rsi14 = rsi(cl, 14).shift(1)\n",
    "        m_line, m_sig, m_hist = macd(cl); m_line, m_sig, m_hist = m_line.shift(1), m_sig.shift(1), m_hist.shift(1)\n",
    "        sk, sd = stoch(hi, lo, cl); sk, sd = sk.shift(1), sd.shift(1)\n",
    "        bb_up, bb_mid, bb_low = bbands(cl); bb_up, bb_mid, bb_low = bb_up.shift(1), bb_mid.shift(1), bb_low.shift(1)\n",
    "        bb_width = (bb_up - bb_low) / (bb_mid + 1e-12)\n",
    "        adx14 = adx(hi, lo, cl, 14).shift(1)\n",
    "        atr14 = atr(hi, lo, cl, 14).shift(1)\n",
    "        mfi14 = mfi(hi, lo, cl, vo, 14).shift(1)\n",
    "        cci20 = cci(hi, lo, cl, 20).shift(1)\n",
    "        will14 = willr(hi, lo, cl, 14).shift(1)\n",
    "        obv_roc = obv(cl, vo).pct_change().shift(1)\n",
    "\n",
    "        X = pd.DataFrame({\n",
    "            \"ret_1\": ret.shift(1), \"ret_2\": ret.shift(2), \"ret_3\": ret.shift(3),\n",
    "            \"ret_5\": ret.shift(5), \"ret_10\": ret.shift(10),\n",
    "            \"mean_5\": ret.rolling(5).mean().shift(1), \"std_5\": ret.rolling(5).std().shift(1),\n",
    "            \"mean_20\": ret.rolling(20).mean().shift(1), \"std_20\": ret.rolling(20).std().shift(1),\n",
    "            \"rsi14\": rsi14, \"macd\": m_line, \"macd_sig\": m_sig, \"macd_hist\": m_hist,\n",
    "            \"stoch_k\": sk, \"stoch_d\": sd, \"bb_width\": bb_width,\n",
    "            \"adx14\": adx14, \"atr14\": atr14, \"mfi14\": mfi14, \"cci20\": cci20, \"will14\": will14,\n",
    "            \"obv_roc\": obv_roc,\n",
    "            \"dow_0\": (df.index.dayofweek==0).astype(int),\n",
    "            \"dow_1\": (df.index.dayofweek==1).astype(int),\n",
    "            \"dow_2\": (df.index.dayofweek==2).astype(int),\n",
    "            \"dow_3\": (df.index.dayofweek==3).astype(int),\n",
    "            \"dow_4\": (df.index.dayofweek==4).astype(int),\n",
    "        }).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        # --- target & vol scaling\n",
    "        y_next = ret.shift(-1)\n",
    "        vol20  = ret.rolling(20).std()\n",
    "        y      = (y_next / (vol20.shift(1) + 1e-12)) if USE_VOL_SCALE else y_next\n",
    "\n",
    "        data = pd.concat([X, y.rename(\"y\"), y_next.rename(\"y_raw\")], axis=1).dropna()\n",
    "        X, y, y_raw = data[X.columns], data[\"y\"], data[\"y_raw\"]\n",
    "\n",
    "        # --- models\n",
    "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "        base_models = []\n",
    "        base_models.append((\"enet\",  ElasticNet(alpha=0.001, l1_ratio=0.15, max_iter=5000, random_state=0)))\n",
    "        base_models.append((\"rf\",    RandomForestRegressor(n_estimators=500, max_depth=9, min_samples_leaf=5,\n",
    "                                                           random_state=0, n_jobs=-1)))\n",
    "        base_models.append((\"huber\", HuberRegressor(epsilon=1.5, alpha=0.0005)))  # needs scaled X\n",
    "        if HAS_XGB:\n",
    "            base_models.append((\"xgb\", XGBRegressor(\n",
    "                n_estimators=700, max_depth=5, learning_rate=0.03, subsample=0.9, colsample_bytree=0.9,\n",
    "                reg_lambda=6.0, random_state=0, tree_method=\"hist\", n_jobs=-1)))\n",
    "        if HAS_LGBM:\n",
    "            try:\n",
    "                base_models.append((\"lgbm\", LGBMRegressor(\n",
    "                    n_estimators=1000, num_leaves=48, learning_rate=0.02, subsample=0.9,\n",
    "                    colsample_bytree=0.9, reg_lambda=6.0, objective=\"huber\", random_state=0, n_jobs=-1)))\n",
    "            except Exception:\n",
    "                base_models.append((\"lgbm\", LGBMRegressor(\n",
    "                    n_estimators=1000, num_leaves=48, learning_rate=0.02, subsample=0.9,\n",
    "                    colsample_bytree=0.9, reg_lambda=6.0, random_state=0, n_jobs=-1)))\n",
    "\n",
    "        # --- OOF predictions\n",
    "        oof_preds = {name: np.full(len(X), np.nan) for name, _ in base_models}\n",
    "        oof_truth = np.full(len(X), np.nan)\n",
    "\n",
    "        for tr, te in ts_purged_splits(X, n_splits=SPLITS, gap=GAP_DAYS):\n",
    "            Xtr, Xte = X.iloc[tr], X.iloc[te]\n",
    "            ytr, yte = y.iloc[tr], y.iloc[te]\n",
    "            Xtr_s = scaler.fit_transform(Xtr); Xte_s = scaler.transform(Xte)\n",
    "\n",
    "            for name, model in base_models:\n",
    "                if name in (\"enet\", \"huber\"):\n",
    "                    model.fit(Xtr_s, ytr)\n",
    "                    pred = model.predict(Xte_s)\n",
    "                else:\n",
    "                    model.fit(Xtr, ytr)\n",
    "                    pred = model.predict(Xte)\n",
    "                oof_preds[name][te] = pred\n",
    "            oof_truth[te] = yte.values\n",
    "\n",
    "        # --- report per model RMSE (scaled)\n",
    "        rmse_scaled_dict = {}\n",
    "        mask_all = ~np.isnan(oof_truth)\n",
    "        for name, _ in base_models:\n",
    "            m = ~np.isnan(oof_preds[name]) & mask_all\n",
    "            rmse_scaled_dict[name] = RMSE(oof_truth[m], oof_preds[name][m])\n",
    "\n",
    "        # --- stacking\n",
    "        Z = pd.DataFrame({name: oof_preds[name] for name, _ in base_models}, index=X.index)\n",
    "        mask_series = pd.Series(mask_all, index=X.index)\n",
    "        mask = Z.notna().all(axis=1) & mask_series\n",
    "\n",
    "        meta = Ridge(alpha=1.0)\n",
    "        meta.fit(Z.loc[mask].values, oof_truth[mask])\n",
    "        stack_rmse_scaled = RMSE(oof_truth[mask], meta.predict(Z.loc[mask].values))\n",
    "\n",
    "        # --- OOF meta preds in raw return units\n",
    "        oof_meta = pd.Series(np.nan, index=X.index)\n",
    "        valid_idx = mask[mask].index\n",
    "        oof_meta.loc[valid_idx] = meta.predict(Z.loc[valid_idx].values)  # scaled if USE_VOL_SCALE\n",
    "\n",
    "        if USE_VOL_SCALE:\n",
    "            vol_aligned = vol20.reindex(oof_meta.index)\n",
    "            pred_unscaled = oof_meta * vol_aligned\n",
    "        else:\n",
    "            pred_unscaled = oof_meta\n",
    "\n",
    "        rmse_raw = RMSE(y_raw.loc[valid_idx], pred_unscaled.loc[valid_idx])\n",
    "\n",
    "        # --- tuned backtest (per symbol)\n",
    "        def backtest_from_preds(pred_raw, pred_sigma, realized_raw, fees_rt, threshold, thresh_in_sigma,\n",
    "                                use_sizing, size_k, allow_shorts):\n",
    "            idx = pred_raw.index.intersection(realized_raw.index)\n",
    "            pr  = pred_raw.loc[idx].copy()\n",
    "            ps  = pred_sigma.loc[idx].copy()\n",
    "            rr  = realized_raw.loc[idx].copy()\n",
    "\n",
    "            if thresh_in_sigma:\n",
    "                gate_long  = ps >  threshold\n",
    "                gate_short = ps < -threshold if allow_shorts else pd.Series(False, index=idx)\n",
    "            else:\n",
    "                gate_long  = pr >  threshold\n",
    "                gate_short = pr < -threshold if allow_shorts else pd.Series(False, index=idx)\n",
    "\n",
    "            if use_sizing:\n",
    "                pos = np.tanh(ps / (size_k + 1e-12))\n",
    "                if not allow_shorts: pos = pos.clip(lower=0.0)\n",
    "                mask_gate = gate_long | gate_short\n",
    "                pos = pos.where(mask_gate, 0.0)\n",
    "            else:\n",
    "                pos = pd.Series(0.0, index=idx)\n",
    "                pos[gate_long]  =  1.0\n",
    "                if allow_shorts:\n",
    "                    pos[gate_short] = -1.0\n",
    "\n",
    "            turnover = pos.diff().abs().fillna(0.0)\n",
    "            fees = turnover * (fees_rt / 2.0)\n",
    "            pnl = pos.shift(1).fillna(0.0) * rr - fees\n",
    "            equity = (INIT_CAPITAL * (1.0 + pnl.fillna(0.0))).cumprod()\n",
    "\n",
    "            ann = 252.0\n",
    "            daily = pnl.fillna(0.0)\n",
    "            sharpe = (np.sqrt(ann) * daily.mean() / (daily.std() + 1e-12)) if daily.std() > 0 else 0.0\n",
    "            total_ret = equity.iloc[-1] / equity.iloc[0] - 1.0 if len(equity) else 0.0\n",
    "            mdd = max_drawdown(equity)\n",
    "            hit = (np.sign(pr) == np.sign(rr)).mean()\n",
    "            return {\"equity\":equity, \"pnl\":daily, \"total_return\":float(total_ret),\n",
    "                    \"sharpe\":float(sharpe), \"mdd\":float(mdd), \"hit_rate\":float(hit)}\n",
    "\n",
    "        def tune_threshold_and_size(pred_raw, pred_sigma, realized_raw):\n",
    "            best = {\"sharpe\": -9e9}\n",
    "            tg = THRESH_GRID_SIGMA if THRESH_IN_SIGMA else THRESH_GRID_RAW\n",
    "            if not tg: tg = [0.0]\n",
    "            for th in tg:\n",
    "                if USE_POSITION_SIZING:\n",
    "                    for k in SIZE_K_GRID:\n",
    "                        res = backtest_from_preds(pred_raw, oof_meta, realized_raw, FEE_RT,\n",
    "                                                  th, THRESH_IN_SIGMA, True, k, ALLOW_SHORTS)\n",
    "                        if res[\"sharpe\"] > best[\"sharpe\"]:\n",
    "                            best = {\"threshold\": th, \"size_k\": k, **res}\n",
    "                else:\n",
    "                    res = backtest_from_preds(pred_raw, oof_meta, realized_raw, FEE_RT,\n",
    "                                              th, THRESH_IN_SIGMA, False, 1.0, ALLOW_SHORTS)\n",
    "                    if res[\"sharpe\"] > best[\"sharpe\"]:\n",
    "                        best = {\"threshold\": th, \"size_k\": 1.0, **res}\n",
    "            return best\n",
    "\n",
    "        best = tune_threshold_and_size(pred_unscaled.dropna(), oof_meta.dropna(), y_raw)\n",
    "\n",
    "        # --- fit all data & next-day forecast\n",
    "        X_s = scaler.fit_transform(X)\n",
    "        for name, model in base_models:\n",
    "            if name in (\"enet\", \"huber\"): model.fit(X_s, y)\n",
    "            else:                         model.fit(X, y)\n",
    "\n",
    "        last_row_raw = X.iloc[[-1]]\n",
    "        last_row_s   = scaler.transform(last_row_raw)\n",
    "        base_next = []\n",
    "        for name, model in base_models:\n",
    "            if name in (\"enet\", \"huber\"):\n",
    "                base_next.append(model.predict(last_row_s)[0])\n",
    "            else:\n",
    "                base_next.append(model.predict(last_row_raw)[0])\n",
    "\n",
    "        y_hat_scaled = float(meta.predict(np.array(base_next).reshape(1, -1))[0])\n",
    "        if USE_VOL_SCALE:\n",
    "            last_vol = float(vol20.reindex(X.index).iloc[-1])\n",
    "            y_hat = y_hat_scaled * (last_vol if np.isfinite(last_vol) else 0.0)\n",
    "        else:\n",
    "            y_hat = y_hat_scaled\n",
    "\n",
    "        last_close = float(cl.reindex(X.index).iloc[-1])\n",
    "        pred_next_close = last_close * (1 + y_hat)\n",
    "\n",
    "        # --- outputs\n",
    "        out.update({\n",
    "            \"rmse_scaled\": rmse_scaled_dict,\n",
    "            \"stack_rmse_scaled\": stack_rmse_scaled,\n",
    "            \"oof_rmse_raw\": rmse_raw,\n",
    "            \"oof_meta_sigma\": oof_meta,         # sigma-units OOF meta preds\n",
    "            \"oof_pred_raw\": pred_unscaled,      # raw-return OOF preds\n",
    "            \"y_raw\": y_raw,                     # realized raw returns\n",
    "            \"best\": best,                       # tuned params + metrics + equity/pnl\n",
    "            \"forecast\": {\"last_close\": last_close, \"pred_ret\": y_hat, \"pred_close\": pred_next_close},\n",
    "        })\n",
    "\n",
    "        if VERBOSE_PER_SYMBOL:\n",
    "            print(f\"\\n=== {ticker} ===\")\n",
    "            print(\"Scaled OOF RMSE:\", {k:f\"{v:.4f}\" for k,v in rmse_scaled_dict.items()}, \"| stack:\", f\"{stack_rmse_scaled:.4f}\")\n",
    "            print(\"OOF RMSE (raw):\", f\"{rmse_raw:.6f}\")\n",
    "            print(\"Tuned Sharpe:\", f\"{best['sharpe']:.2f}\", \"| TotalRet:\", f\"{best['total_return']*100:.2f}%\",\n",
    "                  \"| MDD:\", f\"{best['mdd']*100:.2f}%\", \"| Hit:\", f\"{best['hit_rate']*100:.2f}%\")\n",
    "            fc = out[\"forecast\"]\n",
    "            print(\"Next-day:\", f\"last={fc['last_close']:.2f}\",\n",
    "                  f\"pred_ret={fc['pred_ret']:.6f}\", f\"pred_close={fc['pred_close']:.2f}\")\n",
    "\n",
    "        return out\n",
    "\n",
    "    except Exception as e:\n",
    "        out[\"skipped\"] = True\n",
    "        out[\"error\"] = str(e)\n",
    "        if VERBOSE_PER_SYMBOL:\n",
    "            print(f\"\\n=== {ticker} === ERROR:\", e)\n",
    "        return out\n",
    "\n",
    "# =========================================\n",
    "# RUN ALL TICKERS\n",
    "# =========================================\n",
    "results = []\n",
    "for tk in TICKERS:\n",
    "    results.append(fit_symbol_pipeline(tk))\n",
    "\n",
    "# Filter good ones\n",
    "good = [r for r in results if not r.get(\"skipped\", False)]\n",
    "if not good:\n",
    "    raise RuntimeError(\"No symbols produced results. Check data / config.\")\n",
    "\n",
    "# =========================================\n",
    "# PORTFOLIO: equal-weight of per-symbol tuned OOF PnL\n",
    "# =========================================\n",
    "# Align daily pnl across symbols (outer join on dates), fill missing with 0\n",
    "pnl_df = pd.DataFrame({r[\"ticker\"]: r[\"best\"][\"pnl\"] for r in good}).fillna(0.0)\n",
    "# Equal-weight average pnl (per-day): keep scaling comparable to single-symbol daily returns\n",
    "port_daily_pnl = pnl_df.mean(axis=1)\n",
    "\n",
    "port_equity = (INIT_CAPITAL * (1.0 + port_daily_pnl)).cumprod()\n",
    "\n",
    "port_total_ret = port_equity.iloc[-1] / port_equity.iloc[0] - 1.0 if len(port_equity) else 0.0\n",
    "ann = 252.0\n",
    "port_sharpe = (np.sqrt(ann) * port_daily_pnl.mean() / (port_daily_pnl.std() + 1e-12)) if port_daily_pnl.std() > 0 else 0.0\n",
    "port_mdd = max_drawdown(port_equity)\n",
    "\n",
    "# Aggregate directional accuracy across all symbols (where they had OOF preds)\n",
    "all_pred = []\n",
    "all_real = []\n",
    "for r in good:\n",
    "    pr = r[\"oof_pred_raw\"].dropna()\n",
    "    rr = r[\"y_raw\"].reindex(pr.index)\n",
    "    all_pred.append(np.sign(pr))\n",
    "    all_real.append(np.sign(rr))\n",
    "if all_pred:\n",
    "    s_pred = pd.concat(all_pred, axis=0)\n",
    "    s_real = pd.concat(all_real, axis=0)\n",
    "    port_hit = float((s_pred == s_real).mean())\n",
    "else:\n",
    "    port_hit = float(\"nan\")\n",
    "\n",
    "print(\"\\n================ PORTFOLIO (Equal-weight OOF) ================\")\n",
    "print(f\"Symbols used: {len(good)} / {len(TICKERS)}\")\n",
    "print(f\"Total return:  {port_total_ret*100:.2f}%\")\n",
    "print(f\"Sharpe (daily): {port_sharpe:.2f}\")\n",
    "print(f\"Max drawdown:   {port_mdd*100:.2f}%\")\n",
    "print(f\"Directional acc: {port_hit*100:.2f}%\")\n",
    "\n",
    "# =========================================\n",
    "# SHOW NEXT-DAY FORECASTS PER SYMBOL\n",
    "# =========================================\n",
    "print(\"\\n================ NEXT-DAY FORECASTS =================\")\n",
    "for r in good:\n",
    "    fc = r[\"forecast\"]\n",
    "    print(f\"{r['ticker']:>10s} | last={fc['last_close']:.2f}  pred_ret={fc['pred_ret']:.6f}  pred_close={fc['pred_close']:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".talib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
