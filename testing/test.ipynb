{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bebcd74",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'response'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     15\u001b[39m     user_input = \u001b[33m\"\u001b[39m\u001b[33mExplain me string theory in simple terms.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     reply = \u001b[43mchat_with_ollama\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOllama: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreply\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mchat_with_ollama\u001b[39m\u001b[34m(prompt, model)\u001b[39m\n\u001b[32m     10\u001b[39m response = requests.post(url, json=payload)\n\u001b[32m     11\u001b[39m data = response.json()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: 'response'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def chat_with_ollama(prompt, model=\"llama3.2:latest\"):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False  # disable streaming\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    data = response.json()\n",
    "    return data[\"response\"]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = \"Explain me string theory in simple terms.\"\n",
    "    reply = chat_with_ollama(user_input)\n",
    "    print(f\"Ollama: {reply}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "974f21ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama: The Golden Cross is a technical analysis indicator used to identify the potential start of a new uptrend in a stock's price. It occurs when the 50-day moving average (MA) crosses above the 200-day MA, indicating that the shorter-term trend has shifted into the long-term trend.\n",
      "\n",
      "Here are some key aspects of the Golden Cross:\n",
      "\n",
      "1. **Signal generation**: The Golden Cross is generated when the 50-day MA crosses above the 200-day MA.\n",
      "2. **Confirmation**: A buy signal is confirmed when the stock price closes above its 50-day MA after the cross-over.\n",
      "3. **Mean reversion**: Some traders believe that a Golden Cross can also be a sign of mean reversion, as it indicates that the short-term trend may reverse and become more aligned with the long-term trend.\n",
      "4. **Uptrend confirmation**: The Golden Cross is considered a bullish indicator, confirming an uptrend and indicating that the stock's price is likely to continue rising.\n",
      "\n",
      "**When to use:**\n",
      "\n",
      "1. **Buy signals**: Use the Golden Cross as a buy signal when the 50-day MA crosses above the 200-day MA.\n",
      "2. **Uptrend continuation**: If you're already in an uptrend, the Golden Cross can be used to confirm that the trend is still intact and likely to continue.\n",
      "\n",
      "**How to trade:**\n",
      "\n",
      "1. **Short-term focus**: The Golden Cross is a short-term indicator, so focus on trading within the 50-day timeframe.\n",
      "2. **Position sizing**: Use position sizing strategies based on your risk tolerance and market conditions.\n",
      "3. **Risk management**: Set stop-loss levels below the 200-day MA to limit potential losses.\n",
      "\n",
      "**Caveats:**\n",
      "\n",
      "1. **False signals**: The Golden Cross can generate false signals, especially in volatile markets or when there's a strong trend already established.\n",
      "2. **Market cycles**: The Golden Cross may not always be effective during market cycles that are characterized by high volatility or rapid price movements.\n",
      "3. **Overemphasis on indicators**: Don't rely solely on the Golden Cross for trading decisions; also consider fundamental analysis, news, and other forms of market research.\n",
      "\n",
      "In conclusion, the Golden Cross is a powerful indicator used to identify potential uptrend reversals and confirmations. However, it's essential to use it in conjunction with other technical and fundamental analysis tools to make informed trading decisions.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def chat_with_ollama(prompt, model=\"llama3.2:latest\"):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": True  # enable streaming\n",
    "    }\n",
    "\n",
    "    with requests.post(url, json=payload, stream=True) as resp:\n",
    "        for line in resp.iter_lines():\n",
    "            if line:  # skip empty lines\n",
    "                data = json.loads(line.decode(\"utf-8\"))\n",
    "                if \"response\" in data:\n",
    "                    yield data[\"response\"]\n",
    "                if data.get(\"done\", False):\n",
    "                    break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = \"Tell me about Golden Cross in trading.\"\n",
    "    print(\"Ollama:\", end=\" \", flush=True)\n",
    "    for chunk in chat_with_ollama(user_input):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22d90a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mn/tx231v5158797chnp22wm9000000gp/T/ipykernel_32212/2714210623.py:6: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3.2:latest\", base_url=\"http://localhost:11434\")\n",
      "/var/folders/mn/tx231v5158797chnp22wm9000000gp/T/ipykernel_32212/2714210623.py:14: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain1 = LLMChain(llm=llm, prompt=prompt1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mHere is a 3-point outline for a blog on Kubernetes autoscaling:\n",
      "\n",
      "I. Introduction to Kubernetes Autoscaling\n",
      "\n",
      "* Brief overview of Kubernetes and its benefits\n",
      "* Explanation of what Kubernetes autoscaling is and its importance in modern cloud-native applications\n",
      "* Discussion of the challenges and limitations of traditional scaling approaches (e.g. static scaling, dynamic scaling)\n",
      "\n",
      "II. Types of Kubernetes Autoscaling Strategies\n",
      "\n",
      "* Horizontal Pod Autoscaling (HPA): a popular approach to automatically scale the number of replicas based on CPU utilization or other metrics\n",
      "* Vertical Pod Autoscaling: a strategy for dynamically adjusting resources allocated to individual pods based on their workload needs\n",
      "* Batch-based Autoscaling: an approach that scales applications in batches, often used for applications with predictable and recurring workloads\n",
      "\n",
      "III. Best Practices for Implementing Kubernetes Autoscaling\n",
      "\n",
      "* Overview of key considerations when implementing autoscaling policies (e.g. monitoring requirements, alert thresholds, resource constraints)\n",
      "* Strategies for tuning HPA settings (e.g. scaling coefficients, scaling steps) to achieve optimal performance\n",
      "* Case studies or examples of successful autoscaling implementations in production environments\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m**The Power of Kubernetes Autoscaling: Scaling Your Cloud-Native Applications with Ease**\n",
      "\n",
      "In the world of cloud-native applications, scalability is no longer just a nice-to-have feature – it's a must-have. With the ever-increasing demand for speed and efficiency comes the need to scale your application quickly and efficiently. This is where Kubernetes autoscaling comes in – a game-changing technology that automates the scaling process, ensuring that your application can handle sudden spikes in traffic or changes in workload demands.\n",
      "\n",
      "In this article, we'll delve into the world of Kubernetes autoscaling, exploring its benefits, types, and best practices for implementation. We'll also examine some real-world case studies to illustrate how successful organizations have harnessed the power of Kubernetes autoscaling.\n",
      "\n",
      "**I. Introduction to Kubernetes Autoscaling**\n",
      "\n",
      "Kubernetes is an open-source container orchestration system that automates the deployment, scaling, and management of containers across a cluster of machines. With its emphasis on scalability, flexibility, and reliability, Kubernetes has quickly become the de facto standard for cloud-native applications. One of the key features of Kubernetes is autoscaling – a technology that allows you to dynamically adjust the number of replicas or resources allocated to your application based on its workload needs.\n",
      "\n",
      "Autoscaling is essential in modern cloud-native applications because it enables them to scale quickly and efficiently in response to changing demands. Traditional scaling approaches, such as static scaling (where all pods are created at once) or dynamic scaling (where new pods are spun up as needed), can lead to inefficient resource utilization and downtime. Kubernetes autoscaling, on the other hand, allows you to fine-tune your application's performance by automatically adjusting the number of replicas or resources allocated to individual pods.\n",
      "\n",
      "**II. Types of Kubernetes Autoscaling Strategies**\n",
      "\n",
      "Kubernetes offers several types of autoscaling strategies, each with its strengths and weaknesses.\n",
      "\n",
      "1. **Horizontal Pod Autoscaling (HPA)**: HPA is a popular approach that automatically scales the number of replicas based on CPU utilization or other metrics. It works by monitoring the CPU usage of individual pods and scaling up or down to maintain a target utilization threshold. HPA is suitable for applications with variable workload demands, such as web servers or databases.\n",
      "2. **Vertical Pod Autoscaling (VPA)**: VPA is a strategy that dynamically adjusts resources allocated to individual pods based on their workload needs. It allows you to specify resource quotas and limits, ensuring that each pod receives the optimal amount of resources to run efficiently. VPA is suitable for applications with consistent and predictable workloads, such as batch processing or scientific simulations.\n",
      "3. **Batch-based Autoscaling**: Batch-based autoscaling is an approach that scales applications in batches, often used for applications with predictable and recurring workloads. It involves creating a batch of new pods at regular intervals to handle incoming traffic spikes. This strategy is suitable for applications with well-defined workload patterns.\n",
      "\n",
      "**III. Best Practices for Implementing Kubernetes Autoscaling**\n",
      "\n",
      "Implementing Kubernetes autoscaling effectively requires careful consideration of several key factors.\n",
      "\n",
      "1. **Monitoring Requirements**: To implement effective autoscaling, you need to have robust monitoring in place to track metrics such as CPU utilization, memory usage, and latency.\n",
      "2. **Alert Thresholds**: Setting appropriate alert thresholds is crucial to ensure that your application scales correctly. You need to strike a balance between scaling too aggressively (leading to underutilization) or not enough (resulting in overprovisioning).\n",
      "3. **Resource Constraints**: When implementing autoscaling policies, it's essential to define clear resource constraints to prevent overprovisioning or underutilization of resources.\n",
      "4. **Tuning HPA Settings**: Tuning HPA settings, such as scaling coefficients and scaling steps, is critical to achieving optimal performance. You may need to experiment with different settings to find the sweet spot for your application.\n",
      "\n",
      "**Case Studies: Successful Kubernetes Autoscaling Implementations**\n",
      "\n",
      "Several organizations have successfully implemented Kubernetes autoscaling in production environments. For example:\n",
      "\n",
      "* A web server company scaled its load balancer using HPA to handle a sudden increase in traffic, resulting in a 50% reduction in latency and a 30% decrease in costs.\n",
      "* A cloud provider used VPA to optimize the resource allocation for its containerized applications, achieving an average utilization of 80% across all pods.\n",
      "\n",
      "In conclusion, Kubernetes autoscaling is a powerful technology that can help you scale your cloud-native application quickly and efficiently. By understanding the different types of scaling strategies available, implementing best practices for autoscaling policies, and experimenting with real-world case studies, you'll be well on your way to harnessing the full potential of Kubernetes autoscaling.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Kubernetes autoscaling is a game-changer in the world of cloud-native applications. With its ability to automatically scale your application based on workload demands, it can help reduce costs, improve performance, and increase efficiency. By understanding the different types of scaling strategies available, implementing best practices for autoscaling policies, and experimenting with real-world case studies, you'll be able to unlock the full potential of Kubernetes autoscaling.\n",
      "\n",
      "So why wait? Dive into the world of Kubernetes autoscaling today and discover how it can transform your cloud-native applications!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Kubernetes autoscaling', 'output': \"**The Power of Kubernetes Autoscaling: Scaling Your Cloud-Native Applications with Ease**\\n\\nIn the world of cloud-native applications, scalability is no longer just a nice-to-have feature – it's a must-have. With the ever-increasing demand for speed and efficiency comes the need to scale your application quickly and efficiently. This is where Kubernetes autoscaling comes in – a game-changing technology that automates the scaling process, ensuring that your application can handle sudden spikes in traffic or changes in workload demands.\\n\\nIn this article, we'll delve into the world of Kubernetes autoscaling, exploring its benefits, types, and best practices for implementation. We'll also examine some real-world case studies to illustrate how successful organizations have harnessed the power of Kubernetes autoscaling.\\n\\n**I. Introduction to Kubernetes Autoscaling**\\n\\nKubernetes is an open-source container orchestration system that automates the deployment, scaling, and management of containers across a cluster of machines. With its emphasis on scalability, flexibility, and reliability, Kubernetes has quickly become the de facto standard for cloud-native applications. One of the key features of Kubernetes is autoscaling – a technology that allows you to dynamically adjust the number of replicas or resources allocated to your application based on its workload needs.\\n\\nAutoscaling is essential in modern cloud-native applications because it enables them to scale quickly and efficiently in response to changing demands. Traditional scaling approaches, such as static scaling (where all pods are created at once) or dynamic scaling (where new pods are spun up as needed), can lead to inefficient resource utilization and downtime. Kubernetes autoscaling, on the other hand, allows you to fine-tune your application's performance by automatically adjusting the number of replicas or resources allocated to individual pods.\\n\\n**II. Types of Kubernetes Autoscaling Strategies**\\n\\nKubernetes offers several types of autoscaling strategies, each with its strengths and weaknesses.\\n\\n1. **Horizontal Pod Autoscaling (HPA)**: HPA is a popular approach that automatically scales the number of replicas based on CPU utilization or other metrics. It works by monitoring the CPU usage of individual pods and scaling up or down to maintain a target utilization threshold. HPA is suitable for applications with variable workload demands, such as web servers or databases.\\n2. **Vertical Pod Autoscaling (VPA)**: VPA is a strategy that dynamically adjusts resources allocated to individual pods based on their workload needs. It allows you to specify resource quotas and limits, ensuring that each pod receives the optimal amount of resources to run efficiently. VPA is suitable for applications with consistent and predictable workloads, such as batch processing or scientific simulations.\\n3. **Batch-based Autoscaling**: Batch-based autoscaling is an approach that scales applications in batches, often used for applications with predictable and recurring workloads. It involves creating a batch of new pods at regular intervals to handle incoming traffic spikes. This strategy is suitable for applications with well-defined workload patterns.\\n\\n**III. Best Practices for Implementing Kubernetes Autoscaling**\\n\\nImplementing Kubernetes autoscaling effectively requires careful consideration of several key factors.\\n\\n1. **Monitoring Requirements**: To implement effective autoscaling, you need to have robust monitoring in place to track metrics such as CPU utilization, memory usage, and latency.\\n2. **Alert Thresholds**: Setting appropriate alert thresholds is crucial to ensure that your application scales correctly. You need to strike a balance between scaling too aggressively (leading to underutilization) or not enough (resulting in overprovisioning).\\n3. **Resource Constraints**: When implementing autoscaling policies, it's essential to define clear resource constraints to prevent overprovisioning or underutilization of resources.\\n4. **Tuning HPA Settings**: Tuning HPA settings, such as scaling coefficients and scaling steps, is critical to achieving optimal performance. You may need to experiment with different settings to find the sweet spot for your application.\\n\\n**Case Studies: Successful Kubernetes Autoscaling Implementations**\\n\\nSeveral organizations have successfully implemented Kubernetes autoscaling in production environments. For example:\\n\\n* A web server company scaled its load balancer using HPA to handle a sudden increase in traffic, resulting in a 50% reduction in latency and a 30% decrease in costs.\\n* A cloud provider used VPA to optimize the resource allocation for its containerized applications, achieving an average utilization of 80% across all pods.\\n\\nIn conclusion, Kubernetes autoscaling is a powerful technology that can help you scale your cloud-native application quickly and efficiently. By understanding the different types of scaling strategies available, implementing best practices for autoscaling policies, and experimenting with real-world case studies, you'll be well on your way to harnessing the full potential of Kubernetes autoscaling.\\n\\n**Conclusion**\\n\\nKubernetes autoscaling is a game-changer in the world of cloud-native applications. With its ability to automatically scale your application based on workload demands, it can help reduce costs, improve performance, and increase efficiency. By understanding the different types of scaling strategies available, implementing best practices for autoscaling policies, and experimenting with real-world case studies, you'll be able to unlock the full potential of Kubernetes autoscaling.\\n\\nSo why wait? Dive into the world of Kubernetes autoscaling today and discover how it can transform your cloud-native applications!\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "# 1. Define your LLM (Ollama backend)\n",
    "llm = Ollama(model=\"llama3.2:latest\", base_url=\"http://localhost:11434\")  \n",
    "# If running in K8s with ingress: base_url=\"http://your-ollama-service\"\n",
    "\n",
    "# 2. Chain 1: Generate outline\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables=[\"subject\"],\n",
    "    template=\"Create a 3-point outline for a blog on {subject}.\"\n",
    ")\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1)\n",
    "\n",
    "# 3. Chain 2: Expand outline into a blog\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"outline\"],\n",
    "    template=\"Write a blog article based on this outline:\\n{outline}\"\n",
    ")\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2)\n",
    "\n",
    "# 4. Combine into sequential chain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "\n",
    "# 5. Run the chain\n",
    "print(overall_chain.invoke(\"Kubernetes autoscaling\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".talib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
