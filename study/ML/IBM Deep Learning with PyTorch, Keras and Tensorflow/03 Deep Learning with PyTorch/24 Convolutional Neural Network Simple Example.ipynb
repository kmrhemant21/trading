{"cells":[{"cell_type":"markdown","id":"880dc2bb-330e-4cfa-acd4-57a1b66ddb75","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n","<h1 align=center><font size = 5>Convolutional Neural Network Simple example </font></h1> \n"]},{"cell_type":"markdown","id":"889c55a7-e01e-476e-87b1-2df4ebde8053","metadata":{},"outputs":[],"source":["\n","<h3>Objective for this Notebook<h3>    \n","<h5> 1. Learn Convolutional Neural Network</h5>\n","<h5> 2. Define Softmax, Criterion function, Optimizer and Train the  Model</h5>    \n","\n"]},{"cell_type":"markdown","id":"1f7abefb-05ce-4d91-93d8-03169be6295d","metadata":{},"outputs":[],"source":["\n","# Table of Contents\n","In this lab, we will use a Convolutional Neural Networks to classify horizontal an vertical Lines \n","\n","<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","<li><a href=\"#ref0\">Helper functions </a></li>\n","<li><a href=\"#ref1\"> Prepare Data </a></li>\n","<li><a href=\"#ref2\">Build a Convolutional Neural Network Class </a></li>\n","<li><a href=\"#ref3\">Define the Convolutional Neural Network Classifier, Criterion function, Optimizer and Train the  Model</a></li>\n","<li><a href=\"#ref4\">Analyse Results</a></li>\n","\n","<br>\n","<p></p>\n","Estimated Time Needed: <strong>25 min</strong>\n","</div>\n","\n","<hr>\n"]},{"cell_type":"markdown","id":"c455ab50-c178-40b6-8eb7-e98f0e7210ec","metadata":{},"outputs":[],"source":["<a id=\"ref0\"></a>\n","<a name=\"ref0\"><h2 align=center>Helper functions </h2></a>\n"]},{"cell_type":"code","id":"46525c3c-7f0a-4967-abca-4a6b8cc1253e","metadata":{},"outputs":[],"source":["%%time\n%pip install pandas numpy matplotlib\n%pip install torch==2.8.0+cpu torchvision==0.23.0+cpu torchaudio==2.8.0+cpu \\\n    --index-url https://download.pytorch.org/whl/cpu"]},{"cell_type":"code","id":"02c662fd-4af9-4772-a2bd-8b11f6779f2c","metadata":{},"outputs":[],"source":["import torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport matplotlib.pylab as plt\nimport numpy as np\nimport pandas as pd"]},{"cell_type":"code","id":"f7f69bdd-aae2-4fd0-958f-bfc8684e6de7","metadata":{},"outputs":[],"source":["torch.manual_seed(4)"]},{"cell_type":"markdown","id":"511b570e-7959-4e51-a784-dea5120fe45e","metadata":{},"outputs":[],"source":["function to plot out the parameters of the Convolutional layers  \n"]},{"cell_type":"code","id":"95aa3414-c5e1-48ea-a079-223aad569725","metadata":{},"outputs":[],"source":["def plot_channels(W):\n    #number of output channels \n    n_out=W.shape[0]\n    #number of input channels \n    n_in=W.shape[1]\n    w_min=W.min().item()\n    w_max=W.max().item()\n    fig, axes = plt.subplots(n_out,n_in)\n    fig.subplots_adjust(hspace = 0.1)\n    out_index=0\n    in_index=0\n    #plot outputs as rows inputs as columns \n    for ax in axes.flat:\n    \n        if in_index>n_in-1:\n            out_index=out_index+1\n            in_index=0\n              \n        ax.imshow(W[out_index,in_index,:,:], vmin=w_min, vmax=w_max, cmap='seismic')\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])\n        in_index=in_index+1\n\n    plt.show()"]},{"cell_type":"markdown","id":"e070017e-a95d-4a63-b5c6-21044504385d","metadata":{},"outputs":[],"source":["<code>show_data</code>: plot out data sample\n"]},{"cell_type":"code","id":"425c9a71-8763-4faa-8eeb-e6eae6a79ff8","metadata":{},"outputs":[],"source":["def show_data(dataset,sample):\n\n    plt.imshow(dataset.x[sample,0,:,:].numpy(),cmap='gray')\n    plt.title('y='+str(dataset.y[sample].item()))\n    plt.show()"]},{"cell_type":"markdown","id":"fe963f14-2f41-45f2-9dbd-36c592503f46","metadata":{},"outputs":[],"source":["create some toy data \n"]},{"cell_type":"code","id":"348658b2-36de-469d-a63f-08179749529f","metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\nclass Data(Dataset):\n    def __init__(self,N_images=100,offset=0,p=0.9, train=False):\n        \"\"\"\n        p:portability that pixel is wight  \n        N_images:number of images \n        offset:set a random vertical and horizontal offset images by a sample should be less than 3 \n        \"\"\"\n        if train==True:\n            np.random.seed(1)  \n        \n        #make images multiple of 3 \n        N_images=2*(N_images//2)\n        images=np.zeros((N_images,1,11,11))\n        start1=3\n        start2=1\n        self.y=torch.zeros(N_images).type(torch.long)\n\n        for n in range(N_images):\n            if offset>0:\n        \n                low=int(np.random.randint(low=start1, high=start1+offset, size=1))\n                high=int(np.random.randint(low=start2, high=start2+offset, size=1))\n            else:\n                low=4\n                high=1\n        \n            if n<=N_images//2:\n                self.y[n]=0\n                images[n,0,high:high+9,low:low+3]= np.random.binomial(1, p, (9,3))\n            elif  n>N_images//2:\n                self.y[n]=1\n                images[n,0,low:low+3,high:high+9] = np.random.binomial(1, p, (3,9))\n           \n        \n        \n        self.x=torch.from_numpy(images).type(torch.FloatTensor)\n        self.len=self.x.shape[0]\n        del(images)\n        np.random.seed(0)\n    def __getitem__(self,index):      \n        return self.x[index],self.y[index]\n    def __len__(self):\n        return self.len"]},{"cell_type":"markdown","id":"ec76d84f-0acb-4a99-bd6d-f2275e7f9798","metadata":{},"outputs":[],"source":["<code>plot_activation</code>: plot out the activations of the Convolutional layers  \n"]},{"cell_type":"code","id":"13303903-60b3-4270-a680-e34aa44b20c1","metadata":{},"outputs":[],"source":["def plot_activations(A,number_rows= 1,name=\"\"):\n    A=A[0,:,:,:].detach().numpy()\n    n_activations=A.shape[0]\n    \n    \n    print(n_activations)\n    A_min=A.min().item()\n    A_max=A.max().item()\n\n    if n_activations==1:\n\n        # Plot the image.\n        plt.imshow(A[0,:], vmin=A_min, vmax=A_max, cmap='seismic')\n\n    else:\n        fig, axes = plt.subplots(number_rows, n_activations//number_rows)\n        fig.subplots_adjust(hspace = 0.4)\n        for i,ax in enumerate(axes.flat):\n            if i< n_activations:\n                # Set the label for the sub-plot.\n                ax.set_xlabel( \"activation:{0}\".format(i+1))\n\n                # Plot the image.\n                ax.imshow(A[i,:], vmin=A_min, vmax=A_max, cmap='seismic')\n                ax.set_xticks([])\n                ax.set_yticks([])\n    plt.show()"]},{"cell_type":"markdown","id":"5a9b24f0-65c6-4d31-b915-be01845b856e","metadata":{},"outputs":[],"source":["\n","Utility function for computing output of convolutions\n","takes a tuple of (h,w) and returns a tuple of (h,w)\n"]},{"cell_type":"code","id":"957599f0-1a14-44c8-8676-6d3caaf9b2de","metadata":{},"outputs":[],"source":["\ndef conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n    #by Duane Nielsen\n    from math import floor\n    if type(kernel_size) is not tuple:\n        kernel_size = (kernel_size, kernel_size)\n    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n    return h, w"]},{"cell_type":"markdown","id":"702ce08a-2e69-4e16-93ea-01023f28104b","metadata":{},"outputs":[],"source":["<a id=\"ref1\"></a>\n","<a name=\"ref1\"><h2 align=center>Prepare Data </h2></a>\n"]},{"cell_type":"markdown","id":"0a5fbced-0359-4d12-adc7-577a4011216f","metadata":{},"outputs":[],"source":["Load the training dataset with 10000 samples \n"]},{"cell_type":"code","id":"62c5816b-862c-4c9e-950e-9d75dd6196a4","metadata":{},"outputs":[],"source":["N_images=10000\ntrain_dataset=Data(N_images=N_images)"]},{"cell_type":"markdown","id":"22bbc731-cabc-48fd-894e-d766a59755b1","metadata":{},"outputs":[],"source":["Load the testing dataset\n"]},{"cell_type":"code","id":"83cda8e0-bace-4ae3-906f-51b6cf2d73d5","metadata":{},"outputs":[],"source":["validation_dataset=Data(N_images=1000,train=False)\nvalidation_dataset"]},{"cell_type":"markdown","id":"02495678-30ac-47f0-8b23-3676da4ce73c","metadata":{},"outputs":[],"source":["we can see the data type is long \n"]},{"cell_type":"markdown","id":"756398bb-f5a9-4ce9-82dc-44e9223cbb7b","metadata":{},"outputs":[],"source":["### Data Visualization \n"]},{"cell_type":"markdown","id":"042c5af0-e03d-4ed4-9f24-d89198d7bb68","metadata":{},"outputs":[],"source":["Each element in the rectangular  tensor corresponds to a number representing a pixel intensity  as demonstrated by  the following image.\n"]},{"cell_type":"markdown","id":"901a233a-9cf1-45ff-8654-307562fb9f3a","metadata":{},"outputs":[],"source":["We can print out the third label \n"]},{"cell_type":"code","id":"b5d430f7-cb58-48e6-b604-f9884577ba4a","metadata":{},"outputs":[],"source":["show_data(train_dataset,0)"]},{"cell_type":"code","id":"029d44fc-2ced-443b-a889-4b37a84e3c45","metadata":{},"outputs":[],"source":["show_data(train_dataset,N_images//2+2)"]},{"cell_type":"markdown","id":"1483136f-c2ce-4b8d-80b8-31d3ca3981ac","metadata":{},"outputs":[],"source":["we can plot the 3rd  sample \n"]},{"cell_type":"markdown","id":"65117ecb-20e9-4874-aa15-3d1026d355c3","metadata":{},"outputs":[],"source":["<a id=\"ref2\"></a>\n","<a name=\"ref2\"><h2 align=center>Build a Convolutional Neural Network Class </h2></a> \n","\n"]},{"cell_type":"markdown","id":"8c931ab6-068a-448c-a96b-3aefd00eaf0a","metadata":{},"outputs":[],"source":["The input image is 11 x11, the following will change the size of the activations:\n","<ul>\n","<il>convolutional layer</il> \n","</ul>\n","<ul>\n","<il>max pooling layer</il> \n","</ul>\n","<ul>\n","<il>convolutional layer </il>\n","</ul>\n","<ul>\n","<il>max pooling layer </il>\n","</ul>\n","\n","with the following parameters <code>kernel_size</code>, <code>stride</code> and <code> pad</code>.\n","We use the following  lines of code to change the image before we get tot he fully connected layer \n"]},{"cell_type":"code","id":"509e2d98-ed08-4c56-872b-11241b4ae3dc","metadata":{},"outputs":[],"source":["out=conv_output_shape((11,11), kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out)\nout1=conv_output_shape(out, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out1)\nout2=conv_output_shape(out1, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out2)\n\nout3=conv_output_shape(out2, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out3)"]},{"cell_type":"markdown","id":"05c8c8b5-ef72-4067-996a-0089e0231e3f","metadata":{},"outputs":[],"source":["Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.\n"]},{"cell_type":"code","id":"039e6daf-b2a7-4706-91d2-67699e49b4e7","metadata":{},"outputs":[],"source":["class CNN(nn.Module):\n    def __init__(self,out_1=2,out_2=1):\n        \n        super(CNN,self).__init__()\n        #first Convolutional layers \n        self.cnn1=nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0)\n        self.maxpool1=nn.MaxPool2d(kernel_size=2 ,stride=1)\n\n        #second Convolutional layers\n        self.cnn2=nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0)\n        self.maxpool2=nn.MaxPool2d(kernel_size=2 ,stride=1)\n        #max pooling \n\n        #fully connected layer \n        self.fc1=nn.Linear(out_2*7*7,2)\n        \n    def forward(self,x):\n        #first Convolutional layers\n        x=self.cnn1(x)\n        #activation function \n        x=torch.relu(x)\n        #max pooling \n        x=self.maxpool1(x)\n        #first Convolutional layers\n        x=self.cnn2(x)\n        #activation function\n        x=torch.relu(x)\n        #max pooling\n        x=self.maxpool2(x)\n        #flatten output \n        x=x.view(x.size(0),-1)\n        #fully connected layer\n        x=self.fc1(x)\n        return x\n    \n    def activations(self,x):\n        #outputs activation this is not necessary just for fun \n        z1=self.cnn1(x)\n        a1=torch.relu(z1)\n        out=self.maxpool1(a1)\n        \n        z2=self.cnn2(out)\n        a2=torch.relu(z2)\n        out=self.maxpool2(a2)\n        out=out.view(out.size(0),-1)\n        return z1,a1,z2,a2,out        "]},{"cell_type":"markdown","id":"719a602d-2208-4938-99bc-d9c594f2a33c","metadata":{},"outputs":[],"source":["<a id=\"ref3\"></a>\n","<a name=\"ref3\"><h2> Define the Convolutional Neural Network Classifier, Criterion function, Optimizer and Train the  Model</h2></a> \n"]},{"cell_type":"markdown","id":"f4fd25f7-33d5-430a-b227-d095102df79b","metadata":{},"outputs":[],"source":["There are 2 output channels for the first layer, and 1 outputs channel for the second layer \n"]},{"cell_type":"code","id":"d4bd6fa6-0a82-4802-b7d8-989e0af662a8","metadata":{},"outputs":[],"source":["model=CNN(2,1)"]},{"cell_type":"markdown","id":"9c67438e-1011-4b93-833b-e8bc8c104a2e","metadata":{},"outputs":[],"source":["we can see the model parameters with the object \n"]},{"cell_type":"code","id":"532f36ce-f354-475f-8b61-dbd811925e00","metadata":{},"outputs":[],"source":["model"]},{"cell_type":"markdown","id":"a0a82303-a197-4d15-bb18-06d57a450190","metadata":{},"outputs":[],"source":["Plot the model parameters for the kernels before training the kernels. The kernels are initialized randomly.\n"]},{"cell_type":"code","id":"11d59799-29e2-444e-b6d2-cafc09ae44de","metadata":{},"outputs":[],"source":["\nplot_channels(model.state_dict()['cnn1.weight'])\n"]},{"cell_type":"markdown","id":"8dd1cc8c-c583-4baa-be96-7bc41d707fbe","metadata":{},"outputs":[],"source":["Loss function \n"]},{"cell_type":"code","id":"0182d28a-3202-4c5a-a121-a924dbab59da","metadata":{},"outputs":[],"source":["plot_channels(model.state_dict()['cnn2.weight'])"]},{"cell_type":"markdown","id":"aa2907f5-7fff-4e55-b67a-530465ba4e7b","metadata":{},"outputs":[],"source":["Define the loss function \n"]},{"cell_type":"code","id":"235a75a1-a58b-41cc-9880-b9f6460b4663","metadata":{},"outputs":[],"source":["criterion=nn.CrossEntropyLoss()"]},{"cell_type":"markdown","id":"10650bed-e7a1-448d-a8e8-dc6db935018b","metadata":{},"outputs":[],"source":[" optimizer class \n"]},{"cell_type":"code","id":"140bf2a6-d002-4045-851e-eb3ed945cd94","metadata":{},"outputs":[],"source":["learning_rate=0.001\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"markdown","id":"72ad174f-6de5-48c2-b82a-b7f759e0cbe9","metadata":{},"outputs":[],"source":["Define the optimizer class \n"]},{"cell_type":"code","id":"e72829d6-e622-4af9-a496-3bde62d37f4a","metadata":{},"outputs":[],"source":["\ntrain_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=10)\nvalidation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=20)"]},{"cell_type":"markdown","id":"ee44be12-ad96-4d2f-9524-f52a2deb864d","metadata":{},"outputs":[],"source":["Train the model and determine validation accuracy technically test accuracy **(This may take a long time)**\n"]},{"cell_type":"code","id":"d7cda38f-8a2f-4911-afd9-6b8ac7f7a344","metadata":{},"outputs":[],"source":["n_epochs=10\ncost_list=[]\naccuracy_list=[]\nN_test=len(validation_dataset)\ncost=0\n#n_epochs\nfor epoch in range(n_epochs):\n    cost=0    \n    for x, y in train_loader:\n      \n\n        #clear gradient \n        optimizer.zero_grad()\n        #make a prediction \n        z=model(x)\n        # calculate loss \n        loss=criterion(z,y)\n        # calculate gradients of parameters \n        loss.backward()\n        # update parameters \n        optimizer.step()\n        cost+=loss.item()\n    cost_list.append(cost)\n        \n        \n    correct=0\n    #perform a prediction on the validation  data  \n    for x_test, y_test in validation_loader:\n\n        z=model(x_test)\n        _,yhat=torch.max(z.data,1)\n\n        correct+=(yhat==y_test).sum().item()\n        \n\n    accuracy=correct/N_test\n\n    accuracy_list.append(accuracy)\n    \n\n"]},{"cell_type":"markdown","id":"a9fa634a-761a-4940-85ac-4406041624c0","metadata":{},"outputs":[],"source":["#### <a id=\"ref4\"></a>\n","<a name=\"ref4\"><h2 align=center>Analyse Results</h2></a>\n"]},{"cell_type":"markdown","id":"e2583cec-9c5c-42a3-a9bd-83c6fb685757","metadata":{},"outputs":[],"source":["Plot the loss and accuracy on the validation data:\n"]},{"cell_type":"code","id":"803af4cc-fa51-4fe4-afab-02ea41bd665f","metadata":{},"outputs":[],"source":["fig, ax1 = plt.subplots()\ncolor = 'tab:red'\nax1.plot(cost_list,color=color)\nax1.set_xlabel('epoch',color=color)\nax1.set_ylabel('total loss',color=color)\nax1.tick_params(axis='y', color=color)\n    \nax2 = ax1.twinx()  \ncolor = 'tab:blue'\nax2.set_ylabel('accuracy', color=color)  \nax2.plot( accuracy_list, color=color)\nax2.tick_params(axis='y', labelcolor=color)\nfig.tight_layout()"]},{"cell_type":"markdown","id":"8b180569-7f16-4368-90e8-dc88a05f71b2","metadata":{},"outputs":[],"source":["View the results of the parameters for the Convolutional layers \n"]},{"cell_type":"code","id":"ae3d45c8-d08f-4bd9-b055-588dd020141b","metadata":{},"outputs":[],"source":["model.state_dict()['cnn1.weight']"]},{"cell_type":"code","id":"882fe295-3c93-412e-962e-ef026ed25799","metadata":{},"outputs":[],"source":["plot_channels(model.state_dict()['cnn1.weight'])"]},{"cell_type":"code","id":"1f8bfe42-6d35-41e5-be31-a0e839458316","metadata":{},"outputs":[],"source":["model.state_dict()['cnn1.weight']"]},{"cell_type":"code","id":"2f7e0ed6-d72c-4711-a853-efd6ff528e63","metadata":{},"outputs":[],"source":["plot_channels(model.state_dict()['cnn2.weight'])"]},{"cell_type":"markdown","id":"7f9e061d-611d-4fb5-8d02-21600d2f241d","metadata":{},"outputs":[],"source":["Consider the following sample \n"]},{"cell_type":"code","id":"41bfc883-abd8-4dea-888a-243e8a585a87","metadata":{},"outputs":[],"source":["show_data(train_dataset,N_images//2+2)"]},{"cell_type":"markdown","id":"846b91b9-9471-418a-9dbe-ec5a604eedc8","metadata":{},"outputs":[],"source":["Determine the activations \n"]},{"cell_type":"code","id":"01cdc6a1-c5ed-4532-8a1e-bd0b7808b4f5","metadata":{},"outputs":[],"source":["out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))\nout=model.activations(train_dataset[0][0].view(1,1,11,11))"]},{"cell_type":"markdown","id":"a2fd6134-6977-41f7-b77d-05514bcbd241","metadata":{},"outputs":[],"source":["Plot them out\n"]},{"cell_type":"code","id":"6169eae8-fdd4-4739-b9b7-900abbacaf53","metadata":{},"outputs":[],"source":["plot_activations(out[0],number_rows=1,name=\" feature map\")\nplt.show()\n"]},{"cell_type":"code","id":"f98b3c58-8bea-42f8-9ffa-c9d31b8f1da8","metadata":{},"outputs":[],"source":["plot_activations(out[2],number_rows=1,name=\"2nd feature map\")\nplt.show()"]},{"cell_type":"code","id":"837e73a8-7520-4c1b-8653-86274163b70f","metadata":{},"outputs":[],"source":["plot_activations(out[3],number_rows=1,name=\"first feature map\")\nplt.show()"]},{"cell_type":"markdown","id":"6f217c1e-adbc-45d4-a67d-f36f6a7cfa71","metadata":{},"outputs":[],"source":["we save the output of the activation after flattening  \n"]},{"cell_type":"code","id":"766a49ce-021e-4537-96e1-c5e53b07e005","metadata":{},"outputs":[],"source":["out1=out[4][0].detach().numpy()"]},{"cell_type":"markdown","id":"8cbed8a0-2023-4d2d-9b9a-3cb29cae2f91","metadata":{},"outputs":[],"source":["we can do the same for a sample  where y=0 \n"]},{"cell_type":"code","id":"61c5905d-b4c2-4174-8cb7-488a1eb0ef22","metadata":{},"outputs":[],"source":["out0=model.activations(train_dataset[100][0].view(1,1,11,11))[4][0].detach().numpy()\nout0"]},{"cell_type":"code","id":"e6b0415c-0ea2-4f41-a125-09d44cd31923","metadata":{},"outputs":[],"source":["plt.subplot(2, 1, 1)\nplt.plot( out1, 'b')\nplt.title('Flatted Activation Values  ')\nplt.ylabel('Activation')\nplt.xlabel('index')\nplt.subplot(2, 1, 2)\nplt.plot(out0, 'r')\nplt.xlabel('index')\nplt.ylabel('Activation')"]},{"cell_type":"markdown","id":"cd367325-33b8-4d2a-bd3e-30184a2975a8","metadata":{},"outputs":[],"source":["### About the Authors:  \n","[Joseph Santarcangelo]( https://www.linkedin.com/in/joseph-s-50398b136/) has a PhD in Electrical Engineering. His research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. \n","\n","Other contributors: [Michelle Carey](  https://www.linkedin.com/in/michelleccarey/) \n"]},{"cell_type":"markdown","id":"b2391d69-57ca-43e5-bdeb-4a6ab5b2f757","metadata":{},"outputs":[],"source":["<!--\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-23  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","\n","\n","\n","<hr>\n","-->\n","\n","## <h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"prev_pub_hash":"7329aee370a5d6e3b9b10670b2cdf6ceeb0fe579f7ecd9ca718dc28f15b58d68"},"nbformat":4,"nbformat_minor":4}