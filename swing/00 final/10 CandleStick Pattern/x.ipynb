{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e13f700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 19:01:04 | INFO | Loaded 5 tickers (after cleaning)\n",
      "2025-11-02 19:01:04 | INFO | Bulk downloading with vectorbt.YFData (drop mismatches) ...\n",
      "2025-11-02 19:01:06 | INFO | Per-symbol fallback via yfinance for 5 tickers ...\n",
      "2025-11-02 19:01:36 | WARNING | Dropped 5 tickers (bad/short/missing): RELIANCE.NS, HDFCBANK.NS, INFY.NS, TCS.NS, ICICIBANK.NS\n",
      "2025-11-02 19:01:36 | ERROR | No valid symbols after cleaning. Using SYNTHETIC OHLC (GBM) so you can run end-to-end.\n",
      "2025-11-02 19:01:36 | INFO | Final universe: 5 symbols | Date range: 2023-07-17 -> 2025-10-31 | Points: 600\n",
      "2025-11-02 19:01:37 | INFO | Building vectorbt portfolio...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Portfolio.__init__() got an unexpected keyword argument 'weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 521\u001b[39m\n\u001b[32m    518\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCFG\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 485\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m    482\u001b[39m     weights.loc[dt, cols_today] = w * cfg.deploy_cash_frac\n\u001b[32m    484\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33mBuilding vectorbt portfolio...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m pf = \u001b[43mvbt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPortfolio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_signals\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mClose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mentries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mentries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43msl_stop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstop_loss_pct\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstop_loss_pct\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtp_stop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtarget_pct\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtarget_pct\u001b[49m\u001b[43m    \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfees\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfees\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mslippage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mslippage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_cash\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1_000_000.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcash_sharing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m1D\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m    497\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    499\u001b[39m stats = pf.stats()\n\u001b[32m    500\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Portfolio Stats ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/.talib/lib/python3.12/site-packages/vectorbt/portfolio/base.py:3012\u001b[39m, in \u001b[36mPortfolio.from_signals\u001b[39m\u001b[34m(cls, close, entries, exits, short_entries, short_exits, signal_func_nb, signal_args, size, size_type, price, fees, fixed_fees, slippage, min_size, max_size, size_granularity, reject_prob, lock_cash, allow_partial, raise_reject, log, accumulate, upon_long_conflict, upon_short_conflict, upon_dir_conflict, upon_opposite_entry, direction, val_price, open, high, low, sl_stop, sl_trail, tp_stop, stop_entry_price, stop_exit_price, upon_stop_exit, upon_stop_update, adjust_sl_func_nb, adjust_sl_args, adjust_tp_func_nb, adjust_tp_args, use_stops, init_cash, cash_sharing, call_seq, ffill_val_price, update_value, max_orders, max_logs, seed, group_by, broadcast_named_args, broadcast_kwargs, template_mapping, wrapper_kwargs, freq, attach_call_seq, **kwargs)\u001b[39m\n\u001b[32m   2960\u001b[39m order_records, log_records = nb.simulate_from_signal_func_nb(\n\u001b[32m   2961\u001b[39m     target_shape_2d,\n\u001b[32m   2962\u001b[39m     cs_group_lens,  \u001b[38;5;66;03m# group only if cash sharing is enabled to speed up\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3008\u001b[39m     flex_2d=close.ndim == \u001b[32m2\u001b[39m\n\u001b[32m   3009\u001b[39m )\n\u001b[32m   3011\u001b[39m \u001b[38;5;66;03m# Create an instance\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3012\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   3013\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3014\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3015\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder_records\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3016\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_records\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3017\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_cash\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_cash_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_cash_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3018\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcash_sharing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3019\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcall_seq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcall_seq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattach_call_seq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3020\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   3021\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: Portfolio.__init__() got an unexpected keyword argument 'weights'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "VectorBT – Bullish Candle + (Optional) RSI – Robust Loader with Synthetic Fallback\n",
    "==================================================================================\n",
    "\n",
    "- Bulk (vectorbt.YFData) + per-symbol yfinance with retries.\n",
    "- Accepts OHLC even if Volume missing.\n",
    "- Smart pruning & intersection to avoid index/column warnings.\n",
    "- **Synthetic fallback** (GBM) if all downloads fail (toggleable).\n",
    "\n",
    "Strategy:\n",
    "- Entries: bullish candle mask (+ optional RSI>50; optional trend checks; optional 52w filter)\n",
    "- Throttle: VOLAR top-K per date (optional)\n",
    "- Sizing: MVO via PyPortfolioOpt (optional) else equal-weight, scaled by deploy_cash_frac\n",
    "- Exits: SL/TP (+ optional EMA fast<slow)\n",
    "- Costs: fees & slippage\n",
    "\"\"\"\n",
    "\n",
    "import os, math, json, time, logging, warnings, random\n",
    "from dataclasses import dataclass , field\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import vectorbt as vbt\n",
    "import yfinance as yf\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ========= Logging =========\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"vbt_candle_rsi_final\")\n",
    "\n",
    "# ========= Config =========\n",
    "@dataclass\n",
    "class Config:\n",
    "    start_date: str = \"2018-01-01\"\n",
    "    end_date: str   = \"2025-01-01\"\n",
    "\n",
    "    # Provide either a file OR a list; list is easiest for first run\n",
    "    static_symbols: Optional[List[str]] = field(\n",
    "        default_factory=lambda: [\"RELIANCE.NS\",\"HDFCBANK.NS\",\"INFY.NS\",\"TCS.NS\",\"ICICIBANK.NS\"]\n",
    "    )\n",
    "    static_symbols_path: Optional[str] = None\n",
    "\n",
    "    out_dir: str = \"outputs_vbt\"\n",
    "    write_csv: bool = True\n",
    "    plot: bool = False  # set True if you want an interactive plot\n",
    "\n",
    "    # Data hygiene / loader behavior\n",
    "    min_rows: int = 120         # required AFTER alignment\n",
    "    min_rows_symbol: int = 60   # required per symbol BEFORE alignment\n",
    "    max_universe: int = 200\n",
    "    accept_missing_volume: bool = True\n",
    "    bulk_first: bool = True\n",
    "    bulk_chunk: int = 80\n",
    "    yf_retries: int = 3\n",
    "    yf_retry_base_sleep: float = 0.8\n",
    "\n",
    "    # <<< New >>> Synthetic fallback (if zero symbols downloaded)\n",
    "    fallback_to_synthetic: bool = True\n",
    "    synthetic_days: int = 600   # ~2.5y business days\n",
    "    synthetic_seed: int = 42\n",
    "\n",
    "    # Strategy toggles (you’ll enable RSI + candle)\n",
    "    use_rsi_confirm: bool   = True\n",
    "    use_trend_fast_slow: bool = False\n",
    "    use_htf_trend: bool       = False\n",
    "    use_indicator_exit: bool  = True\n",
    "\n",
    "    # Candle patterns (bullish)\n",
    "    enable_patterns: List[str] = (\n",
    "        \"ENGULFING\",\"PIERCING\",\"MORNING_STAR\",\"HARAMI\",\"HARAMI_CROSS\",\"HAMMER\",\"INVERTED_HAMMER\"\n",
    "    )\n",
    "\n",
    "    # Filters & ranking\n",
    "    use_52w_filter: bool     = True\n",
    "    filter_52w_window: int   = 252\n",
    "    within_pct_of_52w_high: float = 0.50\n",
    "    use_volar_ranking: bool  = True\n",
    "    volar_lookback: int      = 252\n",
    "    top_k_daily: int         = 3\n",
    "\n",
    "    # Sizing\n",
    "    use_mvo_sizing: bool     = False\n",
    "    deploy_cash_frac: float  = 0.25\n",
    "\n",
    "    # Trend params\n",
    "    ema_fast: int = 10\n",
    "    ema_slow: int = 20\n",
    "    ema_htf:  int = 200\n",
    "\n",
    "    # Stops\n",
    "    stop_loss_pct: float = 0.05\n",
    "    target_pct:    float = 0.10\n",
    "\n",
    "    # Costs\n",
    "    fees: float     = 0.0005\n",
    "    slippage: float = 0.0005\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "# ========= Helpers =========\n",
    "def ensure_dir(p): os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def load_universe(cfg: Config) -> List[str]:\n",
    "    if cfg.static_symbols:\n",
    "        syms = list(cfg.static_symbols)\n",
    "    elif cfg.static_symbols_path and os.path.exists(cfg.static_symbols_path):\n",
    "        with open(cfg.static_symbols_path) as f:\n",
    "            syms = [ln.strip() for ln in f if ln.strip()]\n",
    "    else:\n",
    "        raise ValueError(\"Provide CFG.static_symbols or CFG.static_symbols_path.\")\n",
    "\n",
    "    cleaned = []\n",
    "    for s in syms[:cfg.max_universe]:\n",
    "        s = s.strip().upper()\n",
    "        if s.startswith(\"^\"):     # drop indices\n",
    "            continue\n",
    "        if s.endswith(\".BO\"):     # drop BSE for now\n",
    "            continue\n",
    "        if not s.endswith(\".NS\"):\n",
    "            s = s + \".NS\"\n",
    "        cleaned.append(s)\n",
    "\n",
    "    out, seen = [], set()\n",
    "    for s in cleaned:\n",
    "        if s not in seen:\n",
    "            out.append(s); seen.add(s)\n",
    "    if not out:\n",
    "        raise ValueError(\"Universe empty after cleaning.\")\n",
    "    return out\n",
    "\n",
    "def _normalize_df(df: pd.DataFrame, accept_missing_volume: bool) -> Optional[pd.DataFrame]:\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "    df = df.rename(columns=str.title)\n",
    "    needed = ['Open','High','Low','Close']\n",
    "    if not set(needed).issubset(df.columns):\n",
    "        return None\n",
    "    cols = needed + (['Volume'] if ('Volume' in df.columns and not df['Volume'].isna().all()) or not accept_missing_volume else [])\n",
    "    if 'Volume' in cols and 'Volume' not in df.columns:\n",
    "        cols = needed\n",
    "    df = df[cols].dropna(subset=needed)\n",
    "    df.index = pd.to_datetime(df.index).tz_localize(None)\n",
    "    return df\n",
    "\n",
    "def yf_download_symbol(sym: str, start: str, end: str, accept_missing_volume: bool, retries: int, base_sleep: float) -> Optional[pd.DataFrame]:\n",
    "    for a in range(retries):\n",
    "        try:\n",
    "            df = yf.download(sym, start=start, end=end, interval='1d', auto_adjust=True, progress=False, threads=False)\n",
    "            df = _normalize_df(df, accept_missing_volume)\n",
    "            if df is not None and df.shape[0] >= CFG.min_rows_symbol:\n",
    "                return df\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(base_sleep * (2 ** a) + random.uniform(0, 0.3))\n",
    "    return None\n",
    "\n",
    "def bulk_download_vbt(tickers: List[str], start: str, end: str, accept_missing_volume: bool, chunk: int) -> Dict[str, pd.DataFrame]:\n",
    "    kept = {}\n",
    "    for i in range(0, len(tickers), chunk):\n",
    "        batch = tickers[i:i+chunk]\n",
    "        try:\n",
    "            data = vbt.YFData.download(\n",
    "                batch, start=start, end=end, auto_adjust=True, interval='1d',\n",
    "                missing_index='drop', missing_columns='drop'\n",
    "            )\n",
    "            for s in data.symbols:\n",
    "                O = data.select_symbol(s).get('Open')\n",
    "                H = data.select_symbol(s).get('High')\n",
    "                L = data.select_symbol(s).get('Low')\n",
    "                C = data.select_symbol(s).get('Close')\n",
    "                V = data.select_symbol(s).get('Volume')\n",
    "                if accept_missing_volume and (V is None or (hasattr(V,'isna') and V.isna().all())):\n",
    "                    df = pd.DataFrame({'Open': O, 'High': H, 'Low': L, 'Close': C})\n",
    "                else:\n",
    "                    if V is None:\n",
    "                        continue\n",
    "                    df = pd.DataFrame({'Open': O, 'High': H, 'Low': L, 'Close': C, 'Volume': V})\n",
    "                df = df.dropna(subset=['Open','High','Low','Close'])\n",
    "                df.index = pd.to_datetime(df.index).tz_localize(None)\n",
    "                if df.shape[0] >= CFG.min_rows_symbol:\n",
    "                    kept[s] = df\n",
    "        except Exception:\n",
    "            continue\n",
    "    return kept\n",
    "\n",
    "def download_clean_ohlcv(cfg: Config, tickers: List[str]) -> Dict[str, pd.DataFrame]:\n",
    "    kept: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "    if cfg.bulk_first:\n",
    "        log.info(\"Bulk downloading with vectorbt.YFData (drop mismatches) ...\")\n",
    "        kept = bulk_download_vbt(tickers, cfg.start_date, cfg.end_date, cfg.accept_missing_volume, cfg.bulk_chunk)\n",
    "\n",
    "    missing = [s for s in tickers if s not in kept]\n",
    "    if missing:\n",
    "        log.info(\"Per-symbol fallback via yfinance for %d tickers ...\", len(missing))\n",
    "        for j, s in enumerate(missing, 1):\n",
    "            df = yf_download_symbol(s, cfg.start_date, cfg.end_date, cfg.accept_missing_volume, cfg.yf_retries, cfg.yf_retry_base_sleep)\n",
    "            if df is not None:\n",
    "                kept[s] = df\n",
    "            if j % 25 == 0:\n",
    "                log.info(\"  processed %d/%d in fallback; kept=%d\", j, len(missing), len(kept))\n",
    "\n",
    "    dropped = [s for s in tickers if s not in kept]\n",
    "    if dropped:\n",
    "        log.warning(\"Dropped %d tickers (bad/short/missing): %s\", len(dropped), \", \".join(dropped[:20]) + (\" ...\" if len(dropped)>20 else \"\"))\n",
    "\n",
    "    return kept\n",
    "\n",
    "def synth_gbm_df(days: int, seed: int, s0: float = 100.0, mu: float = 0.12, sigma: float = 0.25) -> pd.DataFrame:\n",
    "    np.random.seed(seed)\n",
    "    idx = pd.bdate_range(end=pd.Timestamp.today().normalize(), periods=days)\n",
    "    dt = 1/252\n",
    "    shocks = np.random.normal((mu - 0.5*sigma*sigma)*dt, sigma*np.sqrt(dt), size=len(idx))\n",
    "    logp = np.log(s0) + np.cumsum(shocks)\n",
    "    close = np.exp(logp)\n",
    "    high = close * (1 + np.random.uniform(0.0, 0.01, size=len(idx)))\n",
    "    low  = close * (1 - np.random.uniform(0.0, 0.01, size=len(idx)))\n",
    "    open_ = close * (1 + np.random.uniform(-0.005, 0.005, size=len(idx)))\n",
    "    vol  = np.random.randint(100000, 5000000, size=len(idx))\n",
    "    df = pd.DataFrame({\"Open\": open_, \"High\": high, \"Low\": low, \"Close\": close, \"Volume\": vol}, index=idx)\n",
    "    return df\n",
    "\n",
    "def build_common_index_smart(data_map: Dict[str, pd.DataFrame], min_rows: int) -> Tuple[pd.DatetimeIndex, Dict[str, pd.DataFrame]]:\n",
    "    lengths = sorted(((k, len(v)) for k, v in data_map.items()), key=lambda x: x[1], reverse=True)\n",
    "    symbols = [k for k, _ in lengths]\n",
    "    current = symbols.copy()\n",
    "\n",
    "    def intersection_of(symbols_list):\n",
    "        idxs = [data_map[s].index for s in symbols_list]\n",
    "        if not idxs:\n",
    "            return pd.DatetimeIndex([])\n",
    "        inter = reduce(lambda a, b: a.intersection(b), idxs)\n",
    "        inter = pd.DatetimeIndex(sorted(inter.unique()))\n",
    "        return inter\n",
    "\n",
    "    inter = intersection_of(current)\n",
    "    while len(inter) < min_rows and len(current) > 1:\n",
    "        shortest = min(current, key=lambda s: len(data_map[s]))\n",
    "        current.remove(shortest)\n",
    "        inter = intersection_of(current)\n",
    "\n",
    "    kept_map = {s: data_map[s] for s in current}\n",
    "    return inter, kept_map\n",
    "\n",
    "def align_to_index(data_map: Dict[str, pd.DataFrame], idx: pd.DatetimeIndex) -> Dict[str, pd.DataFrame]:\n",
    "    aligned = {}\n",
    "    for s, df in data_map.items():\n",
    "        d = df.reindex(idx).dropna()\n",
    "        aligned[s] = d\n",
    "    return aligned\n",
    "\n",
    "# ========= Candle patterns =========\n",
    "def _body(O, C): return (C - O).abs()\n",
    "def _is_bull(O, C): return C > O\n",
    "def _is_bear(O, C): return C < O\n",
    "\n",
    "def patt_bull_engulfing(O,H,L,C):\n",
    "    Op, Cp = O.shift(1), C.shift(1)\n",
    "    return _is_bear(Op,Cp) & _is_bull(O,C) & (C >= Op) & (O <= Cp)\n",
    "\n",
    "def patt_piercing(O,H,L,C):\n",
    "    Op, Hp, Lp, Cp = O.shift(1), H.shift(1), L.shift(1), C.shift(1)\n",
    "    mid_prev = (Op + Cp) / 2.0\n",
    "    return _is_bear(Op,Cp) & _is_bull(O,C) & (O < Lp) & (C > mid_prev) & (C < Op)\n",
    "\n",
    "def patt_morning_star(O,H,L,C):\n",
    "    O1, C1 = O.shift(1), C.shift(1)\n",
    "    O2, C2 = O.shift(2), C.shift(2)\n",
    "    bear1 = _is_bear(O2,C2)\n",
    "    small2 = (_body(O1,C1) <= (_body(O2,C2)*0.6))\n",
    "    bull3 = _is_bull(O,C)\n",
    "    mid1 = (O2 + C2) / 2.0\n",
    "    retrace = C > mid1\n",
    "    return bear1 & small2 & bull3 & retrace\n",
    "\n",
    "def patt_harami_bull(O,H,L,C):\n",
    "    O1, C1 = O.shift(1), C.shift(1)\n",
    "    prev_bear = _is_bear(O1,C1)\n",
    "    body_small = (_body(O,C) <= _body(O1,C1) * 0.75)\n",
    "    inside = (np.maximum(O,C) <= O1) & (np.minimum(O,C) >= C1)\n",
    "    return prev_bear & body_small & inside & (C >= O)\n",
    "\n",
    "def patt_harami_cross_bull(O,H,L,C, doji_pct=0.1):\n",
    "    O1, C1 = O.shift(1), C.shift(1)\n",
    "    rng = (H - L).replace(0, np.nan)\n",
    "    doji = (_body(O,C) <= (rng * doji_pct))\n",
    "    prev_bear = _is_bear(O1,C1)\n",
    "    inside = (np.maximum(O,C) <= O1) & (np.minimum(O,C) >= C1)\n",
    "    return prev_bear & doji & inside\n",
    "\n",
    "def patt_hammer(O,H,L,C, shadow_mult=2.0):\n",
    "    body = _body(O,C)\n",
    "    lower_shadow = (np.minimum(O,C) - L).abs()\n",
    "    upper_shadow = (H - np.maximum(O,C)).abs()\n",
    "    return (lower_shadow >= shadow_mult*body) & (upper_shadow <= body) & (C >= O)\n",
    "\n",
    "def patt_inverted_hammer(O,H,L,C, shadow_mult=2.0):\n",
    "    body = _body(O,C)\n",
    "    upper_shadow = (H - np.maximum(O,C)).abs()\n",
    "    lower_shadow = (np.minimum(O,C) - L).abs()\n",
    "    return (upper_shadow >= shadow_mult*body) & (lower_shadow <= body) & (C >= O)\n",
    "\n",
    "def bullish_pattern_mask(O: pd.DataFrame, H: pd.DataFrame, L: pd.DataFrame, C: pd.DataFrame, enabled: List[str]) -> pd.DataFrame:\n",
    "    masks = []\n",
    "    if \"ENGULFING\" in enabled:        masks.append(patt_bull_engulfing(O,H,L,C))\n",
    "    if \"PIERCING\" in enabled:         masks.append(patt_piercing(O,H,L,C))\n",
    "    if \"MORNING_STAR\" in enabled:     masks.append(patt_morning_star(O,H,L,C))\n",
    "    if \"HARAMI\" in enabled:           masks.append(patt_harami_bull(O,H,L,C))\n",
    "    if \"HARAMI_CROSS\" in enabled:     masks.append(patt_harami_cross_bull(O,H,L,C))\n",
    "    if \"HAMMER\" in enabled:           masks.append(patt_hammer(O,H,L,C))\n",
    "    if \"INVERTED_HAMMER\" in enabled:  masks.append(patt_inverted_hammer(O,H,L,C))\n",
    "    if not masks:\n",
    "        return pd.DataFrame(False, index=C.index, columns=C.columns)\n",
    "    out = masks[0]\n",
    "    for m in masks[1:]:\n",
    "        out = out | m\n",
    "    return out.fillna(False)\n",
    "\n",
    "# ========= Indicators / util =========\n",
    "def ema(df: pd.DataFrame, span: int) -> pd.DataFrame:\n",
    "    return df.ewm(span=span, adjust=False, min_periods=span).mean()\n",
    "\n",
    "def rsi(series: pd.DataFrame, length: int = 14) -> pd.DataFrame:\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0.0)).rolling(length).mean()\n",
    "    loss = (-delta.where(delta < 0, 0.0)).rolling(length).mean()\n",
    "    rs = gain / loss.replace(0.0, np.nan)\n",
    "    out = 100 - (100 / (1 + rs))\n",
    "    return out.fillna(50.0)\n",
    "\n",
    "def volar_scores(close: pd.DataFrame, bench: pd.Series, dt: pd.Timestamp, lookback: int) -> pd.Series:\n",
    "    s_ret = close.loc[:dt].pct_change().dropna().iloc[-lookback:]\n",
    "    b_ret = bench.loc[:dt].pct_change().dropna().iloc[-lookback:]\n",
    "    scores = {}\n",
    "    for col in close.columns:\n",
    "        if col not in s_ret:\n",
    "            scores[col] = np.nan; continue\n",
    "        s = s_ret[col]\n",
    "        common = pd.concat([s, b_ret], axis=1, keys=[\"s\",\"b\"]).dropna()\n",
    "        if common.shape[0] < max(20, int(0.4*lookback)):\n",
    "            scores[col] = np.nan; continue\n",
    "        excess = common[\"s\"] - common[\"b\"]\n",
    "        vol = common[\"s\"].std(ddof=0)\n",
    "        scores[col] = 0.0 if vol <= 1e-8 else float((excess.mean() / vol) * math.sqrt(252.0))\n",
    "    return pd.Series(scores)\n",
    "\n",
    "def try_mvo_weights(returns_window: pd.DataFrame) -> pd.Series:\n",
    "    try:\n",
    "        from pypfopt import expected_returns, risk_models, EfficientFrontier\n",
    "        if returns_window.empty:\n",
    "            return pd.Series(dtype=float)\n",
    "        prices_win = (1 + returns_window).cumprod()\n",
    "        mu = expected_returns.mean_historical_return(prices_win, frequency=252)\n",
    "        S  = risk_models.CovarianceShrinkage(prices_win).ledoit_wolf()\n",
    "        ef = EfficientFrontier(mu, S)\n",
    "        w  = ef.max_sharpe()\n",
    "        w  = pd.Series(ef.clean_weights())\n",
    "        w = w.clip(lower=0)\n",
    "        return w / w.sum() if w.sum() > 0 else pd.Series(dtype=float)\n",
    "    except Exception:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "# ========= Main =========\n",
    "def main(cfg: Config):\n",
    "    ensure_dir(cfg.out_dir)\n",
    "    tickers = load_universe(cfg)\n",
    "    log.info(\"Loaded %d tickers (after cleaning)\", len(tickers))\n",
    "\n",
    "    # 1) Try to download real data\n",
    "    data_map = download_clean_ohlcv(cfg, tickers)\n",
    "\n",
    "    # 2) If nothing came back and fallback allowed, synthesize GBM data\n",
    "    if (not data_map) and cfg.fallback_to_synthetic:\n",
    "        log.error(\"No valid symbols after cleaning. Using SYNTHETIC OHLC (GBM) so you can run end-to-end.\")\n",
    "        np.random.seed(cfg.synthetic_seed)\n",
    "        synth_map = {}\n",
    "        cols = tickers[:min(5, len(tickers))]  # keep it small\n",
    "        for i, s in enumerate(cols):\n",
    "            df = synth_gbm_df(cfg.synthetic_days, seed=cfg.synthetic_seed + i, s0=100+10*i)\n",
    "            synth_map[s] = df\n",
    "        data_map = synth_map\n",
    "\n",
    "    if not data_map:\n",
    "        raise RuntimeError(\"No data available (and synthetic disabled). Check network/Yahoo access.\")\n",
    "\n",
    "    # 3) Build intersection index (auto-prune worst offenders) to hit min_rows\n",
    "    inter_idx, kept_map = build_common_index_smart(data_map, cfg.min_rows)\n",
    "    if len(inter_idx) < cfg.min_rows:\n",
    "        # As a last resort with real data, relax requirement:\n",
    "        log.warning(\"Intersection < min_rows; relaxing to len(intersection)=%d\", len(inter_idx))\n",
    "        if len(inter_idx) == 0:\n",
    "            # If synthetic, inter_idx will be fine; if still zero, bail\n",
    "            raise RuntimeError(\"Common intersection is empty after pruning.\")\n",
    "    aligned = align_to_index(kept_map, inter_idx if len(inter_idx)>0 else list(kept_map.values())[0].index)\n",
    "\n",
    "    # Build wide OHLC\n",
    "    Open  = pd.concat({k: v['Open']  for k, v in aligned.items()}, axis=1)\n",
    "    High  = pd.concat({k: v['High']  for k, v in aligned.items()}, axis=1)\n",
    "    Low   = pd.concat({k: v['Low']   for k, v in aligned.items()}, axis=1)\n",
    "    Close = pd.concat({k: v['Close'] for k, v in aligned.items()}, axis=1)\n",
    "\n",
    "    cols = Close.columns\n",
    "    Open, High, Low = (df.reindex(columns=cols).astype(float) for df in (Open, High, Low))\n",
    "    Close = Close.astype(float)\n",
    "\n",
    "    log.info(\"Final universe: %d symbols | Date range: %s -> %s | Points: %d\",\n",
    "             len(cols), Close.index[0].date(), Close.index[-1].date(), len(Close))\n",
    "\n",
    "    # Benchmark for VOLAR\n",
    "    bench_price = Close.pct_change().median(axis=1).add(1).cumprod()\n",
    "    bench_price.index = Close.index\n",
    "\n",
    "    # Indicators\n",
    "    ema_fast = ema(Close, cfg.ema_fast)\n",
    "    ema_slow = ema(Close, cfg.ema_slow)\n",
    "    ema_htf  = ema(Close, cfg.ema_htf)\n",
    "    rsi14    = rsi(Close, 14)\n",
    "\n",
    "    # Candles\n",
    "    bull_candle = bullish_pattern_mask(Open, High, Low, Close, list(cfg.enable_patterns))\n",
    "\n",
    "    # Entries\n",
    "    entries = bull_candle.copy()\n",
    "    if cfg.use_rsi_confirm:\n",
    "        entries &= (rsi14 > 50)\n",
    "    if cfg.use_trend_fast_slow:\n",
    "        entries &= (ema_fast > ema_slow)\n",
    "    if cfg.use_htf_trend:\n",
    "        entries &= (Close > ema_htf)\n",
    "\n",
    "    # 52w filter\n",
    "    if cfg.use_52w_filter:\n",
    "        high_52w = Close.rolling(cfg.filter_52w_window, min_periods=50).max()\n",
    "        entries &= (Close >= cfg.within_pct_of_52w_high * high_52w)\n",
    "\n",
    "    # VOLAR top-K\n",
    "    if cfg.use_volar_ranking and cfg.top_k_daily > 0:\n",
    "        ranked = pd.DataFrame(False, index=entries.index, columns=entries.columns)\n",
    "        idx_with = entries.index[entries.any(axis=1)]\n",
    "        for dt in idx_with:\n",
    "            elig = entries.loc[dt]\n",
    "            if not elig.any(): continue\n",
    "            scores = volar_scores(Close, bench_price, dt, cfg.volar_lookback)\n",
    "            scores = scores.where(elig, np.nan).dropna()\n",
    "            if scores.empty: continue\n",
    "            topn = scores.sort_values(ascending=False).head(cfg.top_k_daily).index\n",
    "            ranked.loc[dt, topn] = True\n",
    "        entries = ranked\n",
    "\n",
    "    # Exits\n",
    "    exits = pd.DataFrame(False, index=entries.index, columns=entries.columns)\n",
    "    if cfg.use_indicator_exit:\n",
    "        exits = ema_fast < ema_slow\n",
    "\n",
    "    # Weights\n",
    "    weights = pd.DataFrame(0.0, index=entries.index, columns=entries.columns)\n",
    "    ret = Close.pct_change()\n",
    "    idx_with = entries.index[entries.any(axis=1)]\n",
    "    for dt in idx_with:\n",
    "        cols_today = entries.columns[entries.loc[dt]]\n",
    "        if len(cols_today) == 0: continue\n",
    "        if cfg.use_mvo_sizing and len(cols_today) >= 2:\n",
    "            R = ret.loc[:dt, cols_today].dropna().iloc[-cfg.volar_lookback:]\n",
    "            w = try_mvo_weights(R)\n",
    "            if w.empty:\n",
    "                w = pd.Series(1/len(cols_today), index=cols_today)\n",
    "            else:\n",
    "                w = w.reindex(cols_today).fillna(0.0)\n",
    "                w = w / w.sum() if w.sum() > 0 else pd.Series(1/len(cols_today), index=cols_today)\n",
    "        else:\n",
    "            w = pd.Series(1/len(cols_today), index=cols_today)\n",
    "        weights.loc[dt, cols_today] = w * cfg.deploy_cash_frac\n",
    "\n",
    "    log.info(\"Building vectorbt portfolio...\")\n",
    "    pf = vbt.Portfolio.from_signals(\n",
    "        Close,\n",
    "        entries=entries,\n",
    "        exits=exits,\n",
    "        sl_stop=cfg.stop_loss_pct if cfg.stop_loss_pct > 0 else None,\n",
    "        tp_stop=cfg.target_pct    if cfg.target_pct    > 0 else None,\n",
    "        fees=cfg.fees,\n",
    "        slippage=cfg.slippage,\n",
    "        init_cash=1_000_000.0,\n",
    "        cash_sharing=True,\n",
    "        weights=weights,\n",
    "        freq='1D'\n",
    "    )\n",
    "\n",
    "    stats = pf.stats()\n",
    "    print(\"\\n=== Portfolio Stats ===\")\n",
    "    print(stats)\n",
    "\n",
    "    stamp = pd.Timestamp.now(tz=\"Asia/Kolkata\").strftime(\"%Y%m%d_%H%M%S\")\n",
    "    if cfg.write_csv:\n",
    "        ensure_dir(cfg.out_dir)\n",
    "        entries.astype(int).to_csv(os.path.join(cfg.out_dir, f\"entries_{stamp}.csv\"))\n",
    "        exits.astype(int).to_csv(os.path.join(cfg.out_dir, f\"exits_{stamp}.csv\"))\n",
    "        weights.to_csv(os.path.join(cfg.out_dir, f\"weights_{stamp}.csv\"))\n",
    "        pf.value().to_csv(os.path.join(cfg.out_dir, f\"equity_{stamp}.csv\"), header=[\"equity\"])\n",
    "        with open(os.path.join(cfg.out_dir, f\"stats_{stamp}.json\"), \"w\") as f:\n",
    "            json.dump(stats.to_dict(), f, indent=2)\n",
    "        log.info(\"Files written to %s\", cfg.out_dir)\n",
    "\n",
    "    if cfg.plot:\n",
    "        try:\n",
    "            pf.value().vbt.plot(title=\"Equity Curve (VectorBT Candle + RSI)\").show()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(CFG)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".talib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
