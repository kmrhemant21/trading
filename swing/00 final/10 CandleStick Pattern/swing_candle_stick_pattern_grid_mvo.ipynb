{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Simple grid search for candle_confirm_mvo (no plotting).\n",
    "- ThreadPool only (pickle-free, notebook-safe).\n",
    "- Cache-only data loading (reads parquet; no network).\n",
    "- Train/Test split with per-combo logging & BEST-SO-FAR updates.\n",
    "- Saves a sorted CSV and artifacts (legs/roundtrips/equity/metrics) for top-N configs.\n",
    "\n",
    "Place next to `candle_confirm_mvo_v1.py` and run:\n",
    "    python grid_search_candle_confirm_mvo.py\n",
    "\"\"\"\n",
    "\n",
    "import os, json, itertools, importlib, logging\n",
    "from copy import deepcopy\n",
    "from typing import Dict, List, Any, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing.dummy import Pool as ThreadPool  # threads only\n",
    "\n",
    "# =========================\n",
    "# CONFIG — EDIT HERE\n",
    "# =========================\n",
    "\n",
    "STRAT_MODULE = \"candle_confirm_mvo_v1\"   # strategy filename without .py\n",
    "\n",
    "# Data windows\n",
    "START_DATE   = \"2015-01-01\"              # global fetch start\n",
    "TRAIN_END    = \"2021-12-31\"              # inclusive\n",
    "TEST_START   = \"2022-01-01\"              # inclusive\n",
    "TEST_END     = \"2025-01-01\"              # inclusive\n",
    "\n",
    "# Universe: set to a text file OR use the inline list below\n",
    "SYMBOLS_FILE = \"nifty500.txt\"  # e.g., \"nifty500.txt\" (one ticker per line)\n",
    "UNIVERSE = [\n",
    "    '360ONE.NS','3MINDIA.NS','AARTIIND.NS','ABB.NS','ACC.NS','ADANIENT.NS','ADANIPORTS.NS',\n",
    "    'APOLLOHOSP.NS','ASIANPAINT.NS','AXISBANK.NS','BAJAJ-AUTO.NS','BAJFINANCE.NS','BHARTIARTL.NS',\n",
    "    'BPCL.NS','BRITANNIA.NS','CIPLA.NS','COALINDIA.NS','COFORGE.NS','DRREDDY.NS','EICHERMOT.NS',\n",
    "    'GRASIM.NS','HCLTECH.NS','HDFCBANK.NS','HINDALCO.NS','HINDUNILVR.NS','ICICIBANK.NS',\n",
    "    'INFY.NS','ITC.NS','JSWSTEEL.NS','KOTAKBANK.NS','LT.NS','MARUTI.NS','NESTLEIND.NS','NTPC.NS',\n",
    "    'ONGC.NS','POWERGRID.NS','RELIANCE.NS','SBIN.NS','SUNPHARMA.NS','TATAMOTORS.NS',\n",
    "    'TATASTEEL.NS','TCS.NS','TECHM.NS','TITAN.NS','ULTRACEMCO.NS','WIPRO.NS'\n",
    "]\n",
    "\n",
    "# Output folder\n",
    "OUT_DIR = \"outputs\"\n",
    "\n",
    "# Execution (threads only; avoids pickling issues)\n",
    "N_JOBS = 4       # set 1 for serial\n",
    "\n",
    "# Save artifacts (legs/roundtrips/equity/metrics) for top-N configs after sweep\n",
    "SAVE_TOP_N = 3\n",
    "# Grid to sweep\n",
    "PARAM_GRID = {\n",
    "    # Trend filters\n",
    "    \"ema_fast\": [8, 10, 12],\n",
    "    \"ema_slow\": [20, 26, 30],\n",
    "    \"use_htf_trend\": [True, False],\n",
    "    \"ema_htf\": [150, 200],\n",
    "\n",
    "    # Exits\n",
    "    \"stop_loss_pct\": [0.03, 0.05, 0.07],\n",
    "    \"target_pct\":    [0.08, 0.10, 0.15],\n",
    "\n",
    "    # Portfolio\n",
    "    \"max_concurrent_positions\": [3, 5, 8],\n",
    "    \"deploy_cash_frac\": [0.25, 0.5],\n",
    "    \"top_k_daily\": [50, 150, 300],\n",
    "\n",
    "    # Ranking filter\n",
    "    \"within_pct_of_52w_high\": [0.4, 0.5, 0.6],\n",
    "\n",
    "    # Liquidity guards (off by default)\n",
    "    \"enable_basic_liquidity\": [False],\n",
    "}\n",
    "\n",
    "# Logging\n",
    "LOG_LEVEL = logging.INFO\n",
    "LOG_NAME  = \"grid_candle_confirm_mvo\"\n",
    "LOG_FMT   = \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "DATE_FMT  = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "logging.basicConfig(level=LOG_LEVEL, format=LOG_FMT, datefmt=DATE_FMT)\n",
    "log = logging.getLogger(LOG_NAME)\n",
    "\n",
    "# =========================\n",
    "# Helpers (no plotting)\n",
    "# =========================\n",
    "def composite_score(m: dict) -> float:\n",
    "    \"\"\"Higher Sharpe & CAGR, penalize deeper drawdown; discard too-few trades.\"\"\"\n",
    "    n_trades = m.get(\"n_trades\", 0) or 0\n",
    "    if n_trades < 50:\n",
    "        return -1e9\n",
    "    sharpe = m.get(\"sharpe\", 0.0) or 0.0\n",
    "    cagr   = (m.get(\"cagr_pct\", 0.0) or 0.0) / 100.0\n",
    "    mdd    = abs(m.get(\"max_drawdown_pct\", 0.0) or 0.0) / 100.0\n",
    "    return (1.00 * sharpe) + (0.75 * cagr * 10.0) - (0.50 * mdd * 10.0)\n",
    "\n",
    "def read_cached_prices(cache_dir: str, tickers: List[str]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Cache-only loader: reads parquet; no network calls here.\"\"\"\n",
    "    dm = {}\n",
    "    for t in tickers:\n",
    "        p = os.path.join(cache_dir, f\"{t.replace('^','_')}.parquet\")\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                df = pd.read_parquet(p)\n",
    "                if df is not None and not df.empty:\n",
    "                    dm[t] = df\n",
    "            except Exception:\n",
    "                pass\n",
    "    return dm\n",
    "\n",
    "def simulate_universe_period(\n",
    "    strat, symbols: List[str], data_map: Dict[str, pd.DataFrame], bench_df: pd.DataFrame,\n",
    "    cfg, period_start: str, period_end: str,\n",
    "):\n",
    "    \"\"\"Slice each ticker’s data to [period_start, period_end] and reuse simulate_ticker + aggregate_and_apply.\"\"\"\n",
    "    ps = pd.to_datetime(period_start); pe = pd.to_datetime(period_end)\n",
    "    all_trades = []\n",
    "    for t in symbols:\n",
    "        df = data_map.get(t)\n",
    "        if df is None or df.empty: continue\n",
    "        dfp = df[(df.index >= ps) & (df.index <= pe)]\n",
    "        if dfp.empty: continue\n",
    "        tr, _ = strat.simulate_ticker(t, dfp, cfg)\n",
    "        if not tr.empty: all_trades.append(tr)\n",
    "    if not all_trades:\n",
    "        # empty-safe metrics\n",
    "        return pd.DataFrame(), pd.DataFrame(), pd.Series(dtype=float), {\n",
    "            \"start_equity_inr\": cfg.initial_capital,\n",
    "            \"final_equity_inr\": cfg.initial_capital,\n",
    "            \"cagr_pct\": 0.0,\n",
    "            \"sharpe\": 0.0,\n",
    "            \"max_drawdown_pct\": 0.0,\n",
    "            \"win_rate_pct\": 0.0,\n",
    "            \"n_trades\": 0,\n",
    "        }\n",
    "    all_trades = pd.concat(all_trades, ignore_index=True)\n",
    "    legs_df, trips_df, equity, metrics = strat.aggregate_and_apply(all_trades, data_map, bench_df, cfg)\n",
    "    return legs_df, trips_df, equity, metrics\n",
    "\n",
    "def log_combo(idx: int, total: int, keys: List[str], row: dict):\n",
    "    if \"error\" in row:\n",
    "        log.error(\"Combo %4d/%d ERROR %s | params=%s\",\n",
    "                  idx, total, row[\"error\"], {k: row.get(k, None) for k in keys})\n",
    "        return\n",
    "    params_str = \", \".join(f\"{k}={row[k]}\" for k in keys)\n",
    "    log.info(\n",
    "        \"Combo %4d/%d  [train: trades=%d, Sharpe=%.3f, CAGR=%.2f%%, MDD=%.2f%%, score=%.3f]  \"\n",
    "        \"[test: trades=%d, Sharpe=%.3f, CAGR=%.2f%%, MDD=%.2f%%, score=%.3f]  | %s\",\n",
    "        idx, total,\n",
    "        row[\"train_n_trades\"], row[\"train_sharpe\"], row[\"train_cagr_pct\"], row[\"train_max_drawdown_pct\"], row[\"train_score\"],\n",
    "        row[\"test_n_trades\"], row[\"test_sharpe\"], row[\"test_cagr_pct\"], row[\"test_max_drawdown_pct\"], row[\"test_score\"],\n",
    "        params_str\n",
    "    )\n",
    "\n",
    "def maybe_update_best(row: dict, best_row: dict, best_score: float, keys: List[str]):\n",
    "    if \"error\" in row: return best_row, best_score\n",
    "    if best_score is None or row[\"test_score\"] > best_score:\n",
    "        params_str = \", \".join(f\"{k}={row[k]}\" for k in keys)\n",
    "        log.warning(\n",
    "            \"BEST-SO-FAR ⇧ test_score=%.3f | test_sharpe=%.3f, test_cagr=%.2f%%, test_mdd=%.2f%%, test_trades=%d | %s\",\n",
    "            row[\"test_score\"], row[\"test_sharpe\"], row[\"test_cagr_pct\"],\n",
    "            row[\"test_max_drawdown_pct\"], row[\"test_n_trades\"], params_str\n",
    "        )\n",
    "        return row, row[\"test_score\"]\n",
    "    return best_row, best_score\n",
    "\n",
    "def save_top_artifacts(strat, keys: List[str], top_rows: pd.DataFrame,\n",
    "                       symbols: List[str], data_map: Dict[str, pd.DataFrame],\n",
    "                       bench_df: pd.DataFrame, base_cfg, stamp: str):\n",
    "    \"\"\"\n",
    "    Re-run top configs on TEST window and dump legs/roundtrips/equity/metrics files (no plotting).\n",
    "    \"\"\"\n",
    "    for i, (_, r) in enumerate(top_rows.iterrows(), 1):\n",
    "        cfg_i = deepcopy(base_cfg)\n",
    "        for k in keys:\n",
    "            if k in r:\n",
    "                setattr(cfg_i, k, r[k])\n",
    "        legs, trips, equity, metrics = simulate_universe_period(\n",
    "            strat, symbols, data_map, bench_df, cfg_i, TEST_START, TEST_END\n",
    "        )\n",
    "        prefix = os.path.join(OUT_DIR, f\"grid_top{i}_{stamp}\")\n",
    "        try:\n",
    "            if legs is not None and not legs.empty:\n",
    "                legs.to_csv(prefix + \"_legs.csv\", index=False)\n",
    "            if trips is not None and not trips.empty:\n",
    "                trips.to_csv(prefix + \"_roundtrips.csv\", index=False)\n",
    "            if equity is not None and not equity.empty:\n",
    "                pd.DataFrame({\"date\": equity.index, \"equity\": equity.values}).to_csv(prefix + \"_equity.csv\", index=False)\n",
    "            with open(prefix + \"_metrics.json\", \"w\") as f:\n",
    "                json.dump(dict(metrics), f, indent=2)\n",
    "            log.info(\"Artifacts saved: %s_*\", prefix)\n",
    "        except Exception as e:\n",
    "            log.error(\"Failed to save artifacts for top-%d: %s\", i, repr(e))\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "def main():\n",
    "    # Import strategy and build a base config\n",
    "    strat = importlib.import_module(STRAT_MODULE)\n",
    "    cfg = strat.Config()\n",
    "    cfg.start_date = START_DATE\n",
    "    cfg.end_date   = TEST_END\n",
    "    cfg.out_dir    = OUT_DIR\n",
    "    cfg.plot       = False  # no images\n",
    "    # Universe\n",
    "    if SYMBOLS_FILE:\n",
    "        cfg.static_symbols = None\n",
    "        cfg.static_symbols_path = SYMBOLS_FILE\n",
    "    else:\n",
    "        cfg.static_symbols = UNIVERSE\n",
    "\n",
    "    # Ensure dirs & resolve symbols\n",
    "    strat.ensure_dirs(cfg.cache_dir, cfg.out_dir)\n",
    "    symbols = strat.load_static_symbols(cfg.static_symbols, cfg.static_symbols_path)\n",
    "    log.info(\"Universe size: %d\", len(symbols))\n",
    "\n",
    "    # Cache-only prices (no network here)\n",
    "    data_map = read_cached_prices(cfg.cache_dir, symbols)\n",
    "    if not data_map:\n",
    "        log.error(\"No cached parquet files found in '%s'. Run your backtest once to build cache, then re-run grid.\", cfg.cache_dir)\n",
    "        return\n",
    "\n",
    "    # Synthetic flat benchmark (keeps VOLAR math stable without fetching)\n",
    "    idx = pd.date_range(start=cfg.start_date, end=cfg.end_date, freq=\"B\")\n",
    "    bench_df = pd.DataFrame({\"Close\": np.ones(len(idx))}, index=idx)\n",
    "\n",
    "    # Build grid\n",
    "    keys = list(PARAM_GRID.keys())\n",
    "    vals = [PARAM_GRID[k] for k in keys]\n",
    "    combos_all = list(itertools.product(*vals))\n",
    "    combos = [c for c in combos_all if dict(zip(keys, c))[\"ema_fast\"] < dict(zip(keys, c))[\"ema_slow\"]]\n",
    "    log.info(\"Grid size (after sanity): %d (of %d)\", len(combos), len(combos_all))\n",
    "    log.info(\"Train: %s → %s | Test: %s → %s\", START_DATE, TRAIN_END, TEST_START, TEST_END)\n",
    "    log.info(\"Threads: %d\", N_JOBS)\n",
    "\n",
    "    # Worker using shared in-memory objects (threads)\n",
    "    def worker(vals_tuple: Tuple[Any, ...]):\n",
    "        overrides = dict(zip(keys, vals_tuple))\n",
    "        try:\n",
    "            cfg_i = deepcopy(cfg)\n",
    "            for k, v in overrides.items():\n",
    "                setattr(cfg_i, k, v)\n",
    "\n",
    "            # TRAIN\n",
    "            _, _, _, met_tr = simulate_universe_period(\n",
    "                strat, symbols, data_map, bench_df, cfg_i, START_DATE, TRAIN_END\n",
    "            )\n",
    "            sc_tr = composite_score(met_tr)\n",
    "\n",
    "            # TEST\n",
    "            _, _, _, met_te = simulate_universe_period(\n",
    "                strat, symbols, data_map, bench_df, cfg_i, TEST_START, TEST_END\n",
    "            )\n",
    "            sc_te = composite_score(met_te)\n",
    "\n",
    "            return {\n",
    "                **overrides,\n",
    "                \"train_cagr_pct\": float(met_tr.get(\"cagr_pct\", 0.0) or 0.0),\n",
    "                \"train_sharpe\": float(met_tr.get(\"sharpe\", 0.0) or 0.0),\n",
    "                \"train_max_drawdown_pct\": float(met_tr.get(\"max_drawdown_pct\", 0.0) or 0.0),\n",
    "                \"train_win_rate_pct\": float(met_tr.get(\"win_rate_pct\", 0.0) or 0.0),\n",
    "                \"train_n_trades\": int(met_tr.get(\"n_trades\", 0) or 0),\n",
    "                \"train_score\": float(sc_tr),\n",
    "\n",
    "                \"test_cagr_pct\": float(met_te.get(\"cagr_pct\", 0.0) or 0.0),\n",
    "                \"test_sharpe\": float(met_te.get(\"sharpe\", 0.0) or 0.0),\n",
    "                \"test_max_drawdown_pct\": float(met_te.get(\"max_drawdown_pct\", 0.0) or 0.0),\n",
    "                \"test_win_rate_pct\": float(met_te.get(\"win_rate_pct\", 0.0) or 0.0),\n",
    "                \"test_n_trades\": int(met_te.get(\"n_trades\", 0) or 0),\n",
    "                \"test_score\": float(sc_te),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            err = {\"error\": repr(e)}\n",
    "            err.update(overrides)\n",
    "            return err\n",
    "\n",
    "    # Run (threads)\n",
    "    results: List[dict] = []\n",
    "    best_row = None\n",
    "    best_score = None\n",
    "\n",
    "    if N_JOBS <= 1:\n",
    "        for i, vals_tuple in enumerate(combos, 1):\n",
    "            row = worker(vals_tuple)\n",
    "            if row is None: continue\n",
    "            results.append(row)\n",
    "            log_combo(i, len(combos), keys, row)\n",
    "            best_row, best_score = maybe_update_best(row, best_row, best_score, keys)\n",
    "    else:\n",
    "        with ThreadPool(N_JOBS) as pool:\n",
    "            for i, row in enumerate(pool.imap_unordered(worker, combos), 1):\n",
    "                if row is None: continue\n",
    "                results.append(row)\n",
    "                log_combo(i, len(combos), keys, row)\n",
    "                best_row, best_score = maybe_update_best(row, best_row, best_score, keys)\n",
    "\n",
    "    if not results:\n",
    "        log.warning(\"No results generated.\")\n",
    "        return\n",
    "\n",
    "    # Save results CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    if \"test_score\" in df.columns:\n",
    "        df = df.sort_values([\"test_score\", \"test_sharpe\", \"test_cagr_pct\"], ascending=[False, False, False]).reset_index(drop=True)\n",
    "\n",
    "    stamp = pd.Timestamp.today(tz=\"Asia/Kolkata\").strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_csv = os.path.join(OUT_DIR, f\"grid_search_results_{stamp}.csv\")\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    log.info(\"Grid results saved → %s\", out_csv)\n",
    "\n",
    "    # Save artifacts for top N (no plots)\n",
    "    if SAVE_TOP_N > 0 and not df.empty:\n",
    "        log.info(\"Saving artifacts for top %d configs…\", min(SAVE_TOP_N, len(df)))\n",
    "        save_top_artifacts(strat, keys, df.head(SAVE_TOP_N), symbols, data_map, bench_df, cfg, stamp)\n",
    "\n",
    "    # Final best summary\n",
    "    if best_row is not None:\n",
    "        params_str = \", \".join(f\"{k}={best_row[k]}\" for k in keys)\n",
    "        log.warning(\n",
    "            \"FINAL BEST (by test_score): test_score=%.3f | test_sharpe=%.3f, test_cagr=%.2f%%, test_mdd=%.2f%%, test_trades=%d | %s\",\n",
    "            best_row[\"test_score\"], best_row[\"test_sharpe\"], best_row[\"test_cagr_pct\"],\n",
    "            best_row[\"test_max_drawdown_pct\"], best_row[\"test_n_trades\"], params_str\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".talib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
